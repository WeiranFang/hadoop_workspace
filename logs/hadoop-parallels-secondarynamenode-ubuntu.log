2015-12-04 21:18:54,092 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-04 21:18:54,100 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-04 21:18:54,582 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-04 21:18:54,640 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-04 21:18:54,640 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-04 21:18:54,850 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-parallels/dfs/namesecondary/in_use.lock acquired by nodename 3582@ubuntu
2015-12-04 21:18:54,855 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-04 21:18:54,861 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-04 21:18:54,894 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-04 21:18:54,894 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-04 21:18:54,895 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-04 21:18:54,897 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 04 21:18:54
2015-12-04 21:18:54,898 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-04 21:18:54,899 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-04 21:18:54,900 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-04 21:18:54,900 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-04 21:18:54,940 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-04 21:18:54,940 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-04 21:18:54,940 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-04 21:18:54,940 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-04 21:18:54,940 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-04 21:18:54,940 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-04 21:18:54,941 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-04 21:18:54,941 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-04 21:18:54,941 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-04 21:18:54,941 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-04 21:18:54,941 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-04 21:18:54,941 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-04 21:18:54,941 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-04 21:18:54,943 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-04 21:18:55,107 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-04 21:18:55,107 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-04 21:18:55,107 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-04 21:18:55,108 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-04 21:18:55,110 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-04 21:18:55,122 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-04 21:18:55,122 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-04 21:18:55,122 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-04 21:18:55,122 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-04 21:18:55,123 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-04 21:18:55,123 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-04 21:18:55,124 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-04 21:18:55,125 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-04 21:18:55,125 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-04 21:18:55,125 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-04 21:18:55,136 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-04 21:18:55,210 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-04 21:18:55,214 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-04 21:18:55,222 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-04 21:18:55,224 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-04 21:18:55,224 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-04 21:18:55,224 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-04 21:18:55,239 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-04 21:18:55,240 INFO org.mortbay.log: jetty-6.1.26
2015-12-04 21:18:55,427 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-04 21:18:55,427 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-04 21:18:55,450 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-04 21:18:55,450 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-04 21:19:55,671 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2015-12-04 21:19:55,858 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=0&storageInfo=-60:288020609:0:CID-fc10af95-6416-4edd-9572-a006bee00a03
2015-12-04 21:19:55,894 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2015-12-04 21:19:56,196 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.03s at 0.00 KB/s
2015-12-04 21:19:56,196 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 356 bytes.
2015-12-04 21:19:56,203 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=1&endTxId=2&storageInfo=-60:288020609:0:CID-fc10af95-6416-4edd-9572-a006bee00a03
2015-12-04 21:19:56,209 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-04 21:19:56,209 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000001-0000000000000000002_0000000000008880763 size 0 bytes.
2015-12-04 21:19:56,268 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-04 21:19:56,314 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-04 21:19:56,314 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-parallels/dfs/namesecondary/current/fsimage_0000000000000000000
2015-12-04 21:19:56,314 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-04 21:19:56,322 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-04 21:19:56,327 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 expecting start txid #1
2015-12-04 21:19:56,327 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002
2015-12-04 21:19:56,352 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 of size 42 edits # 2 loaded in 0 seconds
2015-12-04 21:19:56,399 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-parallels/dfs/namesecondary
2015-12-04 21:19:56,436 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 2 to namenode at http://localhost:50070 in 0.03 seconds
2015-12-04 21:19:56,436 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 356
2015-12-04 22:19:56,925 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-04 22:19:56,926 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=3&endTxId=361&storageInfo=-60:288020609:0:CID-fc10af95-6416-4edd-9572-a006bee00a03
2015-12-04 22:19:56,935 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 6166.67 KB/s
2015-12-04 22:19:56,935 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000003-0000000000000000361_0000000000012481486 size 0 bytes.
2015-12-04 22:19:56,936 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-04 22:19:56,936 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000003-0000000000000000361 expecting start txid #3
2015-12-04 22:19:56,936 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000003-0000000000000000361
2015-12-04 22:19:57,009 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000003-0000000000000000361 of size 38752 edits # 359 loaded in 0 seconds
2015-12-04 22:19:57,026 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2015-12-04 22:19:57,027 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-parallels/dfs/namesecondary/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2015-12-04 22:19:57,043 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 361 to namenode at http://localhost:50070 in 0.011 seconds
2015-12-04 22:19:57,044 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 2800
2015-12-04 22:32:23,750 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-04 22:32:23,768 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 13:07:20,982 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 13:07:20,991 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 13:07:21,472 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 13:07:21,527 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 13:07:21,528 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-05 13:07:21,743 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-parallels/dfs/namesecondary/in_use.lock acquired by nodename 17714@ubuntu
2015-12-05 13:07:21,746 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 13:07:21,752 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 13:07:21,789 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 13:07:21,789 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 13:07:21,790 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 13:07:21,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 13:07:21
2015-12-05 13:07:21,793 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 13:07:21,793 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:07:21,795 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 13:07:21,795 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 13:07:21,832 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 13:07:21,832 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 13:07:21,832 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 13:07:21,832 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 13:07:21,832 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 13:07:21,832 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 13:07:21,832 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 13:07:21,832 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 13:07:21,832 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 13:07:21,833 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 13:07:21,833 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 13:07:21,833 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 13:07:21,833 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 13:07:21,835 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 13:07:22,009 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 13:07:22,009 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:07:22,009 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 13:07:22,009 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 13:07:22,012 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 13:07:22,025 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 13:07:22,025 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:07:22,026 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 13:07:22,026 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 13:07:22,027 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 13:07:22,027 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 13:07:22,027 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 13:07:22,029 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 13:07:22,029 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 13:07:22,029 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 13:07:22,042 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-05 13:07:22,120 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 13:07:22,124 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-05 13:07:22,134 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 13:07:22,137 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-05 13:07:22,137 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 13:07:22,137 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 13:07:22,154 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-05 13:07:22,155 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 13:07:22,338 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-05 13:07:22,338 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-05 13:07:22,367 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-05 13:07:22,367 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-05 13:08:23,409 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:24,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:25,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:26,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:27,414 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:28,415 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:29,417 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:30,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:31,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:32,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:32,425 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 13:09:33,430 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:34,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:35,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:36,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:37,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:38,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:39,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:40,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:41,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:42,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:42,440 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 13:10:43,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:44,446 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:45,448 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:46,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:47,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:48,452 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:49,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:50,456 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:51,457 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:52,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:52,460 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 13:11:53,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:54,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:55,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:56,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:57,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:58,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:59,472 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:00,473 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:01,474 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:02,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:02,477 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 13:13:03,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:04,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:05,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:06,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:07,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:08,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:09,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:10,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:11,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:12,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:12,492 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 13:14:13,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:14,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:15,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:16,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:17,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:18,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:19,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:20,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:21,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:22,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:22,510 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 13:15:23,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:24,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:25,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:26,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:27,517 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:28,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:29,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:30,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:31,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:32,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:32,527 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 13:16:32,685 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2015-12-05 13:16:32,919 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=0&storageInfo=-60:889546093:0:CID-8e6ab86f-c97b-4c9f-9515-70a87b98ae37
2015-12-05 13:16:32,957 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2015-12-05 13:16:33,334 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.03s at 0.00 KB/s
2015-12-05 13:16:33,334 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 356 bytes.
2015-12-05 13:16:33,347 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=1&endTxId=2&storageInfo=-60:889546093:0:CID-8e6ab86f-c97b-4c9f-9515-70a87b98ae37
2015-12-05 13:16:33,351 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-05 13:16:33,351 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000001-0000000000000000002_0000000000004769476 size 0 bytes.
2015-12-05 13:16:33,392 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-05 13:16:33,422 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-05 13:16:33,422 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-parallels/dfs/namesecondary/current/fsimage_0000000000000000000
2015-12-05 13:16:33,423 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-05 13:16:33,433 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-05 13:16:33,438 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 expecting start txid #1
2015-12-05 13:16:33,438 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002
2015-12-05 13:16:33,463 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 of size 42 edits # 2 loaded in 0 seconds
2015-12-05 13:16:33,513 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-parallels/dfs/namesecondary
2015-12-05 13:16:33,555 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 2 to namenode at http://localhost:50070 in 0.03 seconds
2015-12-05 13:16:33,555 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 356
2015-12-05 13:17:34,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:17:35,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:17:36,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:17:37,560 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:17:38,219 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 13:17:38,221 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 13:18:23,968 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 13:18:23,977 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 13:18:24,505 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 13:18:24,562 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 13:18:24,562 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-05 13:18:24,738 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-parallels/dfs/namesecondary/in_use.lock acquired by nodename 21468@ubuntu
2015-12-05 13:18:24,782 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 13:18:24,795 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 13:18:24,821 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 13:18:24,821 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 13:18:24,823 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 13:18:24,824 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 13:18:24
2015-12-05 13:18:24,826 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 13:18:24,826 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:18:24,827 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 13:18:24,827 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 13:18:24,857 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 13:18:24,857 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 13:18:24,857 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 13:18:24,857 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 13:18:24,857 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 13:18:24,857 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 13:18:24,858 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 13:18:24,858 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 13:18:24,858 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 13:18:24,860 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 13:18:24,860 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 13:18:24,860 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 13:18:24,860 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 13:18:24,861 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 13:18:25,000 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 13:18:25,000 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:18:25,000 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 13:18:25,000 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 13:18:25,003 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 13:18:25,013 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 13:18:25,013 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:18:25,013 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 13:18:25,013 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 13:18:25,015 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 13:18:25,015 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 13:18:25,015 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 13:18:25,016 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 13:18:25,016 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 13:18:25,016 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 13:18:25,027 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-05 13:18:25,100 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 13:18:25,103 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-05 13:18:25,112 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 13:18:25,116 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-05 13:18:25,116 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 13:18:25,116 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 13:18:25,129 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-05 13:18:25,129 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 13:18:25,286 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-05 13:18:25,286 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-05 13:18:25,318 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-05 13:18:25,319 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-05 13:19:25,494 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2015-12-05 13:19:25,684 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=2&storageInfo=-60:889546093:0:CID-8e6ab86f-c97b-4c9f-9515-70a87b98ae37
2015-12-05 13:19:25,722 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2015-12-05 13:19:25,903 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-05 13:19:25,903 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 356 bytes.
2015-12-05 13:19:25,910 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=3&endTxId=3&storageInfo=-60:889546093:0:CID-8e6ab86f-c97b-4c9f-9515-70a87b98ae37
2015-12-05 13:19:25,920 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 204800.00 KB/s
2015-12-05 13:19:25,920 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000003-0000000000000000003_0000000000004942039 size 0 bytes.
2015-12-05 13:19:25,920 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=4&endTxId=5&storageInfo=-60:889546093:0:CID-8e6ab86f-c97b-4c9f-9515-70a87b98ae37
2015-12-05 13:19:25,924 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-05 13:19:25,924 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000004-0000000000000000005_0000000000004942049 size 0 bytes.
2015-12-05 13:19:25,951 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-05 13:19:25,979 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-05 13:19:25,980 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 2 from /tmp/hadoop-parallels/dfs/namesecondary/current/fsimage_0000000000000000002
2015-12-05 13:19:25,980 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-05 13:19:25,993 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 2 stream(s).
2015-12-05 13:19:25,997 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000003-0000000000000000003 expecting start txid #3
2015-12-05 13:19:25,997 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000003-0000000000000000003
2015-12-05 13:19:26,015 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000003-0000000000000000003 of size 1048576 edits # 1 loaded in 0 seconds
2015-12-05 13:19:26,015 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000004-0000000000000000005 expecting start txid #4
2015-12-05 13:19:26,015 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000004-0000000000000000005
2015-12-05 13:19:26,015 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000004-0000000000000000005 of size 42 edits # 2 loaded in 0 seconds
2015-12-05 13:19:26,058 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2015-12-05 13:19:26,059 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-parallels/dfs/namesecondary/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2015-12-05 13:19:26,090 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 5 to namenode at http://localhost:50070 in 0.024 seconds
2015-12-05 13:19:26,090 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 356
2015-12-05 13:28:29,277 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:30,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:31,359 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:32,359 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:33,360 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:34,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:35,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:36,369 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:37,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:38,373 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:39,502 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 13:29:40,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:41,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:42,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:43,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:44,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:45,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:46,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:47,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:48,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:49,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:49,707 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 13:30:50,721 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:51,722 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:52,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:53,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:54,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:55,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:56,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:57,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:58,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:59,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:59,734 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 13:32:00,741 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:32:01,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:32:02,743 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:32:03,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:32:04,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:32:05,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:32:06,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:32:07,654 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 13:32:07,717 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 18:29:36,442 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 18:29:36,451 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 18:29:36,929 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 18:29:37,023 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 18:29:37,023 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-05 18:29:37,256 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-parallels/dfs/namesecondary/in_use.lock acquired by nodename 31354@ubuntu
2015-12-05 18:29:37,260 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 18:29:37,266 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 18:29:37,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 18:29:37,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 18:29:37,300 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 18:29:37,302 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 18:29:37
2015-12-05 18:29:37,304 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 18:29:37,304 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 18:29:37,305 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 18:29:37,306 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 18:29:37,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 18:29:37,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 18:29:37,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 18:29:37,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 18:29:37,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 18:29:37,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 18:29:37,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 18:29:37,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 18:29:37,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 18:29:37,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 18:29:37,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 18:29:37,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 18:29:37,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 18:29:37,444 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 18:29:37,621 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 18:29:37,621 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 18:29:37,621 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 18:29:37,622 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 18:29:37,624 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 18:29:37,637 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 18:29:37,638 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 18:29:37,638 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 18:29:37,638 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 18:29:37,639 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 18:29:37,639 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 18:29:37,639 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 18:29:37,641 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 18:29:37,641 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 18:29:37,641 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 18:29:37,655 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-05 18:29:37,726 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 18:29:37,729 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-05 18:29:37,738 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 18:29:37,739 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-05 18:29:37,740 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 18:29:37,740 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 18:29:37,760 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-05 18:29:37,760 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 18:29:37,937 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-05 18:29:37,937 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-05 18:29:37,955 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-05 18:29:37,955 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-05 18:30:38,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:40,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:41,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:42,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:43,005 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:44,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:45,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:46,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:47,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:48,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:48,016 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 18:31:44,666 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 18:31:44,668 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 18:53:28,225 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 18:53:28,232 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 18:53:28,719 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 18:53:28,785 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 18:53:28,785 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-05 18:53:28,994 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-parallels/dfs/namesecondary/in_use.lock acquired by nodename 4906@ubuntu
2015-12-05 18:53:28,997 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 18:53:29,003 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 18:53:29,032 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 18:53:29,033 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 18:53:29,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 18:53:29,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 18:53:29
2015-12-05 18:53:29,037 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 18:53:29,037 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 18:53:29,038 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 18:53:29,038 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 18:53:29,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 18:53:29,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 18:53:29,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 18:53:29,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 18:53:29,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 18:53:29,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 18:53:29,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 18:53:29,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 18:53:29,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 18:53:29,067 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 18:53:29,067 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 18:53:29,067 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 18:53:29,067 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 18:53:29,069 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 18:53:29,210 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 18:53:29,210 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 18:53:29,211 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 18:53:29,211 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 18:53:29,213 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 18:53:29,224 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 18:53:29,224 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 18:53:29,224 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 18:53:29,224 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 18:53:29,225 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 18:53:29,225 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 18:53:29,225 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 18:53:29,227 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 18:53:29,227 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 18:53:29,227 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 18:53:29,238 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-05 18:53:29,303 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 18:53:29,307 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-05 18:53:29,315 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 18:53:29,317 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-05 18:53:29,317 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 18:53:29,317 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 18:53:29,332 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-05 18:53:29,332 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 18:53:29,523 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-05 18:53:29,524 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-05 18:53:29,544 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-05 18:53:29,544 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-05 18:54:30,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:31,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:32,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:33,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:34,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:35,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:36,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:37,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:38,698 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:39,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:39,708 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 18:55:40,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:41,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:42,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:43,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:44,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:45,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:46,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:47,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:48,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:49,736 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:49,737 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 18:56:50,741 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:51,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:52,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:53,745 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:54,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:55,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:56,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:57,751 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:58,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:59,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:59,755 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 18:58:00,759 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:01,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:02,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:03,765 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:04,766 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:05,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:06,768 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:07,769 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:08,770 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:09,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:09,774 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 18:59:10,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:11,781 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:12,783 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:13,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:14,785 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:15,786 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:16,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:17,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:18,790 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:19,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:19,854 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 19:00:20,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:21,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:22,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:23,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:24,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:25,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:26,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:27,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:28,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:29,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:29,884 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 19:01:30,888 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:31,889 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:32,890 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:33,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:34,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:35,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:36,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:37,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:38,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:39,900 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:39,901 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 19:02:40,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:41,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:42,907 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:43,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:44,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:45,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:46,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:47,914 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:48,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:49,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:49,917 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 19:03:50,921 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:51,922 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:52,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:53,924 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:54,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:55,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:56,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:57,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:58,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:59,933 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:59,934 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 19:05:00,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:01,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:02,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:03,939 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:04,940 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:05,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:06,943 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:07,944 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:08,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:09,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:09,948 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 19:06:10,952 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:11,953 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:12,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:13,955 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:14,956 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:15,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:16,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:17,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:18,961 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:19,961 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:19,962 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 19:07:20,965 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:21,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:22,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:23,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:24,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:25,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:26,976 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:27,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:28,979 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:29,980 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:29,981 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 18 more
2015-12-05 19:08:30,254 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2015-12-05 19:08:30,548 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=0&storageInfo=-60:208897031:0:CID-4c805491-133b-45fe-ad2f-46e6c9c45a4a
2015-12-05 19:08:30,581 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2015-12-05 19:08:30,887 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2015-12-05 19:08:30,888 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 355 bytes.
2015-12-05 19:08:30,897 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=1&endTxId=2&storageInfo=-60:208897031:0:CID-4c805491-133b-45fe-ad2f-46e6c9c45a4a
2015-12-05 19:08:30,902 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-05 19:08:30,902 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000001-0000000000000000002_0000000000011487486 size 0 bytes.
2015-12-05 19:08:30,948 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-05 19:08:30,974 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-05 19:08:30,975 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-parallels/dfs/namesecondary/current/fsimage_0000000000000000000
2015-12-05 19:08:30,975 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-05 19:08:30,980 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-05 19:08:30,984 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 expecting start txid #1
2015-12-05 19:08:30,984 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002
2015-12-05 19:08:31,016 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 of size 42 edits # 2 loaded in 0 seconds
2015-12-05 19:08:31,105 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-parallels/dfs/namesecondary
2015-12-05 19:08:31,161 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 2 to namenode at http://localhost:50070 in 0.041 seconds
2015-12-05 19:08:31,161 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 355
2015-12-05 19:21:50,484 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 19:21:50,485 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 19:22:46,734 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 19:22:46,744 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 19:22:47,319 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 19:22:47,398 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 19:22:47,400 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-05 19:22:47,579 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-parallels/dfs/namesecondary/in_use.lock acquired by nodename 12788@ubuntu
2015-12-05 19:22:47,628 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 19:22:47,637 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 19:22:47,684 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 19:22:47,684 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 19:22:47,685 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 19:22:47,687 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 19:22:47
2015-12-05 19:22:47,689 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 19:22:47,689 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 19:22:47,691 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 19:22:47,692 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 19:22:47,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 19:22:47,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 19:22:47,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 19:22:47,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 19:22:47,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 19:22:47,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 19:22:47,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 19:22:47,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 19:22:47,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 19:22:47,729 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 19:22:47,729 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 19:22:47,729 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 19:22:47,729 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 19:22:47,731 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 19:22:47,890 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 19:22:47,890 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 19:22:47,890 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 19:22:47,890 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 19:22:47,893 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 19:22:47,903 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 19:22:47,903 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 19:22:47,903 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 19:22:47,903 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 19:22:47,904 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 19:22:47,905 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 19:22:47,905 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 19:22:47,906 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 19:22:47,906 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 19:22:47,906 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 19:22:47,923 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-05 19:22:47,997 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 19:22:48,001 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-05 19:22:48,016 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 19:22:48,019 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-05 19:22:48,019 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 19:22:48,020 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 19:22:48,040 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-05 19:22:48,040 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 19:22:48,214 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-05 19:22:48,214 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-05 19:22:48,234 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-05 19:22:48,235 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-05 19:23:48,365 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2015-12-05 19:23:48,478 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=2&storageInfo=-60:208897031:0:CID-4c805491-133b-45fe-ad2f-46e6c9c45a4a
2015-12-05 19:23:48,518 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2015-12-05 19:23:48,720 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-05 19:23:48,720 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 355 bytes.
2015-12-05 19:23:48,725 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=3&endTxId=7&storageInfo=-60:208897031:0:CID-4c805491-133b-45fe-ad2f-46e6c9c45a4a
2015-12-05 19:23:48,737 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 204800.00 KB/s
2015-12-05 19:23:48,737 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000003-0000000000000000007_0000000000012405314 size 0 bytes.
2015-12-05 19:23:48,738 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=8&endTxId=9&storageInfo=-60:208897031:0:CID-4c805491-133b-45fe-ad2f-46e6c9c45a4a
2015-12-05 19:23:48,741 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-05 19:23:48,741 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000008-0000000000000000009_0000000000012405326 size 0 bytes.
2015-12-05 19:23:48,769 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-05 19:23:48,792 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-05 19:23:48,792 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 2 from /tmp/hadoop-parallels/dfs/namesecondary/current/fsimage_0000000000000000002
2015-12-05 19:23:48,792 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-05 19:23:48,803 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 2 stream(s).
2015-12-05 19:23:48,807 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000003-0000000000000000007 expecting start txid #3
2015-12-05 19:23:48,807 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000003-0000000000000000007
2015-12-05 19:23:48,836 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000003-0000000000000000007 of size 1048576 edits # 5 loaded in 0 seconds
2015-12-05 19:23:48,836 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000008-0000000000000000009 expecting start txid #8
2015-12-05 19:23:48,836 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000008-0000000000000000009
2015-12-05 19:23:48,837 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-parallels/dfs/namesecondary/current/edits_0000000000000000008-0000000000000000009 of size 42 edits # 2 loaded in 0 seconds
2015-12-05 19:23:48,882 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2015-12-05 19:23:48,883 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-parallels/dfs/namesecondary/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2015-12-05 19:23:48,911 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 9 to namenode at http://localhost:50070 in 0.022 seconds
2015-12-05 19:23:48,911 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 622
2015-12-05 20:12:50,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:51,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:52,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:53,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:54,315 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:55,317 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:56,317 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:57,318 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:58,319 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:59,320 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:59,336 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 17 more
2015-12-05 20:13:00,095 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 20:13:00,098 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 20:16:46,020 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 20:16:46,032 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 20:16:46,535 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 20:16:46,600 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 20:16:46,600 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-05 20:16:46,834 WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory /usr/local/hadoop/tmp/dfs/namesecondary does not exist
2015-12-05 20:16:46,836 FATAL org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Failed to start secondary namenode
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /usr/local/hadoop/tmp/dfs/namesecondary is in an inconsistent state: checkpoint directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.recoverCreate(SecondaryNameNode.java:967)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:243)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:192)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:671)
2015-12-05 20:16:46,838 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 20:16:46,839 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 20:25:12,719 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 20:25:12,734 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 20:25:13,215 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 20:25:13,279 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 20:25:13,279 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-05 20:25:13,440 WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory /usr/local/hadoop/tmp/dfs/namesecondary does not exist
2015-12-05 20:25:13,442 FATAL org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Failed to start secondary namenode
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /usr/local/hadoop/tmp/dfs/namesecondary is in an inconsistent state: checkpoint directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.recoverCreate(SecondaryNameNode.java:967)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:243)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:192)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:671)
2015-12-05 20:25:13,444 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 20:25:13,445 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 20:35:11,700 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 20:35:11,709 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 20:35:12,225 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 20:35:12,288 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 20:35:12,289 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-05 20:35:12,454 WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory /usr/local/hadoop/tmp/dfs/namesecondary does not exist
2015-12-05 20:35:12,456 FATAL org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Failed to start secondary namenode
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /usr/local/hadoop/tmp/dfs/namesecondary is in an inconsistent state: checkpoint directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.recoverCreate(SecondaryNameNode.java:967)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:243)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:192)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:671)
2015-12-05 20:35:12,458 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 20:35:12,459 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 20:46:00,971 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 20:46:00,981 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 20:46:01,453 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 20:46:01,507 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 20:46:01,507 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-05 20:46:01,665 WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory /usr/local/hadoop/tmp/dfs/namesecondary does not exist
2015-12-05 20:46:01,667 FATAL org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Failed to start secondary namenode
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /usr/local/hadoop/tmp/dfs/namesecondary is in an inconsistent state: checkpoint directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.recoverCreate(SecondaryNameNode.java:967)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:243)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:192)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:671)
2015-12-05 20:46:01,670 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 20:46:01,671 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 20:49:35,458 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 20:49:35,466 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 20:49:35,979 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 20:49:36,039 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 20:49:36,039 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-05 20:49:36,201 WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory /usr/local/hadoop/tmp/dfs/namesecondary does not exist
2015-12-05 20:49:36,203 FATAL org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Failed to start secondary namenode
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /usr/local/hadoop/tmp/dfs/namesecondary is in an inconsistent state: checkpoint directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.recoverCreate(SecondaryNameNode.java:967)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:243)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:192)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:671)
2015-12-05 20:49:36,205 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 20:49:36,206 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 21:13:49,378 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 21:13:49,386 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 21:13:49,935 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 21:13:49,993 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 21:13:49,993 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-05 21:13:50,190 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/namesecondary/in_use.lock acquired by nodename 19758@ubuntu
2015-12-05 21:13:50,194 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 21:13:50,200 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 21:13:50,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 21:13:50,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 21:13:50,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 21:13:50,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 21:13:50
2015-12-05 21:13:50,234 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 21:13:50,234 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 21:13:50,235 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 21:13:50,235 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 21:13:50,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 21:13:50,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 21:13:50,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 21:13:50,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 21:13:50,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 21:13:50,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 21:13:50,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 21:13:50,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 21:13:50,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 21:13:50,278 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 21:13:50,278 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 21:13:50,278 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 21:13:50,278 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 21:13:50,280 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 21:13:50,434 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 21:13:50,434 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 21:13:50,435 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 21:13:50,435 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 21:13:50,437 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 21:13:50,449 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 21:13:50,449 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 21:13:50,449 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 21:13:50,449 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 21:13:50,451 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 21:13:50,451 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 21:13:50,451 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 21:13:50,453 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 21:13:50,453 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 21:13:50,453 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 21:13:50,465 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-05 21:13:50,536 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 21:13:50,539 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-05 21:13:50,547 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 21:13:50,549 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-05 21:13:50,549 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 21:13:50,549 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 21:13:50,564 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-05 21:13:50,564 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 21:13:50,806 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-05 21:13:50,806 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-05 21:13:50,838 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-05 21:13:50,838 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-05 21:14:51,054 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2015-12-05 21:14:51,282 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=0&storageInfo=-60:1710165876:0:CID-64431893-233f-4618-b6db-9870599dcfca
2015-12-05 21:14:51,334 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2015-12-05 21:14:51,576 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2015-12-05 21:14:51,576 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 356 bytes.
2015-12-05 21:14:51,582 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=1&endTxId=2&storageInfo=-60:1710165876:0:CID-64431893-233f-4618-b6db-9870599dcfca
2015-12-05 21:14:51,587 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-05 21:14:51,587 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000001-0000000000000000002_0000000000003688606 size 0 bytes.
2015-12-05 21:14:51,621 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-05 21:14:51,673 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-05 21:14:51,673 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000000000
2015-12-05 21:14:51,673 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-05 21:14:51,679 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-05 21:14:51,683 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 expecting start txid #1
2015-12-05 21:14:51,683 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002
2015-12-05 21:14:51,709 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 of size 42 edits # 2 loaded in 0 seconds
2015-12-05 21:14:51,759 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /home/parallels/hadoop/tmp/dfs/namesecondary
2015-12-05 21:14:51,788 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 2 to namenode at http://localhost:50070 in 0.022 seconds
2015-12-05 21:14:51,788 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 356
2015-12-05 21:20:38,647 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 21:20:38,650 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 21:22:33,511 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 21:22:33,529 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 21:22:34,106 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 21:22:34,202 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 21:22:34,202 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-05 21:22:34,377 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/namesecondary/in_use.lock acquired by nodename 3432@ubuntu
2015-12-05 21:22:34,430 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 21:22:34,439 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 21:22:34,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 21:22:34,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 21:22:34,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 21:22:34,470 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 21:22:34
2015-12-05 21:22:34,472 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 21:22:34,472 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 21:22:34,473 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 21:22:34,473 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 21:22:34,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 21:22:34,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 21:22:34,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 21:22:34,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 21:22:34,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 21:22:34,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 21:22:34,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 21:22:34,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 21:22:34,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 21:22:34,512 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 21:22:34,512 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 21:22:34,512 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 21:22:34,512 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 21:22:34,514 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 21:22:34,812 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 21:22:34,812 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 21:22:34,813 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 21:22:34,813 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 21:22:34,819 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 21:22:34,835 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 21:22:34,835 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 21:22:34,836 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 21:22:34,836 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 21:22:34,838 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 21:22:34,838 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 21:22:34,838 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 21:22:34,840 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 21:22:34,840 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 21:22:34,841 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 21:22:34,860 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-05 21:22:34,970 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 21:22:34,976 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-05 21:22:35,000 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 21:22:35,006 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-05 21:22:35,006 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 21:22:35,007 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 21:22:35,047 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-05 21:22:35,047 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 21:22:35,316 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-05 21:22:35,316 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-05 21:22:35,331 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-05 21:22:35,331 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-05 22:21:36,272 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2015-12-05 22:21:36,599 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=2&storageInfo=-60:1710165876:0:CID-64431893-233f-4618-b6db-9870599dcfca
2015-12-05 22:21:36,630 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2015-12-05 22:21:36,896 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2015-12-05 22:21:36,897 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 356 bytes.
2015-12-05 22:21:36,909 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=3&endTxId=18&storageInfo=-60:1710165876:0:CID-64431893-233f-4618-b6db-9870599dcfca
2015-12-05 22:21:36,926 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 113777.78 KB/s
2015-12-05 22:21:36,927 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000003-0000000000000000018_0000000000003621959 size 0 bytes.
2015-12-05 22:21:36,927 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=19&endTxId=35&storageInfo=-60:1710165876:0:CID-64431893-233f-4618-b6db-9870599dcfca
2015-12-05 22:21:36,931 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 1000.00 KB/s
2015-12-05 22:21:36,931 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000019-0000000000000000035_0000000000003621978 size 0 bytes.
2015-12-05 22:21:37,005 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-05 22:21:37,052 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-05 22:21:37,052 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 2 from /home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000000002
2015-12-05 22:21:37,053 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-05 22:21:37,063 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 2 stream(s).
2015-12-05 22:21:37,068 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000003-0000000000000000018 expecting start txid #3
2015-12-05 22:21:37,068 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000003-0000000000000000018
2015-12-05 22:21:37,153 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000003-0000000000000000018 of size 1048576 edits # 16 loaded in 0 seconds
2015-12-05 22:21:37,153 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000019-0000000000000000035 expecting start txid #19
2015-12-05 22:21:37,153 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000019-0000000000000000035
2015-12-05 22:21:37,164 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000019-0000000000000000035 of size 1570 edits # 17 loaded in 0 seconds
2015-12-05 22:21:37,234 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2015-12-05 22:21:37,234 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2015-12-05 22:21:37,293 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 35 to namenode at http://localhost:50070 in 0.044 seconds
2015-12-05 22:21:37,294 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 770
2015-12-05 22:34:55,866 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 22:34:55,870 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 22:42:22,683 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 22:42:22,692 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 22:42:23,157 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 22:42:23,249 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 22:42:23,249 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-05 22:42:23,451 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/namesecondary/in_use.lock acquired by nodename 20844@ubuntu
2015-12-05 22:42:23,513 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 22:42:23,525 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 22:42:23,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 22:42:23,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 22:42:23,572 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 22:42:23,573 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 22:42:23
2015-12-05 22:42:23,575 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 22:42:23,575 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 22:42:23,576 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 22:42:23,576 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 22:42:23,607 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 22:42:23,607 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 22:42:23,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 22:42:23,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 22:42:23,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 22:42:23,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 22:42:23,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 22:42:23,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 22:42:23,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 22:42:23,611 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 22:42:23,611 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 22:42:23,611 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 22:42:23,612 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 22:42:23,614 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 22:42:23,759 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 22:42:23,759 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 22:42:23,760 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 22:42:23,760 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 22:42:23,762 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 22:42:23,772 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 22:42:23,772 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 22:42:23,772 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 22:42:23,772 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 22:42:23,774 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 22:42:23,774 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 22:42:23,774 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 22:42:23,775 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 22:42:23,775 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 22:42:23,776 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 22:42:23,786 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-05 22:42:23,857 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 22:42:23,860 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-05 22:42:23,868 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 22:42:23,869 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-05 22:42:23,870 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 22:42:23,870 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 22:42:23,883 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-05 22:42:23,883 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 22:42:24,055 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-05 22:42:24,055 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-05 22:42:24,071 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-05 22:42:24,071 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-05 22:43:24,275 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2015-12-05 22:43:24,476 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=35&storageInfo=-60:1710165876:0:CID-64431893-233f-4618-b6db-9870599dcfca
2015-12-05 22:43:24,508 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2015-12-05 22:43:24,885 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2015-12-05 22:43:24,885 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000035 size 770 bytes.
2015-12-05 22:43:24,891 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=36&endTxId=41&storageInfo=-60:1710165876:0:CID-64431893-233f-4618-b6db-9870599dcfca
2015-12-05 22:43:24,912 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 73142.86 KB/s
2015-12-05 22:43:24,912 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000036-0000000000000000041_0000000000004929941 size 0 bytes.
2015-12-05 22:43:24,913 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=42&endTxId=43&storageInfo=-60:1710165876:0:CID-64431893-233f-4618-b6db-9870599dcfca
2015-12-05 22:43:24,918 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-05 22:43:24,918 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000042-0000000000000000043_0000000000004929963 size 0 bytes.
2015-12-05 22:43:24,950 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 7 INodes.
2015-12-05 22:43:24,998 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-05 22:43:24,998 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 35 from /home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000000035
2015-12-05 22:43:25,000 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-05 22:43:25,012 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 2 stream(s).
2015-12-05 22:43:25,018 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000036-0000000000000000041 expecting start txid #36
2015-12-05 22:43:25,018 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000036-0000000000000000041
2015-12-05 22:43:25,061 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000036-0000000000000000041 of size 1048576 edits # 6 loaded in 0 seconds
2015-12-05 22:43:25,061 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000042-0000000000000000043 expecting start txid #42
2015-12-05 22:43:25,061 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000042-0000000000000000043
2015-12-05 22:43:25,061 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000042-0000000000000000043 of size 42 edits # 2 loaded in 0 seconds
2015-12-05 22:43:25,119 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 35
2015-12-05 22:43:25,119 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000000002, cpktTxId=0000000000000000002)
2015-12-05 22:43:25,159 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 43 to namenode at http://localhost:50070 in 0.032 seconds
2015-12-05 22:43:25,159 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1096
2015-12-05 22:49:54,405 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 22:49:54,466 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-06 18:11:55,631 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-06 18:11:55,640 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-06 18:11:56,350 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-06 18:11:56,413 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-06 18:11:56,413 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-06 18:11:56,659 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/namesecondary/in_use.lock acquired by nodename 3782@ubuntu
2015-12-06 18:11:56,712 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-06 18:11:56,718 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-06 18:11:56,751 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-06 18:11:56,751 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-06 18:11:56,752 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-06 18:11:56,753 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 06 18:11:56
2015-12-06 18:11:56,755 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-06 18:11:56,755 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:11:56,756 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-06 18:11:56,756 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-06 18:11:56,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-06 18:11:56,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-06 18:11:56,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-06 18:11:56,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-06 18:11:56,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-06 18:11:56,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-06 18:11:56,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-06 18:11:56,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-06 18:11:56,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-06 18:11:56,793 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-06 18:11:56,793 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-06 18:11:56,793 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-06 18:11:56,793 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-06 18:11:56,795 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-06 18:11:56,998 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-06 18:11:56,998 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:11:56,998 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-06 18:11:56,998 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-06 18:11:57,011 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-06 18:11:57,023 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-06 18:11:57,023 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:11:57,024 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-06 18:11:57,024 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-06 18:11:57,025 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-06 18:11:57,026 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-06 18:11:57,026 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-06 18:11:57,028 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-06 18:11:57,028 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-06 18:11:57,028 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-06 18:11:57,052 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-06 18:11:57,155 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-06 18:11:57,160 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-06 18:11:57,182 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-06 18:11:57,185 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-06 18:11:57,185 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-06 18:11:57,185 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-06 18:11:57,203 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-06 18:11:57,203 INFO org.mortbay.log: jetty-6.1.26
2015-12-06 18:11:57,566 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-06 18:11:57,566 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-06 18:11:57,586 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-06 18:11:57,587 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-06 18:28:57,465 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-06 18:28:57,468 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-06 18:29:22,453 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-06 18:29:22,464 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-06 18:29:23,157 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-06 18:29:23,225 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-06 18:29:23,225 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-06 18:29:23,414 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/namesecondary/in_use.lock acquired by nodename 8251@ubuntu
2015-12-06 18:29:23,478 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-06 18:29:23,484 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-06 18:29:23,525 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-06 18:29:23,525 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-06 18:29:23,526 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-06 18:29:23,527 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 06 18:29:23
2015-12-06 18:29:23,529 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-06 18:29:23,529 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:29:23,530 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-06 18:29:23,530 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-06 18:29:23,572 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-06 18:29:23,572 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-06 18:29:23,572 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-06 18:29:23,572 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-06 18:29:23,572 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-06 18:29:23,572 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-06 18:29:23,572 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-06 18:29:23,572 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-06 18:29:23,572 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-06 18:29:23,573 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-06 18:29:23,573 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-06 18:29:23,573 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-06 18:29:23,573 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-06 18:29:23,575 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-06 18:29:23,740 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-06 18:29:23,740 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:29:23,740 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-06 18:29:23,740 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-06 18:29:23,742 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-06 18:29:23,754 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-06 18:29:23,754 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:29:23,754 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-06 18:29:23,754 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-06 18:29:23,755 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-06 18:29:23,756 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-06 18:29:23,756 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-06 18:29:23,758 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-06 18:29:23,758 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-06 18:29:23,758 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-06 18:29:23,770 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-06 18:29:23,827 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-06 18:29:23,831 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-06 18:29:23,841 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-06 18:29:23,843 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-06 18:29:23,843 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-06 18:29:23,843 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-06 18:29:23,858 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-06 18:29:23,858 INFO org.mortbay.log: jetty-6.1.26
2015-12-06 18:29:24,116 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-06 18:29:24,116 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-06 18:29:24,142 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-06 18:29:24,142 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-06 18:30:57,824 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-06 18:30:57,826 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-06 18:33:28,938 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-06 18:33:28,945 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-06 18:33:29,630 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-06 18:33:29,691 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-06 18:33:29,691 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-06 18:33:29,885 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/namesecondary/in_use.lock acquired by nodename 10033@ubuntu
2015-12-06 18:33:29,957 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-06 18:33:29,964 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-06 18:33:30,001 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-06 18:33:30,001 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-06 18:33:30,002 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-06 18:33:30,003 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 06 18:33:30
2015-12-06 18:33:30,005 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-06 18:33:30,005 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:33:30,006 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-06 18:33:30,006 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-06 18:33:30,046 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-06 18:33:30,046 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-06 18:33:30,046 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-06 18:33:30,046 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-06 18:33:30,046 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-06 18:33:30,046 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-06 18:33:30,046 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-06 18:33:30,046 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-06 18:33:30,047 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-06 18:33:30,047 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-06 18:33:30,047 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-06 18:33:30,047 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-06 18:33:30,047 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-06 18:33:30,049 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-06 18:33:30,235 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-06 18:33:30,235 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:33:30,235 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-06 18:33:30,236 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-06 18:33:30,238 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-06 18:33:30,249 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-06 18:33:30,249 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:33:30,249 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-06 18:33:30,249 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-06 18:33:30,251 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-06 18:33:30,251 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-06 18:33:30,251 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-06 18:33:30,253 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-06 18:33:30,253 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-06 18:33:30,253 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-06 18:33:30,265 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-06 18:33:30,327 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-06 18:33:30,331 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-06 18:33:30,342 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-06 18:33:30,345 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-06 18:33:30,345 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-06 18:33:30,345 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-06 18:33:30,361 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-06 18:33:30,361 INFO org.mortbay.log: jetty-6.1.26
2015-12-06 18:33:30,624 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-06 18:33:30,624 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-06 18:33:30,640 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-06 18:33:30,640 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-06 18:36:48,716 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-06 18:36:48,718 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-06 18:41:16,600 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-06 18:41:16,612 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-06 18:41:17,294 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-06 18:41:17,377 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-06 18:41:17,377 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-06 18:41:17,608 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/namesecondary/in_use.lock acquired by nodename 12493@ubuntu
2015-12-06 18:41:17,615 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-06 18:41:17,621 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-06 18:41:17,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-06 18:41:17,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-06 18:41:17,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-06 18:41:17,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 06 18:41:17
2015-12-06 18:41:17,659 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-06 18:41:17,659 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:41:17,661 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-06 18:41:17,661 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-06 18:41:17,703 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-06 18:41:17,703 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-06 18:41:17,703 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-06 18:41:17,703 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-06 18:41:17,703 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-06 18:41:17,703 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-06 18:41:17,703 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-06 18:41:17,703 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-06 18:41:17,703 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-06 18:41:17,704 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-06 18:41:17,704 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-06 18:41:17,704 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-06 18:41:17,704 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-06 18:41:17,706 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-06 18:41:17,880 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-06 18:41:17,880 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:41:17,880 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-06 18:41:17,880 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-06 18:41:17,882 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-06 18:41:17,918 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-06 18:41:17,918 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:41:17,918 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-06 18:41:17,918 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-06 18:41:17,921 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-06 18:41:17,921 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-06 18:41:17,921 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-06 18:41:17,924 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-06 18:41:17,924 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-06 18:41:17,925 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-06 18:41:17,940 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-06 18:41:18,012 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-06 18:41:18,016 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-06 18:41:18,041 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-06 18:41:18,043 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-06 18:41:18,043 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-06 18:41:18,043 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-06 18:41:18,061 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-06 18:41:18,061 INFO org.mortbay.log: jetty-6.1.26
2015-12-06 18:41:18,303 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-06 18:41:18,303 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-06 18:41:18,322 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-06 18:41:18,322 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-06 19:10:19,602 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2015-12-06 19:10:20,392 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=0&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-06 19:10:20,527 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2015-12-06 19:10:21,541 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.15s at 0.00 KB/s
2015-12-06 19:10:21,542 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 356 bytes.
2015-12-06 19:10:21,570 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=1&endTxId=114&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-06 19:10:21,577 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 6500.00 KB/s
2015-12-06 19:10:21,578 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000001-0000000000000000114_0000000000003661701 size 0 bytes.
2015-12-06 19:10:21,748 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-06 19:10:21,880 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-06 19:10:21,881 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000000000
2015-12-06 19:10:21,881 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-06 19:10:21,890 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-06 19:10:21,896 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000114 expecting start txid #1
2015-12-06 19:10:21,896 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000114
2015-12-06 19:10:22,053 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000114 of size 13972 edits # 114 loaded in 0 seconds
2015-12-06 19:10:22,132 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /home/parallels/hadoop/tmp/dfs/namesecondary
2015-12-06 19:10:22,179 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 114 to namenode at http://localhost:50070 in 0.038 seconds
2015-12-06 19:10:22,180 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1786
2015-12-06 19:29:08,823 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-06 19:29:08,941 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-10 17:08:52,203 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-10 17:08:52,215 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-10 17:08:52,964 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-10 17:08:53,062 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-10 17:08:53,062 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-10 17:08:53,408 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/namesecondary/in_use.lock acquired by nodename 27121@ubuntu
2015-12-10 17:08:53,466 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-10 17:08:53,478 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-10 17:08:53,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-10 17:08:53,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-10 17:08:53,531 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-10 17:08:53,532 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 10 17:08:53
2015-12-10 17:08:53,536 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-10 17:08:53,536 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-10 17:08:53,537 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-10 17:08:53,537 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-10 17:08:54,696 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-10 17:08:54,696 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-10 17:08:54,696 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-10 17:08:54,696 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-10 17:08:54,696 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-10 17:08:54,696 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-10 17:08:54,696 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-10 17:08:54,696 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-10 17:08:54,696 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-10 17:08:54,697 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-10 17:08:54,697 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-10 17:08:54,697 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-10 17:08:54,697 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-10 17:08:54,700 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-10 17:08:54,931 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-10 17:08:54,931 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-10 17:08:54,931 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-10 17:08:54,931 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-10 17:08:54,934 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-10 17:08:54,946 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-10 17:08:54,946 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-10 17:08:54,946 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-10 17:08:54,946 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-10 17:08:54,948 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-10 17:08:54,948 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-10 17:08:54,948 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-10 17:08:54,950 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-10 17:08:54,950 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-10 17:08:54,950 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-10 17:08:54,967 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-10 17:08:55,043 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-10 17:08:55,047 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-10 17:08:55,059 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-10 17:08:55,061 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-10 17:08:55,061 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-10 17:08:55,061 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-10 17:08:55,091 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-10 17:08:55,091 INFO org.mortbay.log: jetty-6.1.26
2015-12-10 17:08:55,413 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-10 17:08:55,413 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-10 17:08:55,444 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-10 17:08:55,444 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-10 17:09:56,025 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2015-12-10 17:09:56,615 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=115&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-10 17:09:56,686 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2015-12-10 17:09:57,248 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.08s at 13.16 KB/s
2015-12-10 17:09:57,248 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000115 size 1786 bytes.
2015-12-10 17:09:57,270 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=116&endTxId=117&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-10 17:09:57,278 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-10 17:09:57,279 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000116-0000000000000000117_0000000000028477501 size 0 bytes.
2015-12-10 17:09:57,364 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 20 INodes.
2015-12-10 17:09:57,420 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-10 17:09:57,420 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 115 from /home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000000115
2015-12-10 17:09:57,420 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-10 17:09:57,428 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-10 17:09:57,432 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000116-0000000000000000117 expecting start txid #116
2015-12-10 17:09:57,432 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000116-0000000000000000117
2015-12-10 17:09:57,486 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000116-0000000000000000117 of size 42 edits # 2 loaded in 0 seconds
2015-12-10 17:09:57,580 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 115
2015-12-10 17:09:57,580 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000000114, cpktTxId=0000000000000000114)
2015-12-10 17:09:57,581 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2015-12-10 17:09:57,633 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 117 to namenode at http://localhost:50070 in 0.036 seconds
2015-12-10 17:09:57,634 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1786
2015-12-10 17:34:52,553 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-10 17:34:52,594 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-10 21:55:45,771 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-10 21:55:45,785 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-10 21:55:46,689 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-10 21:55:46,854 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-10 21:55:46,854 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-10 21:55:47,150 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/namesecondary/in_use.lock acquired by nodename 26636@ubuntu
2015-12-10 21:55:47,224 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-10 21:55:47,235 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-10 21:55:47,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-10 21:55:47,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-10 21:55:47,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-10 21:55:47,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 10 21:55:47
2015-12-10 21:55:47,284 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-10 21:55:47,284 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-10 21:55:47,286 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-10 21:55:47,286 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-10 21:55:47,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-10 21:55:47,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-10 21:55:47,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-10 21:55:47,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-10 21:55:47,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-10 21:55:47,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-10 21:55:47,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-10 21:55:47,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-10 21:55:47,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-10 21:55:47,341 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-10 21:55:47,341 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-10 21:55:47,341 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-10 21:55:47,342 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-10 21:55:47,346 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-10 21:55:47,558 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-10 21:55:47,558 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-10 21:55:47,559 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-10 21:55:47,559 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-10 21:55:47,561 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-10 21:55:47,576 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-10 21:55:47,576 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-10 21:55:47,577 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-10 21:55:47,577 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-10 21:55:47,578 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-10 21:55:47,578 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-10 21:55:47,578 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-10 21:55:47,581 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-10 21:55:47,581 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-10 21:55:47,581 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-10 21:55:47,594 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-10 21:55:47,662 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-10 21:55:47,665 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-10 21:55:47,677 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-10 21:55:47,679 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-10 21:55:47,679 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-10 21:55:47,679 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-10 21:55:47,700 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-10 21:55:47,700 INFO org.mortbay.log: jetty-6.1.26
2015-12-10 21:55:48,047 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-10 21:55:48,047 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-10 21:55:48,069 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-10 21:55:48,069 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-10 21:56:48,534 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2015-12-10 21:56:49,289 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=403&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-10 21:56:49,430 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2015-12-10 21:56:49,794 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 250.00 KB/s
2015-12-10 21:56:49,794 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000403 size 3066 bytes.
2015-12-10 21:56:49,819 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=404&endTxId=434&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-10 21:56:49,841 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 214.29 KB/s
2015-12-10 21:56:49,841 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000404-0000000000000000434_0000000000037555689 size 0 bytes.
2015-12-10 21:56:49,943 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 31 INodes.
2015-12-10 21:56:50,062 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-10 21:56:50,063 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 403 from /home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000000403
2015-12-10 21:56:50,063 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-10 21:56:50,085 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-10 21:56:50,095 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000404-0000000000000000434 expecting start txid #404
2015-12-10 21:56:50,096 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000404-0000000000000000434
2015-12-10 21:56:50,272 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000404-0000000000000000434 of size 3251 edits # 31 loaded in 0 seconds
2015-12-10 21:56:50,362 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 403
2015-12-10 21:56:50,363 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000000117, cpktTxId=0000000000000000117)
2015-12-10 21:56:50,363 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000000115, cpktTxId=0000000000000000115)
2015-12-10 21:56:50,455 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 434 to namenode at http://localhost:50070 in 0.064 seconds
2015-12-10 21:56:50,456 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 3244
2015-12-10 23:11:43,953 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-10 23:11:43,959 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=435&endTxId=853&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-10 23:11:43,985 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 18333.33 KB/s
2015-12-10 23:11:43,985 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000435-0000000000000000853_0000000000041158855 size 0 bytes.
2015-12-10 23:11:43,987 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-10 23:11:43,988 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000435-0000000000000000853 expecting start txid #435
2015-12-10 23:11:43,989 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000435-0000000000000000853
2015-12-10 23:11:44,255 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000000435-0000000000000000853 of size 56620 edits # 419 loaded in 0 seconds
2015-12-10 23:11:44,389 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 434
2015-12-10 23:11:44,389 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000000403, cpktTxId=0000000000000000403)
2015-12-10 23:11:44,435 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 853 to namenode at http://localhost:50070 in 0.034 seconds
2015-12-10 23:11:44,436 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 5199
2015-12-11 00:04:47,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-11 00:04:48,563 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-11 00:04:48,586 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-11 10:06:46,212 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-11 10:06:46,219 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-11 10:06:46,881 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-11 10:06:46,939 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-11 10:06:46,939 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-11 10:06:47,152 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/namesecondary/in_use.lock acquired by nodename 305@ubuntu
2015-12-11 10:06:47,205 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-11 10:06:47,211 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-11 10:06:47,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-11 10:06:47,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-11 10:06:47,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-11 10:06:47,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 11 10:06:47
2015-12-11 10:06:47,248 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-11 10:06:47,248 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-11 10:06:47,249 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-11 10:06:47,249 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-11 10:06:47,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-11 10:06:47,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-11 10:06:47,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-11 10:06:47,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-11 10:06:47,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-11 10:06:47,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-11 10:06:47,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-11 10:06:47,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-11 10:06:47,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-11 10:06:47,291 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-11 10:06:47,291 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-11 10:06:47,291 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-11 10:06:47,291 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-11 10:06:47,293 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-11 10:06:47,459 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-11 10:06:47,459 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-11 10:06:47,459 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-11 10:06:47,459 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-11 10:06:47,462 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-11 10:06:47,476 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-11 10:06:47,476 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-11 10:06:47,476 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-11 10:06:47,476 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-11 10:06:47,478 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-11 10:06:47,478 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-11 10:06:47,478 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-11 10:06:47,480 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-11 10:06:47,480 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-11 10:06:47,480 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-11 10:06:47,491 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-11 10:06:47,540 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-11 10:06:47,543 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-11 10:06:47,556 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-11 10:06:47,558 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-11 10:06:47,558 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-11 10:06:47,558 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-11 10:06:47,577 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-11 10:06:47,577 INFO org.mortbay.log: jetty-6.1.26
2015-12-11 10:06:47,866 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-11 10:06:47,866 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-11 10:06:47,883 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-11 10:06:47,883 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-11 10:07:49,296 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2015-12-11 10:07:51,991 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=1506&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-11 10:07:52,189 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2015-12-11 10:07:54,426 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 189.19 KB/s
2015-12-11 10:07:54,429 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000001506 size 8057 bytes.
2015-12-11 10:07:54,493 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=1507&endTxId=1546&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-11 10:07:54,508 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 666.67 KB/s
2015-12-11 10:07:54,508 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000001507-0000000000000001546_0000000000046375965 size 0 bytes.
2015-12-11 10:07:54,957 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 73 INodes.
2015-12-11 10:07:55,095 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-11 10:07:55,096 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 1506 from /home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000001506
2015-12-11 10:07:55,096 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-11 10:07:55,135 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-11 10:07:55,163 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000001507-0000000000000001546 expecting start txid #1507
2015-12-11 10:07:55,163 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000001507-0000000000000001546
2015-12-11 10:07:55,469 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000001507-0000000000000001546 of size 4489 edits # 40 loaded in 0 seconds
2015-12-11 10:07:55,621 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 1506
2015-12-11 10:07:55,622 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000000853, cpktTxId=0000000000000000853)
2015-12-11 10:07:55,622 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000000434, cpktTxId=0000000000000000434)
2015-12-11 10:07:55,748 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 1546 to namenode at http://localhost:50070 in 0.097 seconds
2015-12-11 10:07:55,748 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 8747
2015-12-11 11:07:57,395 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-11 11:07:57,398 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=1547&endTxId=1925&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-11 11:07:57,414 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 25500.00 KB/s
2015-12-11 11:07:57,414 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000001547-0000000000000001925_0000000000049978870 size 0 bytes.
2015-12-11 11:07:57,414 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-11 11:07:57,415 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000001547-0000000000000001925 expecting start txid #1547
2015-12-11 11:07:57,415 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000001547-0000000000000001925
2015-12-11 11:07:57,550 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000001547-0000000000000001925 of size 52375 edits # 379 loaded in 0 seconds
2015-12-11 11:07:57,590 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 1546
2015-12-11 11:07:57,591 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000001506, cpktTxId=0000000000000001506)
2015-12-11 11:07:57,616 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 1925 to namenode at http://localhost:50070 in 0.019 seconds
2015-12-11 11:07:57,616 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 9733
2015-12-11 12:07:58,709 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-11 12:07:58,712 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=1926&endTxId=2091&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-11 12:07:58,726 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 7333.33 KB/s
2015-12-11 12:07:58,726 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000001926-0000000000000002091_0000000000053580184 size 0 bytes.
2015-12-11 12:07:58,727 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-11 12:07:58,728 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000001926-0000000000000002091 expecting start txid #1926
2015-12-11 12:07:58,729 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000001926-0000000000000002091
2015-12-11 12:07:58,771 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000001926-0000000000000002091 of size 22633 edits # 166 loaded in 0 seconds
2015-12-11 12:07:58,821 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 1925
2015-12-11 12:07:58,821 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000001546, cpktTxId=0000000000000001546)
2015-12-11 12:07:58,846 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 2091 to namenode at http://localhost:50070 in 0.019 seconds
2015-12-11 12:07:58,846 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 10639
2015-12-11 13:07:59,441 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-11 13:07:59,442 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=2092&endTxId=2093&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-11 13:07:59,448 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-11 13:07:59,448 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000002092-0000000000000002093_0000000000057180914 size 0 bytes.
2015-12-11 13:07:59,451 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-11 13:07:59,451 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002092-0000000000000002093 expecting start txid #2092
2015-12-11 13:07:59,451 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002092-0000000000000002093
2015-12-11 13:07:59,452 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002092-0000000000000002093 of size 42 edits # 2 loaded in 0 seconds
2015-12-11 13:07:59,473 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2091
2015-12-11 13:07:59,473 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000001925, cpktTxId=0000000000000001925)
2015-12-11 13:07:59,498 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 2093 to namenode at http://localhost:50070 in 0.02 seconds
2015-12-11 13:07:59,498 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 10639
2015-12-11 14:44:46,607 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-11 14:44:46,609 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=2094&endTxId=2095&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-11 14:44:46,619 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2015-12-11 14:44:46,620 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000002094-0000000000000002095_0000000000060781535 size 0 bytes.
2015-12-11 14:44:46,620 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-11 14:44:46,620 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002094-0000000000000002095 expecting start txid #2094
2015-12-11 14:44:46,620 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002094-0000000000000002095
2015-12-11 14:44:46,621 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002094-0000000000000002095 of size 42 edits # 2 loaded in 0 seconds
2015-12-11 14:44:46,636 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2093
2015-12-11 14:44:46,637 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000002091, cpktTxId=0000000000000002091)
2015-12-11 14:44:46,656 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 2095 to namenode at http://localhost:50070 in 0.012 seconds
2015-12-11 14:44:46,656 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 10639
2015-12-11 23:36:09,788 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-11 23:36:09,811 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=2096&endTxId=2232&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-11 23:36:09,852 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 4250.00 KB/s
2015-12-11 23:36:09,853 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000002096-0000000000000002232_0000000000064382586 size 0 bytes.
2015-12-11 23:36:09,856 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-11 23:36:09,859 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002096-0000000000000002232 expecting start txid #2096
2015-12-11 23:36:09,863 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002096-0000000000000002232
2015-12-11 23:36:09,956 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002096-0000000000000002232 of size 17597 edits # 137 loaded in 0 seconds
2015-12-11 23:36:10,082 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2095
2015-12-11 23:36:10,082 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000002093, cpktTxId=0000000000000002093)
2015-12-11 23:36:10,124 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 2232 to namenode at http://localhost:50070 in 0.027 seconds
2015-12-11 23:36:10,124 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 11157
2015-12-12 00:36:11,765 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-12 00:36:11,785 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=2233&endTxId=2688&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-12 00:36:11,849 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 14500.00 KB/s
2015-12-12 00:36:11,849 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000002233-0000000000000002688_0000000000067984562 size 0 bytes.
2015-12-12 00:36:11,852 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-12 00:36:11,856 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002233-0000000000000002688 expecting start txid #2233
2015-12-12 00:36:11,861 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002233-0000000000000002688
2015-12-12 00:36:12,109 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002233-0000000000000002688 of size 60205 edits # 456 loaded in 0 seconds
2015-12-12 00:36:12,258 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2232
2015-12-12 00:36:12,258 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000002095, cpktTxId=0000000000000002095)
2015-12-12 00:36:12,311 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 2688 to namenode at http://localhost:50070 in 0.036 seconds
2015-12-12 00:36:12,311 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 13639
2015-12-12 00:46:14,384 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:15,393 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:16,395 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:17,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:18,400 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:19,402 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:20,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:21,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:22,408 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:23,409 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:23,502 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 17 more
2015-12-12 00:47:24,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:25,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:26,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:27,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:28,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:29,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:30,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:31,566 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:32,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:33,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:33,572 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 17 more
2015-12-12 00:47:44,891 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-12 00:47:44,909 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-12 00:48:36,070 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-12 00:48:36,084 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-12 00:48:36,755 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-12 00:48:36,826 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-12 00:48:36,826 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-12 00:48:37,061 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/namesecondary/in_use.lock acquired by nodename 24331@ubuntu
2015-12-12 00:48:37,119 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-12 00:48:37,127 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-12 00:48:37,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-12 00:48:37,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-12 00:48:37,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-12 00:48:37,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 12 00:48:37
2015-12-12 00:48:37,172 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-12 00:48:37,173 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-12 00:48:37,174 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-12 00:48:37,174 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-12 00:48:37,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-12 00:48:37,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-12 00:48:37,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-12 00:48:37,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-12 00:48:37,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-12 00:48:37,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-12 00:48:37,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-12 00:48:37,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-12 00:48:37,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-12 00:48:37,225 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-12 00:48:37,225 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-12 00:48:37,225 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-12 00:48:37,225 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-12 00:48:37,230 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-12 00:48:37,454 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-12 00:48:37,454 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-12 00:48:37,454 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-12 00:48:37,454 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-12 00:48:37,463 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-12 00:48:37,488 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-12 00:48:37,488 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-12 00:48:37,488 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-12 00:48:37,488 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-12 00:48:37,490 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-12 00:48:37,490 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-12 00:48:37,490 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-12 00:48:37,493 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-12 00:48:37,493 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-12 00:48:37,493 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-12 00:48:37,509 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-12 00:48:37,562 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-12 00:48:37,567 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-12 00:48:37,579 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-12 00:48:37,583 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-12 00:48:37,583 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-12 00:48:37,583 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-12 00:48:37,602 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-12 00:48:37,603 INFO org.mortbay.log: jetty-6.1.26
2015-12-12 00:48:37,958 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-12 00:48:37,958 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-12 00:48:37,980 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-12 00:48:37,981 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-12 00:49:39,563 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2015-12-12 00:49:41,506 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=2688&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-12 00:49:41,729 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2015-12-12 00:49:43,164 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.36s at 36.62 KB/s
2015-12-12 00:49:43,164 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000002688 size 13639 bytes.
2015-12-12 00:49:43,209 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=2689&endTxId=2877&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-12 00:49:43,267 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 22755.56 KB/s
2015-12-12 00:49:43,267 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000002689-0000000000000002877_0000000000068795987 size 0 bytes.
2015-12-12 00:49:43,268 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=2878&endTxId=2917&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-12 00:49:43,276 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 800.00 KB/s
2015-12-12 00:49:43,277 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000002878-0000000000000002917_0000000000068796046 size 0 bytes.
2015-12-12 00:49:43,477 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 121 INodes.
2015-12-12 00:49:43,721 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-12 00:49:43,722 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 2688 from /home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000002688
2015-12-12 00:49:43,722 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-12 00:49:43,747 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 2 stream(s).
2015-12-12 00:49:43,796 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002689-0000000000000002877 expecting start txid #2689
2015-12-12 00:49:43,796 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002689-0000000000000002877
2015-12-12 00:49:44,021 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002689-0000000000000002877 of size 1048576 edits # 189 loaded in 0 seconds
2015-12-12 00:49:44,036 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002878-0000000000000002917 expecting start txid #2878
2015-12-12 00:49:44,036 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002878-0000000000000002917
2015-12-12 00:49:44,046 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002878-0000000000000002917 of size 4493 edits # 40 loaded in 0 seconds
2015-12-12 00:49:44,367 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2688
2015-12-12 00:49:44,368 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000002232, cpktTxId=0000000000000002232)
2015-12-12 00:49:44,616 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 2917 to namenode at http://localhost:50070 in 0.141 seconds
2015-12-12 00:49:44,616 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 15928
2015-12-12 00:55:43,836 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-12 00:55:43,852 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-12 11:55:55,855 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-12 11:55:55,871 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-12 11:55:56,719 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-12 11:55:56,823 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-12 11:55:56,824 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-12 11:55:57,178 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/namesecondary/in_use.lock acquired by nodename 28194@ubuntu
2015-12-12 11:55:57,245 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-12 11:55:57,259 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-12 11:55:57,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-12 11:55:57,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-12 11:55:57,335 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-12 11:55:57,338 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 12 11:55:57
2015-12-12 11:55:57,341 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-12 11:55:57,341 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-12 11:55:57,344 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-12 11:55:57,344 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-12 11:55:57,401 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-12 11:55:57,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-12 11:55:57,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-12 11:55:57,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-12 11:55:57,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-12 11:55:57,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-12 11:55:57,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-12 11:55:57,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-12 11:55:57,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-12 11:55:57,402 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-12 11:55:57,402 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-12 11:55:57,402 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-12 11:55:57,402 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-12 11:55:57,404 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-12 11:55:57,677 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-12 11:55:57,677 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-12 11:55:57,677 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-12 11:55:57,677 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-12 11:55:57,680 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-12 11:55:57,797 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-12 11:55:57,797 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-12 11:55:57,798 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-12 11:55:57,798 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-12 11:55:57,799 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-12 11:55:57,799 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-12 11:55:57,799 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-12 11:55:57,801 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-12 11:55:57,802 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-12 11:55:57,802 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-12 11:55:57,825 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-12 11:55:57,924 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-12 11:55:57,927 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-12 11:55:57,937 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-12 11:55:57,939 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-12 11:55:57,940 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-12 11:55:57,940 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-12 11:55:57,971 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-12 11:55:57,971 INFO org.mortbay.log: jetty-6.1.26
2015-12-12 11:55:58,317 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-12 11:55:58,318 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-12 11:55:58,339 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-12 11:55:58,339 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-12 11:56:58,642 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2015-12-12 11:56:59,020 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=2961&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-12 11:56:59,071 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2015-12-12 11:56:59,608 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.03s at 454.55 KB/s
2015-12-12 11:56:59,608 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000002961 size 15604 bytes.
2015-12-12 11:56:59,617 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=2962&endTxId=2963&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-12 11:56:59,622 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-12 11:56:59,622 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000002962-0000000000000002963_0000000000077386036 size 0 bytes.
2015-12-12 11:56:59,680 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 140 INodes.
2015-12-12 11:56:59,745 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-12 11:56:59,745 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 2961 from /home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000002961
2015-12-12 11:56:59,745 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-12 11:56:59,754 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-12 11:56:59,759 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002962-0000000000000002963 expecting start txid #2962
2015-12-12 11:56:59,759 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002962-0000000000000002963
2015-12-12 11:56:59,801 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002962-0000000000000002963 of size 42 edits # 2 loaded in 0 seconds
2015-12-12 11:56:59,883 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2961
2015-12-12 11:56:59,883 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000002917, cpktTxId=0000000000000002917)
2015-12-12 11:56:59,885 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000002688, cpktTxId=0000000000000002688)
2015-12-12 11:56:59,935 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 2963 to namenode at http://localhost:50070 in 0.033 seconds
2015-12-12 11:56:59,935 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 15604
2015-12-12 15:49:41,395 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-12 15:49:41,402 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=2964&endTxId=3300&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-12 15:49:41,426 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 21000.00 KB/s
2015-12-12 15:49:41,426 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000002964-0000000000000003300_0000000000080988349 size 0 bytes.
2015-12-12 15:49:41,429 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-12 15:49:41,429 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002964-0000000000000003300 expecting start txid #2964
2015-12-12 15:49:41,431 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002964-0000000000000003300
2015-12-12 15:49:41,660 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000002964-0000000000000003300 of size 43803 edits # 337 loaded in 0 seconds
2015-12-12 15:49:41,772 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2963
2015-12-12 15:49:41,772 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000002961, cpktTxId=0000000000000002961)
2015-12-12 15:49:41,800 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 3300 to namenode at http://localhost:50070 in 0.019 seconds
2015-12-12 15:49:41,801 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 17293
2015-12-12 16:49:43,295 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-12 16:49:43,303 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=3301&endTxId=3528&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-12 16:49:43,346 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 4285.71 KB/s
2015-12-12 16:49:43,346 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000003301-0000000000000003528_0000000000084590250 size 0 bytes.
2015-12-12 16:49:43,350 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-12 16:49:43,353 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000003301-0000000000000003528 expecting start txid #3301
2015-12-12 16:49:43,354 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000003301-0000000000000003528
2015-12-12 16:49:43,456 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000003301-0000000000000003528 of size 30985 edits # 228 loaded in 0 seconds
2015-12-12 16:49:43,554 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 3300
2015-12-12 16:49:43,554 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000002963, cpktTxId=0000000000000002963)
2015-12-12 16:49:43,611 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 3528 to namenode at http://localhost:50070 in 0.038 seconds
2015-12-12 16:49:43,612 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 18304
2015-12-12 18:05:16,147 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-12 18:05:16,151 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=3529&endTxId=3688&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-12 18:05:16,165 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 21000.00 KB/s
2015-12-12 18:05:16,166 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000003529-0000000000000003688_0000000000088191950 size 0 bytes.
2015-12-12 18:05:16,168 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-12 18:05:16,169 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000003529-0000000000000003688 expecting start txid #3529
2015-12-12 18:05:16,170 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000003529-0000000000000003688
2015-12-12 18:05:16,223 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000003529-0000000000000003688 of size 22180 edits # 160 loaded in 0 seconds
2015-12-12 18:05:16,287 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 3528
2015-12-12 18:05:16,288 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000003300, cpktTxId=0000000000000003300)
2015-12-12 18:05:16,314 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 3688 to namenode at http://localhost:50070 in 0.018 seconds
2015-12-12 18:05:16,314 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 19199
2015-12-12 19:05:18,428 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-12 19:05:18,433 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=3689&endTxId=3849&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-12 19:05:18,451 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 10500.00 KB/s
2015-12-12 19:05:18,451 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000003689-0000000000000003849_0000000000091794232 size 0 bytes.
2015-12-12 19:05:18,453 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-12 19:05:18,454 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000003689-0000000000000003849 expecting start txid #3689
2015-12-12 19:05:18,454 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000003689-0000000000000003849
2015-12-12 19:05:18,515 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000003689-0000000000000003849 of size 22258 edits # 161 loaded in 0 seconds
2015-12-12 19:05:18,594 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 3688
2015-12-12 19:05:18,594 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000003528, cpktTxId=0000000000000003528)
2015-12-12 19:05:18,638 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 3849 to namenode at http://localhost:50070 in 0.028 seconds
2015-12-12 19:05:18,638 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 19915
2015-12-12 22:52:51,954 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-12 22:52:51,956 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=3850&endTxId=3851&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-12 22:52:51,966 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-12 22:52:51,967 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000003850-0000000000000003851_0000000000095395170 size 0 bytes.
2015-12-12 22:52:51,969 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-12 22:52:51,969 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000003850-0000000000000003851 expecting start txid #3850
2015-12-12 22:52:51,969 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000003850-0000000000000003851
2015-12-12 22:52:51,970 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000003850-0000000000000003851 of size 42 edits # 2 loaded in 0 seconds
2015-12-12 22:52:51,999 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 3849
2015-12-12 22:52:51,999 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000003688, cpktTxId=0000000000000003688)
2015-12-12 22:52:52,030 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 3851 to namenode at http://localhost:50070 in 0.021 seconds
2015-12-12 22:52:52,030 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 19915
2015-12-13 00:02:45,193 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-13 00:02:45,199 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=3852&endTxId=4015&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-13 00:02:45,223 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 7000.00 KB/s
2015-12-13 00:02:45,223 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000003852-0000000000000004015_0000000000098996737 size 0 bytes.
2015-12-13 00:02:45,224 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-13 00:02:45,226 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000003852-0000000000000004015 expecting start txid #3852
2015-12-13 00:02:45,227 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000003852-0000000000000004015
2015-12-13 00:02:45,308 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000003852-0000000000000004015 of size 22507 edits # 164 loaded in 0 seconds
2015-12-13 00:02:45,403 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 3851
2015-12-13 00:02:45,404 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000003849, cpktTxId=0000000000000003849)
2015-12-13 00:02:45,470 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 4015 to namenode at http://localhost:50070 in 0.038 seconds
2015-12-13 00:02:45,471 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 20646
2015-12-13 00:31:35,188 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-13 00:31:35,299 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-13 10:34:25,324 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-13 10:34:25,339 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-13 10:34:26,070 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-13 10:34:26,127 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-13 10:34:26,128 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-13 10:34:26,307 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/namesecondary/in_use.lock acquired by nodename 24127@ubuntu
2015-12-13 10:34:26,360 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-13 10:34:26,366 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-13 10:34:26,399 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-13 10:34:26,399 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-13 10:34:26,400 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-13 10:34:26,401 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 13 10:34:26
2015-12-13 10:34:26,402 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-13 10:34:26,402 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-13 10:34:26,403 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-13 10:34:26,403 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-13 10:34:26,440 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-13 10:34:26,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-13 10:34:26,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-13 10:34:26,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-13 10:34:26,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-13 10:34:26,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-13 10:34:26,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-13 10:34:26,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-13 10:34:26,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-13 10:34:26,441 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-13 10:34:26,441 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-13 10:34:26,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-13 10:34:26,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-13 10:34:26,443 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-13 10:34:26,620 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-13 10:34:26,621 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-13 10:34:26,621 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-13 10:34:26,621 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-13 10:34:26,626 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-13 10:34:26,640 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-13 10:34:26,640 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-13 10:34:26,641 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-13 10:34:26,641 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-13 10:34:26,642 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-13 10:34:26,642 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-13 10:34:26,642 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-13 10:34:26,645 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-13 10:34:26,645 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-13 10:34:26,645 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-13 10:34:26,666 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-13 10:34:26,721 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-13 10:34:26,725 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-13 10:34:26,735 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-13 10:34:26,737 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-13 10:34:26,737 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-13 10:34:26,737 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-13 10:34:26,752 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-13 10:34:26,752 INFO org.mortbay.log: jetty-6.1.26
2015-12-13 10:34:27,051 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-13 10:34:27,051 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-13 10:34:27,071 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-13 10:34:27,071 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-13 10:35:27,248 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2015-12-13 10:35:27,680 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=4099&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-13 10:35:27,723 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2015-12-13 10:35:27,981 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 3333.33 KB/s
2015-12-13 10:35:27,981 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000004099 size 21196 bytes.
2015-12-13 10:35:27,989 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=4100&endTxId=4102&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-13 10:35:27,992 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-13 10:35:27,992 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000004100-0000000000000004102_0000000000102310951 size 0 bytes.
2015-12-13 10:35:28,046 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 191 INodes.
2015-12-13 10:35:28,208 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-13 10:35:28,209 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 4099 from /home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000004099
2015-12-13 10:35:28,209 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-13 10:35:28,230 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-13 10:35:28,288 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004100-0000000000000004102 expecting start txid #4100
2015-12-13 10:35:28,303 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004100-0000000000000004102
2015-12-13 10:35:28,430 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004100-0000000000000004102 of size 125 edits # 3 loaded in 0 seconds
2015-12-13 10:35:28,638 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 4099
2015-12-13 10:35:28,639 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000003851, cpktTxId=0000000000000003851)
2015-12-13 10:35:28,641 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000004015, cpktTxId=0000000000000004015)
2015-12-13 10:35:28,704 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 4102 to namenode at http://localhost:50070 in 0.035 seconds
2015-12-13 10:35:28,705 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 21003
2015-12-13 11:35:31,168 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-13 11:35:31,183 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=4103&endTxId=4496&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-13 11:35:31,226 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 13000.00 KB/s
2015-12-13 11:35:31,226 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000004103-0000000000000004496_0000000000105914142 size 0 bytes.
2015-12-13 11:35:31,228 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-13 11:35:31,232 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004103-0000000000000004496 expecting start txid #4103
2015-12-13 11:35:31,233 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004103-0000000000000004496
2015-12-13 11:35:31,619 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004103-0000000000000004496 of size 53613 edits # 394 loaded in 0 seconds
2015-12-13 11:35:31,698 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 4102
2015-12-13 11:35:31,699 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000004099, cpktTxId=0000000000000004099)
2015-12-13 11:35:31,758 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 4496 to namenode at http://localhost:50070 in 0.037 seconds
2015-12-13 11:35:31,758 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 22978
2015-12-13 13:03:51,368 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-13 13:03:51,389 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=4497&endTxId=4582&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-13 13:03:51,456 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 1833.33 KB/s
2015-12-13 13:03:51,456 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000004497-0000000000000004582_0000000000109515781 size 0 bytes.
2015-12-13 13:03:51,459 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-13 13:03:51,462 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004497-0000000000000004582 expecting start txid #4497
2015-12-13 13:03:51,465 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004497-0000000000000004582
2015-12-13 13:03:51,530 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004497-0000000000000004582 of size 11516 edits # 86 loaded in 0 seconds
2015-12-13 13:03:51,627 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 4496
2015-12-13 13:03:51,628 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000004102, cpktTxId=0000000000000004102)
2015-12-13 13:03:51,699 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 4582 to namenode at http://localhost:50070 in 0.049 seconds
2015-12-13 13:03:51,701 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 23335
2015-12-13 14:03:53,551 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-13 14:03:53,555 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=4583&endTxId=4766&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-13 14:03:53,576 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 5750.00 KB/s
2015-12-13 14:03:53,576 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000004583-0000000000000004766_0000000000113117947 size 0 bytes.
2015-12-13 14:03:53,581 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-13 14:03:53,586 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004583-0000000000000004766 expecting start txid #4583
2015-12-13 14:03:53,586 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004583-0000000000000004766
2015-12-13 14:03:53,644 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004583-0000000000000004766 of size 23730 edits # 184 loaded in 0 seconds
2015-12-13 14:03:53,705 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 4582
2015-12-13 14:03:53,705 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000004496, cpktTxId=0000000000000004496)
2015-12-13 14:03:53,749 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 4766 to namenode at http://localhost:50070 in 0.028 seconds
2015-12-13 14:03:53,750 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 24547
2015-12-13 15:04:21,699 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-13 15:04:21,718 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=4767&endTxId=4852&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-13 15:04:21,761 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 3666.67 KB/s
2015-12-13 15:04:21,762 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000004767-0000000000000004852_0000000000116718985 size 0 bytes.
2015-12-13 15:04:21,764 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-13 15:04:21,766 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004767-0000000000000004852 expecting start txid #4767
2015-12-13 15:04:21,769 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004767-0000000000000004852
2015-12-13 15:04:21,816 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004767-0000000000000004852 of size 11656 edits # 86 loaded in 0 seconds
2015-12-13 15:04:21,955 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 4766
2015-12-13 15:04:21,956 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000004582, cpktTxId=0000000000000004582)
2015-12-13 15:04:22,078 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 4852 to namenode at http://localhost:50070 in 0.049 seconds
2015-12-13 15:04:22,079 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 24912
2015-12-13 16:42:57,124 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-13 16:42:57,133 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=4853&endTxId=5489&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-13 16:42:57,166 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 21250.00 KB/s
2015-12-13 16:42:57,166 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000004853-0000000000000005489_0000000000120321556 size 0 bytes.
2015-12-13 16:42:57,171 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-13 16:42:57,177 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004853-0000000000000005489 expecting start txid #4853
2015-12-13 16:42:57,179 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004853-0000000000000005489
2015-12-13 16:42:57,553 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000004853-0000000000000005489 of size 87869 edits # 637 loaded in 0 seconds
2015-12-13 16:42:57,801 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 4852
2015-12-13 16:42:57,801 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000004766, cpktTxId=0000000000000004766)
2015-12-13 16:42:57,876 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 5489 to namenode at http://localhost:50070 in 0.039 seconds
2015-12-13 16:42:57,876 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 27934
2015-12-13 19:22:30,798 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-13 19:22:30,811 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=5490&endTxId=5655&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-13 19:22:30,855 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 4400.00 KB/s
2015-12-13 19:22:30,856 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000005490-0000000000000005655_0000000000123923587 size 0 bytes.
2015-12-13 19:22:30,858 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-13 19:22:30,861 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000005490-0000000000000005655 expecting start txid #5490
2015-12-13 19:22:30,862 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000005490-0000000000000005655
2015-12-13 19:22:30,964 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000005490-0000000000000005655 of size 22722 edits # 166 loaded in 0 seconds
2015-12-13 19:22:31,194 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 5489
2015-12-13 19:22:31,194 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000004852, cpktTxId=0000000000000004852)
2015-12-13 19:22:31,242 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 5655 to namenode at http://localhost:50070 in 0.031 seconds
2015-12-13 19:22:31,243 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 28642
2015-12-13 20:22:33,277 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-13 20:22:33,287 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=5656&endTxId=6387&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-13 20:22:33,326 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 24500.00 KB/s
2015-12-13 20:22:33,326 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000005656-0000000000000006387_0000000000127526064 size 0 bytes.
2015-12-13 20:22:33,328 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-13 20:22:33,331 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000005656-0000000000000006387 expecting start txid #5656
2015-12-13 20:22:33,334 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000005656-0000000000000006387
2015-12-13 20:22:33,607 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000005656-0000000000000006387 of size 100601 edits # 732 loaded in 0 seconds
2015-12-13 20:22:33,723 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 5655
2015-12-13 20:22:33,724 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000005489, cpktTxId=0000000000000005489)
2015-12-13 20:22:33,781 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 6387 to namenode at http://localhost:50070 in 0.034 seconds
2015-12-13 20:22:33,782 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 32028
2015-12-13 20:38:34,294 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "ubuntu/127.0.1.1"; destination host is: "localhost":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy9.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:125)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:412)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-12-13 20:38:45,943 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-13 20:38:45,949 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-15 01:11:19,633 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-15 01:11:19,648 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-15 01:11:20,558 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-15 01:11:20,670 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-15 01:11:20,670 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-15 01:11:21,265 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/namesecondary/in_use.lock acquired by nodename 3950@ubuntu
2015-12-15 01:11:21,350 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-15 01:11:21,370 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-15 01:11:21,436 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-15 01:11:21,436 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-15 01:11:21,437 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-15 01:11:21,442 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 15 01:11:21
2015-12-15 01:11:21,445 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-15 01:11:21,445 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-15 01:11:21,449 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-15 01:11:21,449 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-15 01:11:21,503 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-15 01:11:21,503 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-15 01:11:21,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-15 01:11:21,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-15 01:11:21,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-15 01:11:21,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-15 01:11:21,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-15 01:11:21,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-15 01:11:21,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-15 01:11:21,504 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-15 01:11:21,504 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-15 01:11:21,504 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-15 01:11:21,504 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-15 01:11:21,508 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-15 01:11:21,789 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-15 01:11:21,789 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-15 01:11:21,790 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-15 01:11:21,790 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-15 01:11:21,797 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-15 01:11:21,832 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-15 01:11:21,832 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-15 01:11:21,832 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-15 01:11:21,832 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-15 01:11:21,834 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-15 01:11:21,834 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-15 01:11:21,834 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-15 01:11:21,837 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-15 01:11:21,837 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-15 01:11:21,837 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-15 01:11:21,867 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-15 01:11:21,947 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-15 01:11:21,950 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-15 01:11:21,960 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-15 01:11:21,962 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-15 01:11:21,962 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-15 01:11:21,962 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-15 01:11:22,002 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-15 01:11:22,002 INFO org.mortbay.log: jetty-6.1.26
2015-12-15 01:11:22,454 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-15 01:11:22,455 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-15 01:11:22,476 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-15 01:11:22,476 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-15 01:12:22,702 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2015-12-15 01:12:23,084 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=6388&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-15 01:12:23,129 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2015-12-15 01:12:23,517 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 3444.44 KB/s
2015-12-15 01:12:23,517 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000006388 size 32028 bytes.
2015-12-15 01:12:23,525 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=6389&endTxId=6391&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-15 01:12:23,532 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-15 01:12:23,532 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000006389-0000000000000006391_0000000000010789466 size 0 bytes.
2015-12-15 01:12:23,588 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 288 INodes.
2015-12-15 01:12:23,676 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-15 01:12:23,676 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 6388 from /home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000006388
2015-12-15 01:12:23,676 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-15 01:12:23,695 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-15 01:12:23,702 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000006389-0000000000000006391 expecting start txid #6389
2015-12-15 01:12:23,702 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000006389-0000000000000006391
2015-12-15 01:12:23,764 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000006389-0000000000000006391 of size 136 edits # 3 loaded in 0 seconds
2015-12-15 01:12:23,840 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 6388
2015-12-15 01:12:23,840 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000005655, cpktTxId=0000000000000005655)
2015-12-15 01:12:23,841 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000006387, cpktTxId=0000000000000006387)
2015-12-15 01:12:23,895 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 6391 to namenode at http://localhost:50070 in 0.036 seconds
2015-12-15 01:12:23,895 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 31824
2015-12-15 01:15:47,652 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-15 01:15:47,697 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-15 01:16:22,212 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-15 01:16:22,219 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-15 01:16:22,902 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-15 01:16:22,983 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-15 01:16:22,983 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2015-12-15 01:16:23,171 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/namesecondary/in_use.lock acquired by nodename 7437@ubuntu
2015-12-15 01:16:23,227 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-15 01:16:23,233 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-15 01:16:23,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-15 01:16:23,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-15 01:16:23,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-15 01:16:23,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 15 01:16:23
2015-12-15 01:16:23,270 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-15 01:16:23,271 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-15 01:16:23,272 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-15 01:16:23,272 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-15 01:16:23,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-15 01:16:23,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-15 01:16:23,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-15 01:16:23,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-15 01:16:23,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-15 01:16:23,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-15 01:16:23,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-15 01:16:23,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-15 01:16:23,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-15 01:16:23,312 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-15 01:16:23,312 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-15 01:16:23,312 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-15 01:16:23,312 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-15 01:16:23,314 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-15 01:16:23,476 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-15 01:16:23,476 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-15 01:16:23,476 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-15 01:16:23,476 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-15 01:16:23,478 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-15 01:16:23,490 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-15 01:16:23,490 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-15 01:16:23,490 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-15 01:16:23,490 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-15 01:16:23,491 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-15 01:16:23,491 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-15 01:16:23,491 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-15 01:16:23,493 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-15 01:16:23,493 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-15 01:16:23,494 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-15 01:16:23,505 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2015-12-15 01:16:23,576 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-15 01:16:23,579 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2015-12-15 01:16:23,589 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-15 01:16:23,592 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2015-12-15 01:16:23,592 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-15 01:16:23,592 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-15 01:16:23,606 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2015-12-15 01:16:23,606 INFO org.mortbay.log: jetty-6.1.26
2015-12-15 01:16:23,909 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2015-12-15 01:16:23,909 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2015-12-15 01:16:23,943 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2015-12-15 01:16:23,943 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2015-12-15 01:17:24,121 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2015-12-15 01:17:24,505 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=6391&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-15 01:17:24,550 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2015-12-15 01:17:24,920 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 3875.00 KB/s
2015-12-15 01:17:24,920 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000006391 size 31824 bytes.
2015-12-15 01:17:24,929 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=6392&endTxId=6476&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-15 01:17:24,940 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 146285.71 KB/s
2015-12-15 01:17:24,940 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000006392-0000000000000006476_0000000000011090870 size 0 bytes.
2015-12-15 01:17:24,941 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=6477&endTxId=6478&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-15 01:17:24,950 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-15 01:17:24,950 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000006477-0000000000000006478_0000000000011090881 size 0 bytes.
2015-12-15 01:17:25,003 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 285 INodes.
2015-12-15 01:17:25,087 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-15 01:17:25,087 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 6391 from /home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000006391
2015-12-15 01:17:25,087 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-15 01:17:25,101 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 2 stream(s).
2015-12-15 01:17:25,107 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000006392-0000000000000006476 expecting start txid #6392
2015-12-15 01:17:25,107 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000006392-0000000000000006476
2015-12-15 01:17:25,186 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000006392-0000000000000006476 of size 1048576 edits # 85 loaded in 0 seconds
2015-12-15 01:17:25,186 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000006477-0000000000000006478 expecting start txid #6477
2015-12-15 01:17:25,186 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000006477-0000000000000006478
2015-12-15 01:17:25,187 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000006477-0000000000000006478 of size 42 edits # 2 loaded in 0 seconds
2015-12-15 01:17:25,257 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 6391
2015-12-15 01:17:25,257 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000006388, cpktTxId=0000000000000006388)
2015-12-15 01:17:25,317 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 6478 to namenode at http://localhost:50070 in 0.045 seconds
2015-12-15 01:17:25,318 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 32189
2015-12-15 02:17:26,598 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2015-12-15 02:17:26,601 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=6479&endTxId=6965&storageInfo=-60:1379726899:0:CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1
2015-12-15 02:17:26,616 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 32500.00 KB/s
2015-12-15 02:17:26,616 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000006479-0000000000000006965_0000000000014692541 size 0 bytes.
2015-12-15 02:17:26,617 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2015-12-15 02:17:26,617 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000006479-0000000000000006965 expecting start txid #6479
2015-12-15 02:17:26,618 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000006479-0000000000000006965
2015-12-15 02:17:26,741 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/namesecondary/current/edits_0000000000000006479-0000000000000006965 of size 67493 edits # 487 loaded in 0 seconds
2015-12-15 02:17:26,802 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 6478
2015-12-15 02:17:26,803 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/namesecondary/current/fsimage_0000000000000006391, cpktTxId=0000000000000006391)
2015-12-15 02:17:26,853 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 6965 to namenode at http://localhost:50070 in 0.034 seconds
2015-12-15 02:17:26,853 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 34506
2015-12-15 09:39:44,360 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-15 09:39:44,456 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ubuntu/127.0.1.1
************************************************************/
