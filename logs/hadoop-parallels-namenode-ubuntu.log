2015-12-04 21:18:32,341 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-04 21:18:32,356 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-04 21:18:32,359 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-04 21:18:32,659 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-04 21:18:32,793 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-04 21:18:32,793 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-04 21:18:32,795 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-04 21:18:32,796 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-04 21:18:32,982 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-04 21:18:33,035 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-04 21:18:33,039 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-04 21:18:33,051 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-04 21:18:33,053 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-04 21:18:33,053 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-04 21:18:33,053 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-04 21:18:33,080 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-04 21:18:33,082 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-04 21:18:33,101 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-04 21:18:33,101 INFO org.mortbay.log: jetty-6.1.26
2015-12-04 21:18:33,309 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-04 21:18:33,361 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-04 21:18:33,361 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-04 21:18:33,396 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-04 21:18:33,403 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-04 21:18:33,435 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-04 21:18:33,435 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-04 21:18:33,438 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-04 21:18:33,439 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 04 21:18:33
2015-12-04 21:18:33,441 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-04 21:18:33,441 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-04 21:18:33,442 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-04 21:18:33,442 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-04 21:18:33,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-04 21:18:33,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-04 21:18:33,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-04 21:18:33,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-04 21:18:33,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-04 21:18:33,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-04 21:18:33,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-04 21:18:33,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-04 21:18:33,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-04 21:18:33,474 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-04 21:18:33,474 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-04 21:18:33,474 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-04 21:18:33,474 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-04 21:18:33,476 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-04 21:18:33,642 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-04 21:18:33,643 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-04 21:18:33,643 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-04 21:18:33,643 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-04 21:18:33,646 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-04 21:18:33,657 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-04 21:18:33,657 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-04 21:18:33,657 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-04 21:18:33,657 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-04 21:18:33,659 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-04 21:18:33,659 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-04 21:18:33,659 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-04 21:18:33,660 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-04 21:18:33,660 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-04 21:18:33,662 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-04 21:18:33,662 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-04 21:18:33,662 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-04 21:18:33,662 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-04 21:18:33,665 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-04 21:18:33,665 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-04 21:18:33,666 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-04 21:18:33,678 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-parallels/dfs/name/in_use.lock acquired by nodename 3189@ubuntu
2015-12-04 21:18:33,747 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-parallels/dfs/name/current
2015-12-04 21:18:33,747 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2015-12-04 21:18:33,780 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-04 21:18:33,810 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-04 21:18:33,810 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-parallels/dfs/name/current/fsimage_0000000000000000000
2015-12-04 21:18:33,815 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-12-04 21:18:33,815 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2015-12-04 21:18:33,931 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-04 21:18:33,931 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 265 msecs
2015-12-04 21:18:34,219 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-04 21:18:34,226 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-04 21:18:34,238 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-04 21:18:34,267 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-04 21:18:34,279 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-04 21:18:34,279 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-04 21:18:34,279 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-04 21:18:34,279 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2015-12-04 21:18:34,280 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2015-12-04 21:18:34,280 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-12-04 21:18:34,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2015-12-04 21:18:34,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-04 21:18:34,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-12-04 21:18:34,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-04 21:18:34,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-04 21:18:34,295 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 14 msec
2015-12-04 21:18:34,316 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-04 21:18:34,317 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-04 21:18:34,323 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-04 21:18:34,323 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-04 21:18:34,332 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-04 21:18:34,332 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 8798893 milliseconds
2015-12-04 21:18:34,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:18:43,388 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=7b1d6000-b128-42b1-b8a1-6bfae41da4eb, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-fc10af95-6416-4edd-9572-a006bee00a03;nsid=288020609;c=0) storage 7b1d6000-b128-42b1-b8a1-6bfae41da4eb
2015-12-04 21:18:43,391 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-04 21:18:43,392 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-04 21:18:43,521 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-04 21:18:43,528 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-f46deaf3-2033-4196-9670-32be7e60fbca for DN 127.0.0.1:50010
2015-12-04 21:18:43,566 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-f46deaf3-2033-4196-9670-32be7e60fbca,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-12-04 21:18:43,566 INFO BlockStateChange: BLOCK* processReport: from storage DS-f46deaf3-2033-4196-9670-32be7e60fbca node DatanodeRegistration(127.0.0.1, datanodeUuid=7b1d6000-b128-42b1-b8a1-6bfae41da4eb, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-fc10af95-6416-4edd-9572-a006bee00a03;nsid=288020609;c=0), blocks: 0, hasStaleStorages: false, processing time: 1 msecs
2015-12-04 21:19:04,333 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:19:04,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:19:34,333 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:19:34,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:19:55,563 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-04 21:19:55,563 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-04 21:19:55,563 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2015-12-04 21:19:55,563 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 9 
2015-12-04 21:19:55,569 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 15 
2015-12-04 21:19:55,570 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-parallels/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-parallels/dfs/name/current/edits_0000000000000000001-0000000000000000002
2015-12-04 21:19:55,574 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2015-12-04 21:19:56,424 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-04 21:19:56,424 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 356 bytes.
2015-12-04 21:19:56,430 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2015-12-04 21:20:04,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:20:04,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:20:12,640 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-04 21:20:34,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:20:34,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:21:04,335 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:21:04,335 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:21:34,335 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:21:34,336 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:21:39,882 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 21 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 5 
2015-12-04 21:22:04,336 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:22:04,337 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:22:34,336 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:22:34,337 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:23:04,337 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:23:04,338 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:23:34,337 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:23:34,337 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:23:49,315 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-04 21:24:00,748 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-04 21:24:04,337 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:24:04,337 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:24:34,337 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:24:34,338 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:25:04,339 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:25:04,339 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:25:34,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:25:34,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:26:04,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:26:04,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:26:34,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:26:34,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:27:04,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:27:04,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:27:34,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:27:34,342 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:28:04,342 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:28:04,343 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:28:34,343 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:28:34,344 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:29:04,343 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:29:04,343 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:29:34,343 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:29:34,344 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:30:04,344 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:30:04,345 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:30:34,346 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:30:34,346 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:31:04,346 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:31:04,346 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:31:34,347 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:31:34,347 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:32:04,347 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:32:04,348 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:32:34,347 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:32:34,348 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:33:04,347 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:33:04,347 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:33:34,346 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:33:34,347 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:34:04,348 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:34:04,348 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:34:34,347 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:34:34,348 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:35:04,349 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:35:04,349 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:35:34,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:35:34,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:36:04,349 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:36:04,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:36:34,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:36:34,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:37:04,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:37:04,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:37:34,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:37:34,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:38:04,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:38:04,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:38:34,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:38:34,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:39:04,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:39:04,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:39:34,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:39:34,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:40:04,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:40:04,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:40:34,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:40:34,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:41:04,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:41:04,353 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:41:34,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:41:34,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:42:04,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:42:04,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:42:34,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:42:34,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:43:04,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:43:04,353 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:43:34,353 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:43:34,354 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:44:04,354 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:44:04,355 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:44:34,356 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:44:34,356 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:45:04,357 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:45:04,357 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:45:34,358 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:45:34,358 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:46:04,358 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:46:04,358 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:46:34,359 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:46:34,359 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:47:04,360 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:47:04,360 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:47:34,360 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:47:34,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:48:04,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:48:04,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:48:34,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:48:34,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:49:04,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:49:04,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:49:34,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:49:34,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:50:04,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:50:04,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:50:34,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:50:34,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:51:04,361 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:51:04,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:51:34,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:51:34,363 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:52:04,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:52:04,363 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:52:34,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:52:34,363 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:53:04,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:53:04,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:53:34,363 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:53:34,363 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:54:04,369 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30006 milliseconds
2015-12-04 21:54:04,369 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:54:34,368 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:54:34,369 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:55:04,369 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:55:04,370 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:55:34,369 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:55:34,369 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:55:50,893 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 21 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 63 
2015-12-04 21:56:04,369 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:56:04,369 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:56:34,369 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:56:34,370 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:57:04,370 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:57:04,371 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:57:34,371 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:57:34,371 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:58:04,371 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:58:04,371 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:58:34,371 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 21:58:34,372 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 21:59:04,372 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:59:04,372 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 21:59:34,372 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 21:59:34,372 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:00:04,372 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:00:04,372 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:00:34,371 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:00:34,372 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:01:04,373 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:01:04,373 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:01:34,373 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:01:34,374 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:02:04,374 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:02:04,374 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:02:34,374 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:02:34,374 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:03:04,374 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:03:04,374 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:03:34,374 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:03:34,375 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:04:04,375 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:04:04,376 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:04:34,375 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:04:34,376 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:05:04,377 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:05:04,377 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:05:34,377 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:05:34,378 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:06:04,378 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:06:04,379 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:06:34,380 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:06:34,380 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:07:04,380 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:07:04,381 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:07:34,381 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:07:34,382 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-04 22:08:04,380 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:08:04,381 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:08:26,043 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 5 Total time for transactions(ms): 22 Number of transactions batched in Syncs: 0 Number of syncs: 5 SyncTimes(ms): 69 
2015-12-04 22:08:34,381 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:08:34,382 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:09:01,236 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/capacity-scheduler.xml._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:01,557 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-04 22:09:01,557 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2015-12-04 22:09:01,563 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 4436
2015-12-04 22:09:01,985 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/capacity-scheduler.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,013 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/configuration.xsl._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,023 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,026 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/configuration.xsl._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,043 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/container-executor.cfg._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,051 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,054 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/container-executor.cfg._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,072 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/core-site.xml._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,084 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,087 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/core-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,110 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/hadoop-env.cmd._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,129 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,132 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/hadoop-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,144 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/hadoop-env.sh._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,153 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,155 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/hadoop-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,167 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/hadoop-metrics.properties._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741831_1007{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,174 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741831_1007{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,178 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/hadoop-metrics.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,189 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/hadoop-metrics2.properties._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741832_1008{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,204 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741832_1008{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,208 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/hadoop-metrics2.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,221 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/hadoop-policy.xml._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741833_1009{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,230 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741833_1009{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,232 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/hadoop-policy.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,245 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/hdfs-site.xml._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741834_1010{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,255 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741834_1010{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,257 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/hdfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,268 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/httpfs-env.sh._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741835_1011{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,275 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741835_1011{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,277 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/httpfs-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,288 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/httpfs-log4j.properties._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741836_1012{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,296 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741836_1012{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,298 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/httpfs-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,310 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/httpfs-signature.secret._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741837_1013{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,318 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741837_1013{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,321 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/httpfs-signature.secret._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,334 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/httpfs-site.xml._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741838_1014{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,341 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741838_1014{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,345 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/httpfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,363 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/kms-acls.xml._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741839_1015{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,375 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741839_1015{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,379 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/kms-acls.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,393 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/kms-env.sh._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741840_1016{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,402 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741840_1016{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,404 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/kms-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,419 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/kms-log4j.properties._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741841_1017{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,431 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741841_1017{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,433 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/kms-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,443 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/kms-site.xml._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741842_1018{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,450 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741842_1018{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,456 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/kms-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,468 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/log4j.properties._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741843_1019{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,475 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741843_1019{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,477 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,492 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/mapred-env.cmd._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741844_1020{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,499 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741844_1020{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,502 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/mapred-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,513 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/mapred-env.sh._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741845_1021{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,520 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741845_1021{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,522 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/mapred-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,533 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/mapred-queues.xml.template._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741846_1022{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,540 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741846_1022{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,542 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/mapred-queues.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,558 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/mapred-site.xml.template._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741847_1023{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,567 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741847_1023{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,569 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/mapred-site.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,583 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/slaves._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741848_1024{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,590 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741848_1024{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,597 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/slaves._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,610 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/ssl-client.xml.example._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741849_1025{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,618 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741849_1025{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,620 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/ssl-client.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,638 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/ssl-server.xml.example._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741850_1026{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,645 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741850_1026{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,647 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/ssl-server.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,660 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/yarn-env.cmd._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741851_1027{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,668 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741851_1027{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,670 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/yarn-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,680 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/yarn-env.sh._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741852_1028{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,687 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741852_1028{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,689 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/yarn-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:02,703 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/weiranfang/input/hadoop/yarn-site.xml._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741853_1029{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:09:02,714 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741853_1029{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:09:02,719 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/weiranfang/input/hadoop/yarn-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-507732525_1
2015-12-04 22:09:04,382 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:09:04,383 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:09:34,383 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:09:34,384 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:10:04,384 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:10:04,384 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:10:34,384 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:10:34,384 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:11:04,384 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:11:04,385 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:11:34,385 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:11:34,385 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:12:04,385 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:12:04,385 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:12:34,385 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:12:34,386 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:13:04,386 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:13:04,387 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:13:34,386 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:13:34,386 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:13:54,374 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 181 Total time for transactions(ms): 33 Number of transactions batched in Syncs: 0 Number of syncs: 124 SyncTimes(ms): 175 
2015-12-04 22:13:54,377 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741825_1001 127.0.0.1:50010 
2015-12-04 22:13:54,378 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741826_1002 127.0.0.1:50010 
2015-12-04 22:13:54,378 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741827_1003 127.0.0.1:50010 
2015-12-04 22:13:54,378 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741828_1004 127.0.0.1:50010 
2015-12-04 22:13:54,378 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741829_1005 127.0.0.1:50010 
2015-12-04 22:13:54,378 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741830_1006 127.0.0.1:50010 
2015-12-04 22:13:54,378 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741831_1007 127.0.0.1:50010 
2015-12-04 22:13:54,378 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741832_1008 127.0.0.1:50010 
2015-12-04 22:13:54,378 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741833_1009 127.0.0.1:50010 
2015-12-04 22:13:54,378 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741834_1010 127.0.0.1:50010 
2015-12-04 22:13:54,378 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741835_1011 127.0.0.1:50010 
2015-12-04 22:13:54,378 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741836_1012 127.0.0.1:50010 
2015-12-04 22:13:54,378 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741837_1013 127.0.0.1:50010 
2015-12-04 22:13:54,378 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741838_1014 127.0.0.1:50010 
2015-12-04 22:13:54,379 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741839_1015 127.0.0.1:50010 
2015-12-04 22:13:54,379 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741840_1016 127.0.0.1:50010 
2015-12-04 22:13:54,379 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741841_1017 127.0.0.1:50010 
2015-12-04 22:13:54,379 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741842_1018 127.0.0.1:50010 
2015-12-04 22:13:54,379 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741843_1019 127.0.0.1:50010 
2015-12-04 22:13:54,379 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741844_1020 127.0.0.1:50010 
2015-12-04 22:13:54,379 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741845_1021 127.0.0.1:50010 
2015-12-04 22:13:54,379 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741846_1022 127.0.0.1:50010 
2015-12-04 22:13:54,379 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741847_1023 127.0.0.1:50010 
2015-12-04 22:13:54,379 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741848_1024 127.0.0.1:50010 
2015-12-04 22:13:54,379 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741849_1025 127.0.0.1:50010 
2015-12-04 22:13:54,379 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741850_1026 127.0.0.1:50010 
2015-12-04 22:13:54,379 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741851_1027 127.0.0.1:50010 
2015-12-04 22:13:54,379 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741852_1028 127.0.0.1:50010 
2015-12-04 22:13:54,379 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741853_1029 127.0.0.1:50010 
2015-12-04 22:13:55,735 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741825_1001, blk_1073741826_1002, blk_1073741827_1003, blk_1073741828_1004, blk_1073741829_1005, blk_1073741830_1006, blk_1073741831_1007, blk_1073741832_1008, blk_1073741833_1009, blk_1073741834_1010, blk_1073741835_1011, blk_1073741836_1012, blk_1073741837_1013, blk_1073741838_1014, blk_1073741839_1015, blk_1073741840_1016, blk_1073741841_1017, blk_1073741842_1018, blk_1073741843_1019, blk_1073741844_1020, blk_1073741845_1021, blk_1073741846_1022, blk_1073741847_1023, blk_1073741848_1024, blk_1073741849_1025, blk_1073741850_1026, blk_1073741851_1027, blk_1073741852_1028, blk_1073741853_1029]
2015-12-04 22:14:04,385 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:14:04,386 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:14:34,386 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:14:34,386 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:15:04,386 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:15:04,387 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:15:34,387 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:15:34,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:15:41,432 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 184 Total time for transactions(ms): 34 Number of transactions batched in Syncs: 0 Number of syncs: 127 SyncTimes(ms): 180 
2015-12-04 22:15:41,543 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/capacity-scheduler.xml._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741854_1030{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:41,688 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741854_1030{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:41,693 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/capacity-scheduler.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:41,717 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/configuration.xsl._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741855_1031{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:41,726 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741855_1031{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:41,728 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/configuration.xsl._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:41,744 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/container-executor.cfg._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741856_1032{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:41,753 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741856_1032{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:41,761 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/container-executor.cfg._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:41,778 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/core-site.xml._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741857_1033{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:41,785 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741857_1033{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:41,787 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/core-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:41,804 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/hadoop-env.cmd._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741858_1034{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:41,809 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741858_1034{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:41,811 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/hadoop-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:41,825 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/hadoop-env.sh._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741859_1035{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:41,837 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741859_1035{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:41,838 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/hadoop-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:41,852 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/hadoop-metrics.properties._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741860_1036{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:41,858 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741860_1036{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:41,861 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/hadoop-metrics.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:41,875 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/hadoop-metrics2.properties._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741861_1037{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:41,887 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741861_1037{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:41,889 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/hadoop-metrics2.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:41,904 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/hadoop-policy.xml._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741862_1038{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:41,910 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741862_1038{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:41,915 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/hadoop-policy.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:41,928 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/hdfs-site.xml._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741863_1039{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:41,934 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741863_1039{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:41,937 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/hdfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:41,948 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/httpfs-env.sh._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741864_1040{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:41,955 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741864_1040{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:41,957 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/httpfs-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:41,971 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/httpfs-log4j.properties._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741865_1041{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:41,978 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741865_1041{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:41,980 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/httpfs-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:41,997 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/httpfs-signature.secret._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741866_1042{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:42,004 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741866_1042{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:42,007 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/httpfs-signature.secret._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:42,023 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/httpfs-site.xml._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741867_1043{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:42,029 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741867_1043{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:42,031 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/httpfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:42,046 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/kms-acls.xml._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741868_1044{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:42,051 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741868_1044{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:42,053 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/kms-acls.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:42,064 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/kms-env.sh._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741869_1045{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:42,070 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741869_1045{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:42,073 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/kms-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:42,087 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/kms-log4j.properties._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741870_1046{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:42,098 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741870_1046{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:42,103 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/kms-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:42,133 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/kms-site.xml._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741871_1047{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:42,139 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741871_1047{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:42,141 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/kms-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:42,155 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/log4j.properties._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741872_1048{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:42,162 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741872_1048{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:42,163 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:42,174 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/mapred-env.cmd._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741873_1049{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:42,182 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741873_1049{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:42,185 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/mapred-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:42,201 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/mapred-env.sh._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741874_1050{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:42,210 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741874_1050{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:42,212 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/mapred-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:42,221 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/mapred-queues.xml.template._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741875_1051{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:42,228 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741875_1051{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:42,230 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/mapred-queues.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:42,241 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/mapred-site.xml.template._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741876_1052{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:42,249 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741876_1052{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:42,251 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/mapred-site.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:42,263 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/slaves._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741877_1053{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:42,272 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741877_1053{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:42,274 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/slaves._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:42,283 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/ssl-client.xml.example._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741878_1054{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:42,294 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741878_1054{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:42,297 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/ssl-client.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:42,307 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/ssl-server.xml.example._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741879_1055{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:42,313 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741879_1055{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:42,315 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/ssl-server.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:42,326 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/yarn-env.cmd._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741880_1056{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:42,333 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741880_1056{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:42,336 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/yarn-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:42,345 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/yarn-env.sh._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741881_1057{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:42,351 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741881_1057{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:15:42,353 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/yarn-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:15:42,368 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/yarn-site.xml._COPYING_. BP-1952917173-127.0.1.1-1449292642740 blk_1073741882_1058{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:15:42,374 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741882_1058{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-04 22:15:42,375 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741882_1058{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 690
2015-12-04 22:15:42,778 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/yarn-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_-2051444295_1
2015-12-04 22:16:04,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:16:04,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:16:34,387 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:16:34,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:17:04,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:17:04,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:17:34,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:17:34,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:18:04,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:18:04,389 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:18:34,389 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:18:34,389 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:19:04,389 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:19:04,389 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:19:34,390 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:19:34,390 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:19:56,912 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-04 22:19:56,912 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-04 22:19:56,912 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 3
2015-12-04 22:19:56,912 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 359 Total time for transactions(ms): 40 Number of transactions batched in Syncs: 1 Number of syncs: 244 SyncTimes(ms): 280 
2015-12-04 22:19:56,914 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 359 Total time for transactions(ms): 40 Number of transactions batched in Syncs: 1 Number of syncs: 245 SyncTimes(ms): 282 
2015-12-04 22:19:56,914 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-parallels/dfs/name/current/edits_inprogress_0000000000000000003 -> /tmp/hadoop-parallels/dfs/name/current/edits_0000000000000000003-0000000000000000361
2015-12-04 22:19:56,914 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 362
2015-12-04 22:19:57,038 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 666.67 KB/s
2015-12-04 22:19:57,038 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000361 size 2800 bytes.
2015-12-04 22:19:57,040 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2015-12-04 22:19:57,041 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-parallels/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2015-12-04 22:20:04,390 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:20:04,391 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:20:29,271 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/grep-temp-1276802994/_temporary/0/_temporary/attempt_local894618712_0001_r_000000_0/part-r-00000. BP-1952917173-127.0.1.1-1449292642740 blk_1073741883_1059{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:20:29,393 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741883_1059{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:20:29,403 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/grep-temp-1276802994/_temporary/0/_temporary/attempt_local894618712_0001_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_132275941_1
2015-12-04 22:20:29,561 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/grep-temp-1276802994/_SUCCESS is closed by DFSClient_NONMAPREDUCE_132275941_1
2015-12-04 22:20:30,518 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/output/_temporary/0/_temporary/attempt_local384194407_0002_r_000000_0/part-r-00000. BP-1952917173-127.0.1.1-1449292642740 blk_1073741884_1060{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-04 22:20:30,544 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741884_1060{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-f46deaf3-2033-4196-9670-32be7e60fbca:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-04 22:20:30,546 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/output/_temporary/0/_temporary/attempt_local384194407_0002_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_132275941_1
2015-12-04 22:20:30,598 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_132275941_1
2015-12-04 22:20:31,353 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741883_1059 127.0.0.1:50010 
2015-12-04 22:20:31,786 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741883_1059]
2015-12-04 22:20:34,390 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:20:34,391 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:21:04,391 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:21:04,392 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:21:34,393 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:21:34,393 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:22:04,394 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:22:04,394 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:22:34,395 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:22:34,395 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:23:04,395 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:23:04,396 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:23:34,396 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:23:34,396 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:24:04,396 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:24:04,396 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:24:34,396 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:24:34,397 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:25:04,397 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:25:04,398 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:25:34,398 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:25:34,398 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:26:04,398 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:26:04,399 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:26:34,400 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:26:34,400 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:27:04,400 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:27:04,401 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:27:34,401 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:27:34,401 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:28:04,401 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:28:04,401 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:28:34,400 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:28:34,401 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:29:04,401 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:29:04,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:29:34,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:29:34,403 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:30:04,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:30:04,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:30:34,401 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:30:34,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:31:04,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:31:04,403 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:31:34,403 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-04 22:31:34,404 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-04 22:32:04,404 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-04 22:32:04,404 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-04 22:32:23,752 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-04 22:32:23,767 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 13:05:05,232 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 13:05:05,246 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 13:05:05,249 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-05 13:05:05,553 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 13:05:05,724 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 13:05:05,724 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-05 13:05:05,727 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-05 13:05:05,727 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-05 13:05:05,847 FATAL org.apache.hadoop.conf.Configuration: error parsing conf mapred-site.xml
org.xml.sax.SAXParseException; systemId: file:/home/parallels/Development/hadoop-2.6.0/etc/hadoop/mapred-site.xml; lineNumber: 23; columnNumber: 5; The element type "proterty" must be terminated by the matching end-tag "</proterty>".
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2352)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2340)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2411)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2364)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2281)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:888)
	at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:907)
	at org.apache.hadoop.conf.Configuration.getLong(Configuration.java:1180)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:77)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:66)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:271)
	at org.apache.hadoop.security.UserGroupInformation.setConfiguration(UserGroupInformation.java:299)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:583)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2015-12-05 13:05:05,849 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
java.lang.RuntimeException: org.xml.sax.SAXParseException; systemId: file:/home/parallels/Development/hadoop-2.6.0/etc/hadoop/mapred-site.xml; lineNumber: 23; columnNumber: 5; The element type "proterty" must be terminated by the matching end-tag "</proterty>".
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2517)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2364)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2281)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:888)
	at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:907)
	at org.apache.hadoop.conf.Configuration.getLong(Configuration.java:1180)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:77)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:66)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:271)
	at org.apache.hadoop.security.UserGroupInformation.setConfiguration(UserGroupInformation.java:299)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:583)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
Caused by: org.xml.sax.SAXParseException; systemId: file:/home/parallels/Development/hadoop-2.6.0/etc/hadoop/mapred-site.xml; lineNumber: 23; columnNumber: 5; The element type "proterty" must be terminated by the matching end-tag "</proterty>".
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2352)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2340)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2411)
	... 15 more
2015-12-05 13:05:05,850 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 13:05:05,852 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 13:07:05,748 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 13:07:05,759 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 13:07:05,761 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-05 13:07:06,039 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 13:07:06,145 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 13:07:06,145 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-05 13:07:06,146 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-05 13:07:06,147 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-05 13:07:06,365 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-05 13:07:06,417 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 13:07:06,420 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-05 13:07:06,437 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 13:07:06,443 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-05 13:07:06,444 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 13:07:06,445 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 13:07:06,491 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-05 13:07:06,494 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 13:07:06,531 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-05 13:07:06,531 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 13:07:06,786 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 13:07:06,856 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 13:07:06,856 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 13:07:06,905 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 13:07:06,917 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 13:07:06,973 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 13:07:06,973 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 13:07:06,977 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 13:07:06,979 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 13:07:06
2015-12-05 13:07:06,983 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 13:07:06,983 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:07:06,987 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 13:07:06,987 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 13:07:07,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 13:07:07,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 13:07:07,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 13:07:07,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 13:07:07,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 13:07:07,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 13:07:07,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 13:07:07,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 13:07:07,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 13:07:07,037 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 13:07:07,037 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 13:07:07,037 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 13:07:07,037 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 13:07:07,039 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 13:07:07,249 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 13:07:07,249 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:07:07,250 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 13:07:07,250 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 13:07:07,252 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 13:07:07,273 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 13:07:07,273 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:07:07,273 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 13:07:07,273 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 13:07:07,276 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 13:07:07,276 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 13:07:07,277 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 13:07:07,286 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-05 13:07:07,286 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-05 13:07:07,295 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-05 13:07:07,295 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:07:07,295 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-05 13:07:07,295 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-05 13:07:07,300 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 13:07:07,300 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 13:07:07,300 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 13:07:07,301 WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-parallels/dfs/name does not exist
2015-12-05 13:07:07,302 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-parallels/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:313)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1020)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:739)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:536)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:595)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2015-12-05 13:07:07,306 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 13:07:07,408 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2015-12-05 13:07:07,408 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2015-12-05 13:07:07,409 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2015-12-05 13:07:07,409 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-parallels/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:313)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1020)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:739)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:536)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:595)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2015-12-05 13:07:07,411 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 13:07:07,412 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 13:15:42,546 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 13:15:42,559 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 13:15:42,561 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-05 13:15:42,863 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 13:15:42,994 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 13:15:42,995 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-05 13:15:42,996 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-05 13:15:42,997 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-05 13:15:43,194 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-05 13:15:43,243 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 13:15:43,248 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-05 13:15:43,262 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 13:15:43,264 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-05 13:15:43,264 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 13:15:43,264 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 13:15:43,293 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-05 13:15:43,295 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 13:15:43,318 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-05 13:15:43,318 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 13:15:43,512 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 13:15:43,572 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 13:15:43,572 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 13:15:43,609 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 13:15:43,616 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 13:15:43,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 13:15:43,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 13:15:43,653 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 13:15:43,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 13:15:43
2015-12-05 13:15:43,656 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 13:15:43,656 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:15:43,658 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 13:15:43,658 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 13:15:43,685 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 13:15:43,686 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 13:15:43,686 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 13:15:43,686 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 13:15:43,686 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 13:15:43,686 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 13:15:43,686 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 13:15:43,686 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 13:15:43,686 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 13:15:43,691 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 13:15:43,691 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 13:15:43,691 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 13:15:43,691 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 13:15:43,693 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 13:15:43,861 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 13:15:43,861 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:15:43,862 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 13:15:43,862 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 13:15:43,864 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 13:15:43,875 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 13:15:43,875 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:15:43,875 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 13:15:43,875 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 13:15:43,877 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 13:15:43,877 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 13:15:43,877 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 13:15:43,878 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-05 13:15:43,878 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-05 13:15:43,880 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-05 13:15:43,880 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:15:43,880 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-05 13:15:43,880 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-05 13:15:43,884 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 13:15:43,884 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 13:15:43,884 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 13:15:43,892 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-parallels/dfs/name/in_use.lock acquired by nodename 19682@ubuntu
2015-12-05 13:15:43,977 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-parallels/dfs/name/current
2015-12-05 13:15:43,977 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2015-12-05 13:15:44,026 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-05 13:15:44,070 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-05 13:15:44,070 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-parallels/dfs/name/current/fsimage_0000000000000000000
2015-12-05 13:15:44,076 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-12-05 13:15:44,076 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2015-12-05 13:15:44,217 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-05 13:15:44,217 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 333 msecs
2015-12-05 13:15:44,542 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-05 13:15:44,550 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-05 13:15:44,564 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-05 13:15:44,598 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-05 13:15:44,610 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-05 13:15:44,610 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-05 13:15:44,610 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-05 13:15:44,610 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2015-12-05 13:15:44,611 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2015-12-05 13:15:44,611 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-12-05 13:15:44,633 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2015-12-05 13:15:44,633 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-05 13:15:44,633 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-12-05 13:15:44,634 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-05 13:15:44,634 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-05 13:15:44,634 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 23 msec
2015-12-05 13:15:44,662 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-05 13:15:44,671 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-05 13:15:44,679 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-05 13:15:44,679 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-05 13:15:44,695 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-05 13:15:44,695 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 4720824 milliseconds
2015-12-05 13:15:44,697 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-05 13:15:45,148 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=aac9c00d-3ead-4052-8364-863df82ca9d6, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-8e6ab86f-c97b-4c9f-9515-70a87b98ae37;nsid=889546093;c=0) storage aac9c00d-3ead-4052-8364-863df82ca9d6
2015-12-05 13:15:45,151 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-05 13:15:45,152 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-05 13:15:45,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-05 13:15:45,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-74c8ca63-148d-4729-b94d-db8eb4578ed3 for DN 127.0.0.1:50010
2015-12-05 13:15:45,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-74c8ca63-148d-4729-b94d-db8eb4578ed3,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-12-05 13:15:45,277 INFO BlockStateChange: BLOCK* processReport: from storage DS-74c8ca63-148d-4729-b94d-db8eb4578ed3 node DatanodeRegistration(127.0.0.1, datanodeUuid=aac9c00d-3ead-4052-8364-863df82ca9d6, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-8e6ab86f-c97b-4c9f-9515-70a87b98ae37;nsid=889546093;c=0), blocks: 0, hasStaleStorages: false, processing time: 1 msecs
2015-12-05 13:16:14,695 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 13:16:14,696 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 13:16:32,585 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-05 13:16:32,585 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-05 13:16:32,585 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2015-12-05 13:16:32,592 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 31 
2015-12-05 13:16:32,593 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-parallels/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-parallels/dfs/name/current/edits_0000000000000000001-0000000000000000002
2015-12-05 13:16:32,597 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2015-12-05 13:16:33,544 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-05 13:16:33,545 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 356 bytes.
2015-12-05 13:16:33,548 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2015-12-05 13:16:44,695 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 13:16:44,696 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 13:17:14,695 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 13:17:14,696 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 13:17:20,301 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 13:17:20,303 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 13:18:09,066 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 13:18:09,079 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 13:18:09,082 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-05 13:18:09,379 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 13:18:09,508 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 13:18:09,508 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-05 13:18:09,510 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-05 13:18:09,510 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-05 13:18:09,711 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-05 13:18:09,748 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 13:18:09,754 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-05 13:18:09,767 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 13:18:09,769 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-05 13:18:09,769 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 13:18:09,770 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 13:18:09,798 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-05 13:18:09,801 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 13:18:09,821 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-05 13:18:09,821 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 13:18:10,017 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 13:18:10,086 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 13:18:10,086 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 13:18:10,132 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 13:18:10,143 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 13:18:10,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 13:18:10,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 13:18:10,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 13:18:10,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 13:18:10
2015-12-05 13:18:10,201 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 13:18:10,201 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:18:10,202 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 13:18:10,203 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 13:18:10,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 13:18:10,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 13:18:10,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 13:18:10,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 13:18:10,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 13:18:10,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 13:18:10,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 13:18:10,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 13:18:10,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 13:18:10,244 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 13:18:10,244 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 13:18:10,244 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 13:18:10,244 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 13:18:10,246 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 13:18:10,499 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 13:18:10,499 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:18:10,500 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 13:18:10,500 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 13:18:10,502 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 13:18:10,515 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 13:18:10,515 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:18:10,515 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 13:18:10,515 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 13:18:10,516 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 13:18:10,516 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 13:18:10,516 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 13:18:10,517 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-05 13:18:10,517 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-05 13:18:10,521 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-05 13:18:10,521 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:18:10,522 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-05 13:18:10,522 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-05 13:18:10,526 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 13:18:10,526 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 13:18:10,526 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 13:18:10,538 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-parallels/dfs/name/in_use.lock acquired by nodename 21103@ubuntu
2015-12-05 13:18:10,631 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-parallels/dfs/name/current
2015-12-05 13:18:10,661 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-parallels/dfs/name/current/edits_inprogress_0000000000000000003 -> /tmp/hadoop-parallels/dfs/name/current/edits_0000000000000000003-0000000000000000003
2015-12-05 13:18:10,710 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-05 13:18:10,738 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-05 13:18:10,738 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 2 from /tmp/hadoop-parallels/dfs/name/current/fsimage_0000000000000000002
2015-12-05 13:18:10,738 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4c550889 expecting start txid #3
2015-12-05 13:18:10,738 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-parallels/dfs/name/current/edits_0000000000000000003-0000000000000000003
2015-12-05 13:18:10,740 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/tmp/hadoop-parallels/dfs/name/current/edits_0000000000000000003-0000000000000000003' to transaction ID 3
2015-12-05 13:18:10,742 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-parallels/dfs/name/current/edits_0000000000000000003-0000000000000000003 of size 1048576 edits # 1 loaded in 0 seconds
2015-12-05 13:18:10,744 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-12-05 13:18:10,744 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 4
2015-12-05 13:18:10,842 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-05 13:18:10,843 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 317 msecs
2015-12-05 13:18:11,076 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-05 13:18:11,084 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-05 13:18:11,095 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-05 13:18:11,124 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-05 13:18:11,132 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-05 13:18:11,132 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-05 13:18:11,132 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-05 13:18:11,132 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2015-12-05 13:18:11,132 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2015-12-05 13:18:11,132 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-12-05 13:18:11,155 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2015-12-05 13:18:11,155 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-05 13:18:11,155 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-12-05 13:18:11,155 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-05 13:18:11,155 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-05 13:18:11,155 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 22 msec
2015-12-05 13:18:11,171 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-05 13:18:11,173 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-05 13:18:11,174 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-05 13:18:11,174 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-05 13:18:11,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-05 13:18:11,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 4867310 milliseconds
2015-12-05 13:18:11,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 13:18:17,564 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=aac9c00d-3ead-4052-8364-863df82ca9d6, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-8e6ab86f-c97b-4c9f-9515-70a87b98ae37;nsid=889546093;c=0) storage aac9c00d-3ead-4052-8364-863df82ca9d6
2015-12-05 13:18:17,567 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-05 13:18:17,568 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-05 13:18:17,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-05 13:18:17,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-74c8ca63-148d-4729-b94d-db8eb4578ed3 for DN 127.0.0.1:50010
2015-12-05 13:18:17,685 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-74c8ca63-148d-4729-b94d-db8eb4578ed3,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-12-05 13:18:17,685 INFO BlockStateChange: BLOCK* processReport: from storage DS-74c8ca63-148d-4729-b94d-db8eb4578ed3 node DatanodeRegistration(127.0.0.1, datanodeUuid=aac9c00d-3ead-4052-8364-863df82ca9d6, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-8e6ab86f-c97b-4c9f-9515-70a87b98ae37;nsid=889546093;c=0), blocks: 0, hasStaleStorages: false, processing time: 1 msecs
2015-12-05 13:18:39,138 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-05 13:18:41,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 13:18:41,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 13:19:11,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 13:19:11,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 13:19:25,396 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-05 13:19:25,396 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-05 13:19:25,396 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 4
2015-12-05 13:19:25,396 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 6 
2015-12-05 13:19:25,398 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 8 
2015-12-05 13:19:25,399 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-parallels/dfs/name/current/edits_inprogress_0000000000000000004 -> /tmp/hadoop-parallels/dfs/name/current/edits_0000000000000000004-0000000000000000005
2015-12-05 13:19:25,400 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 6
2015-12-05 13:19:26,083 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-05 13:19:26,084 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000005 size 356 bytes.
2015-12-05 13:19:26,088 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2015-12-05 13:19:26,088 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-parallels/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2015-12-05 13:19:41,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 13:19:41,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 13:20:11,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 13:20:11,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 13:20:41,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 13:20:41,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 13:21:11,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 13:21:11,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 13:21:18,386 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 19 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 6 
2015-12-05 13:21:41,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 13:21:41,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 13:22:11,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 13:22:11,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 13:22:41,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 13:22:41,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 13:23:11,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 13:23:11,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-05 13:23:41,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 13:23:41,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 13:24:11,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 13:24:11,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 13:24:41,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 13:24:41,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 13:25:03,968 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 21 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 72 
2015-12-05 13:25:04,406 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/capacity-scheduler.xml._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:05,241 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-05 13:25:05,258 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 4436
2015-12-05 13:25:05,693 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/capacity-scheduler.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:05,738 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/configuration.xsl._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:05,751 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:05,754 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/configuration.xsl._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:05,781 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/container-executor.cfg._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:05,793 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:05,802 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/container-executor.cfg._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:05,830 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/core-site.xml._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:05,847 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:05,850 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/core-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:05,873 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/hadoop-env.cmd._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:05,888 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:05,890 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/hadoop-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:05,916 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/hadoop-env.sh._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:05,932 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741830_1006{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-05 13:25:05,932 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741830_1006{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 4236
2015-12-05 13:25:06,336 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/hadoop-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:06,353 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/hadoop-metrics.properties._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741831_1007{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:06,372 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741831_1007{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:06,377 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/hadoop-metrics.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:06,391 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/hadoop-metrics2.properties._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741832_1008{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:06,403 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741832_1008{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:06,405 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/hadoop-metrics2.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:06,420 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/hadoop-policy.xml._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741833_1009{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:06,430 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741833_1009{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:06,432 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/hadoop-policy.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:06,449 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/hdfs-site.xml._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741834_1010{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:06,460 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741834_1010{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-05 13:25:06,461 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741834_1010{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 855
2015-12-05 13:25:06,865 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/hdfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:06,881 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/httpfs-env.sh._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741835_1011{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:06,890 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741835_1011{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:06,894 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/httpfs-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:06,913 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/httpfs-log4j.properties._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741836_1012{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:06,925 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741836_1012{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:06,928 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/httpfs-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:06,945 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/httpfs-signature.secret._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741837_1013{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:06,955 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741837_1013{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:06,960 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/httpfs-signature.secret._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:06,976 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/httpfs-site.xml._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741838_1014{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:06,986 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741838_1014{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:06,990 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/httpfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:07,007 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/kms-acls.xml._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741839_1015{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:07,017 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741839_1015{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:07,020 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/kms-acls.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:07,038 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/kms-env.sh._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741840_1016{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:07,046 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741840_1016{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:07,050 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/kms-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:07,072 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/kms-log4j.properties._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741841_1017{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:07,083 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741841_1017{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:07,086 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/kms-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:07,104 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/kms-site.xml._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741842_1018{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:07,111 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741842_1018{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:07,114 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/kms-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:07,128 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/log4j.properties._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741843_1019{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:07,143 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741843_1019{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:07,144 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:07,164 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/mapred-env.cmd._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741844_1020{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:07,173 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741844_1020{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:07,176 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/mapred-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:07,190 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/mapred-env.sh._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741845_1021{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:07,205 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741845_1021{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:07,208 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/mapred-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:07,228 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/mapred-queues.xml.template._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741846_1022{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:07,245 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741846_1022{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:07,248 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/mapred-queues.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:07,261 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/mapred-site.xml._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741847_1023{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:07,270 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741847_1023{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:07,271 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/mapred-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:07,286 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/mapred-site.xml.template._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741848_1024{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:07,296 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741848_1024{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:07,299 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/mapred-site.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:07,314 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/slaves._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741849_1025{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:07,325 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741849_1025{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:07,330 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/slaves._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:07,347 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/ssl-client.xml.example._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741850_1026{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:07,355 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741850_1026{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:07,358 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/ssl-client.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:07,374 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/ssl-server.xml.example._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741851_1027{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:07,382 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741851_1027{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:07,385 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/ssl-server.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:07,399 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/yarn-env.cmd._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741852_1028{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:07,407 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741852_1028{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:07,410 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/yarn-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:07,427 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/yarn-env.sh._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741853_1029{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:07,433 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741853_1029{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:07,438 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/yarn-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:07,452 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/yarn-site.xml._COPYING_. BP-1595073404-127.0.1.1-1449350125252 blk_1073741854_1030{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:25:07,460 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741854_1030{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:25:07,467 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/yarn-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_2054405698_1
2015-12-05 13:25:11,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 13:25:11,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 13:25:20,515 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-05 13:25:41,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 13:25:41,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 13:26:11,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 13:26:11,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 13:26:41,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 13:26:41,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 13:26:55,578 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 189 Total time for transactions(ms): 28 Number of transactions batched in Syncs: 3 Number of syncs: 125 SyncTimes(ms): 317 
2015-12-05 13:26:56,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449350578721_0001/job.split
2015-12-05 13:26:56,275 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449350578721_0001/job.split. BP-1595073404-127.0.1.1-1449350125252 blk_1073741855_1031{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:26:56,417 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741855_1031{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:26:56,425 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449350578721_0001/job.split is closed by DFSClient_NONMAPREDUCE_-1183674985_1
2015-12-05 13:26:56,442 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449350578721_0001/job.splitmetainfo. BP-1595073404-127.0.1.1-1449350125252 blk_1073741856_1032{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:26:56,449 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741856_1032{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:26:56,452 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449350578721_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1183674985_1
2015-12-05 13:26:56,648 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449350578721_0001/job.xml. BP-1595073404-127.0.1.1-1449350125252 blk_1073741857_1033{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:26:56,656 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741857_1033{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:26:56,658 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449350578721_0001/job.xml is closed by DFSClient_NONMAPREDUCE_-1183674985_1
2015-12-05 13:27:08,659 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449350578721_0001/job_1449350578721_0001_1_conf.xml. BP-1595073404-127.0.1.1-1449350125252 blk_1073741858_1034{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:27:08,818 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741858_1034{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 13:27:08,830 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449350578721_0001/job_1449350578721_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_2041276604_1
2015-12-05 13:27:11,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30005 milliseconds
2015-12-05 13:27:11,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 6 millisecond(s).
2015-12-05 13:27:41,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30011 milliseconds
2015-12-05 13:27:41,556 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 350 millisecond(s).
2015-12-05 13:27:50,412 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449350578721_0001/job_1449350578721_0001_1.jhist. BP-1595073404-127.0.1.1-1449350125252 blk_1073741859_1035{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-74c8ca63-148d-4729-b94d-db8eb4578ed3:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 13:27:50,718 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449350578721_0001/job_1449350578721_0001_1.jhist for DFSClient_NONMAPREDUCE_2041276604_1
2015-12-05 18:29:19,148 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 18:29:19,168 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 18:29:19,174 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-05 18:29:19,503 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 18:29:19,647 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 18:29:19,648 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-05 18:29:19,651 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-05 18:29:19,652 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-05 18:29:19,866 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-05 18:29:19,914 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 18:29:19,917 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-05 18:29:19,929 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 18:29:19,931 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-05 18:29:19,932 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 18:29:19,932 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 18:29:19,974 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-05 18:29:19,976 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 18:29:20,011 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-05 18:29:20,011 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 18:29:20,268 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 18:29:20,336 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 18:29:20,336 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 18:29:20,398 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 18:29:20,418 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 18:29:20,471 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 18:29:20,471 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 18:29:20,479 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 18:29:20,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 18:29:20
2015-12-05 18:29:20,484 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 18:29:20,484 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 18:29:20,488 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 18:29:20,488 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 18:29:20,523 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 18:29:20,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 18:29:20,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 18:29:20,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 18:29:20,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 18:29:20,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 18:29:20,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 18:29:20,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 18:29:20,524 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 18:29:20,529 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 18:29:20,529 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 18:29:20,529 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 18:29:20,529 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 18:29:20,531 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 18:29:20,784 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 18:29:20,784 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 18:29:20,784 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 18:29:20,784 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 18:29:20,786 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 18:29:20,800 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 18:29:20,801 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 18:29:20,801 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 18:29:20,801 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 18:29:20,802 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 18:29:20,802 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 18:29:20,802 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 18:29:20,803 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-05 18:29:20,804 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-05 18:29:20,806 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-05 18:29:20,806 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 18:29:20,806 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-05 18:29:20,806 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-05 18:29:20,810 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 18:29:20,811 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 18:29:20,811 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 18:29:20,811 WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-parallels/dfs/name does not exist
2015-12-05 18:29:20,813 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-parallels/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:313)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1020)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:739)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:536)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:595)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2015-12-05 18:29:20,822 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 18:29:20,924 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2015-12-05 18:29:20,925 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2015-12-05 18:29:20,925 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2015-12-05 18:29:20,925 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-parallels/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:313)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1020)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:739)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:536)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:595)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2015-12-05 18:29:20,927 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 18:29:20,928 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 18:53:13,292 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 18:53:13,312 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 18:53:13,315 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-05 18:53:13,609 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 18:53:13,744 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 18:53:13,744 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-05 18:53:13,746 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-05 18:53:13,747 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-05 18:53:13,943 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-05 18:53:13,993 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 18:53:13,997 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-05 18:53:14,009 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 18:53:14,014 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-05 18:53:14,014 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 18:53:14,015 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 18:53:14,059 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-05 18:53:14,062 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 18:53:14,084 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-05 18:53:14,084 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 18:53:14,300 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 18:53:14,375 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 18:53:14,375 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 18:53:14,436 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 18:53:14,447 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 18:53:14,502 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 18:53:14,502 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 18:53:14,507 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 18:53:14,509 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 18:53:14
2015-12-05 18:53:14,513 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 18:53:14,513 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 18:53:14,516 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 18:53:14,516 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 18:53:14,549 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 18:53:14,550 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 18:53:14,550 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 18:53:14,550 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 18:53:14,550 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 18:53:14,550 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 18:53:14,550 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 18:53:14,550 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 18:53:14,550 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 18:53:14,556 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 18:53:14,556 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 18:53:14,556 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 18:53:14,556 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 18:53:14,557 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 18:53:14,790 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 18:53:14,790 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 18:53:14,790 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 18:53:14,790 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 18:53:14,793 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 18:53:14,804 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 18:53:14,804 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 18:53:14,805 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 18:53:14,805 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 18:53:14,806 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 18:53:14,806 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 18:53:14,806 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 18:53:14,807 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-05 18:53:14,807 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-05 18:53:14,810 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-05 18:53:14,810 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 18:53:14,811 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-05 18:53:14,811 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-05 18:53:14,816 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 18:53:14,816 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 18:53:14,816 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 18:53:14,818 WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-parallels/dfs/name does not exist
2015-12-05 18:53:14,819 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-parallels/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:313)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1020)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:739)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:536)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:595)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2015-12-05 18:53:14,823 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 18:53:14,824 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2015-12-05 18:53:14,824 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2015-12-05 18:53:14,824 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2015-12-05 18:53:14,824 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-parallels/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:313)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1020)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:739)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:536)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:595)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2015-12-05 18:53:14,826 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 18:53:14,828 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 19:07:48,282 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 19:07:48,296 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 19:07:48,298 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-05 19:07:48,568 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 19:07:48,707 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 19:07:48,707 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-05 19:07:48,710 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-05 19:07:48,711 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-05 19:07:48,926 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-05 19:07:48,978 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 19:07:48,981 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-05 19:07:48,996 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 19:07:48,998 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-05 19:07:48,998 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 19:07:48,998 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 19:07:49,027 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-05 19:07:49,028 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 19:07:49,054 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-05 19:07:49,055 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 19:07:49,295 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 19:07:49,339 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 19:07:49,339 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 19:07:49,373 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 19:07:49,380 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 19:07:49,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 19:07:49,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 19:07:49,413 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 19:07:49,415 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 19:07:49
2015-12-05 19:07:49,416 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 19:07:49,416 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 19:07:49,418 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 19:07:49,418 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 19:07:49,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 19:07:49,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 19:07:49,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 19:07:49,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 19:07:49,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 19:07:49,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 19:07:49,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 19:07:49,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 19:07:49,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 19:07:49,455 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 19:07:49,455 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 19:07:49,455 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 19:07:49,455 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 19:07:49,456 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 19:07:49,603 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 19:07:49,603 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 19:07:49,603 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 19:07:49,603 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 19:07:49,605 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 19:07:49,622 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 19:07:49,622 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 19:07:49,622 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 19:07:49,622 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 19:07:49,624 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 19:07:49,624 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 19:07:49,624 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 19:07:49,626 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-05 19:07:49,626 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-05 19:07:49,633 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-05 19:07:49,633 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 19:07:49,634 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-05 19:07:49,634 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-05 19:07:49,639 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 19:07:49,639 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 19:07:49,639 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 19:07:49,658 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-parallels/dfs/name/in_use.lock acquired by nodename 8726@ubuntu
2015-12-05 19:07:49,748 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-parallels/dfs/name/current
2015-12-05 19:07:49,749 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2015-12-05 19:07:49,786 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-05 19:07:49,820 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-05 19:07:49,821 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-parallels/dfs/name/current/fsimage_0000000000000000000
2015-12-05 19:07:49,825 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-12-05 19:07:49,826 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2015-12-05 19:07:49,964 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-05 19:07:49,964 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 325 msecs
2015-12-05 19:07:50,285 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-05 19:07:50,292 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-05 19:07:50,306 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-05 19:07:50,338 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-05 19:07:50,347 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-05 19:07:50,347 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-05 19:07:50,347 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-05 19:07:50,350 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2015-12-05 19:07:50,350 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2015-12-05 19:07:50,350 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-12-05 19:07:50,363 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2015-12-05 19:07:50,363 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-05 19:07:50,363 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-12-05 19:07:50,363 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-05 19:07:50,364 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-05 19:07:50,364 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 14 msec
2015-12-05 19:07:50,396 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-05 19:07:50,401 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-05 19:07:50,401 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-05 19:07:50,402 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-05 19:07:50,407 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-05 19:07:50,407 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 11446996 milliseconds
2015-12-05 19:07:50,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-05 19:07:53,442 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=70e40ee2-5188-4a05-ac21-5d90b743dd79, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-4c805491-133b-45fe-ad2f-46e6c9c45a4a;nsid=208897031;c=0) storage 70e40ee2-5188-4a05-ac21-5d90b743dd79
2015-12-05 19:07:53,445 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-05 19:07:53,446 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-05 19:07:53,547 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-05 19:07:53,548 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-54135721-87d9-4976-9bbd-05ba3b9871ae for DN 127.0.0.1:50010
2015-12-05 19:07:53,588 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-54135721-87d9-4976-9bbd-05ba3b9871ae,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-12-05 19:07:53,589 INFO BlockStateChange: BLOCK* processReport: from storage DS-54135721-87d9-4976-9bbd-05ba3b9871ae node DatanodeRegistration(127.0.0.1, datanodeUuid=70e40ee2-5188-4a05-ac21-5d90b743dd79, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-4c805491-133b-45fe-ad2f-46e6c9c45a4a;nsid=208897031;c=0), blocks: 0, hasStaleStorages: false, processing time: 1 msecs
2015-12-05 19:08:08,456 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-05 19:08:20,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:08:20,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:08:30,143 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-05 19:08:30,143 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-05 19:08:30,144 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2015-12-05 19:08:30,146 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 21 
2015-12-05 19:08:30,148 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-parallels/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-parallels/dfs/name/current/edits_0000000000000000001-0000000000000000002
2015-12-05 19:08:30,154 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2015-12-05 19:08:31,144 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-05 19:08:31,144 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 355 bytes.
2015-12-05 19:08:31,152 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2015-12-05 19:08:50,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:08:50,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:09:20,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:09:20,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:09:50,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:09:50,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:10:20,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:10:20,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:10:50,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:10:50,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:11:20,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:11:20,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:11:50,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:11:50,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:12:20,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:12:20,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:12:39,616 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 30 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 7 
2015-12-05 19:12:50,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:12:50,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:13:20,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:13:20,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:13:50,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:13:50,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:14:20,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:14:20,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:14:50,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:14:50,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:15:20,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:15:20,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:15:50,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:15:50,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:16:20,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:16:20,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:16:50,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:16:50,412 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:17:20,412 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:17:20,412 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:17:50,413 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:17:50,414 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:18:20,414 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:18:20,415 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:18:50,415 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:18:50,415 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:19:20,416 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:19:20,416 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:19:50,415 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:19:50,416 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:20:20,416 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:20:20,417 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:20:50,418 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-05 19:20:50,418 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:21:20,420 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:21:20,420 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:21:33,765 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 19:21:33,768 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 19:22:32,044 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 19:22:32,060 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 19:22:32,064 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-05 19:22:32,394 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 19:22:32,554 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 19:22:32,554 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-05 19:22:32,556 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-05 19:22:32,556 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-05 19:22:32,759 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-05 19:22:32,816 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 19:22:32,820 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-05 19:22:32,833 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 19:22:32,835 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-05 19:22:32,835 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 19:22:32,835 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 19:22:32,872 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-05 19:22:32,874 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 19:22:32,895 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-05 19:22:32,895 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 19:22:33,123 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 19:22:33,180 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 19:22:33,180 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 19:22:33,235 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 19:22:33,243 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 19:22:33,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 19:22:33,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 19:22:33,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 19:22:33,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 19:22:33
2015-12-05 19:22:33,304 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 19:22:33,304 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 19:22:33,306 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 19:22:33,306 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 19:22:33,339 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 19:22:33,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 19:22:33,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 19:22:33,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 19:22:33,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 19:22:33,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 19:22:33,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 19:22:33,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 19:22:33,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 19:22:33,346 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 19:22:33,346 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 19:22:33,346 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 19:22:33,346 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 19:22:33,348 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 19:22:33,572 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 19:22:33,572 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 19:22:33,572 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 19:22:33,572 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 19:22:33,574 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 19:22:33,585 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 19:22:33,585 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 19:22:33,585 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 19:22:33,585 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 19:22:33,586 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 19:22:33,586 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 19:22:33,587 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 19:22:33,588 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-05 19:22:33,588 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-05 19:22:33,590 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-05 19:22:33,590 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 19:22:33,591 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-05 19:22:33,591 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-05 19:22:33,595 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 19:22:33,595 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 19:22:33,595 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 19:22:33,606 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-parallels/dfs/name/in_use.lock acquired by nodename 12415@ubuntu
2015-12-05 19:22:33,683 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-parallels/dfs/name/current
2015-12-05 19:22:33,749 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-parallels/dfs/name/current/edits_inprogress_0000000000000000003 -> /tmp/hadoop-parallels/dfs/name/current/edits_0000000000000000003-0000000000000000007
2015-12-05 19:22:33,782 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-05 19:22:33,806 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-05 19:22:33,807 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 2 from /tmp/hadoop-parallels/dfs/name/current/fsimage_0000000000000000002
2015-12-05 19:22:33,807 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2a79d4b1 expecting start txid #3
2015-12-05 19:22:33,807 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-parallels/dfs/name/current/edits_0000000000000000003-0000000000000000007
2015-12-05 19:22:33,809 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/tmp/hadoop-parallels/dfs/name/current/edits_0000000000000000003-0000000000000000007' to transaction ID 3
2015-12-05 19:22:33,817 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-parallels/dfs/name/current/edits_0000000000000000003-0000000000000000007 of size 1048576 edits # 5 loaded in 0 seconds
2015-12-05 19:22:33,817 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-12-05 19:22:33,818 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 8
2015-12-05 19:22:33,922 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-05 19:22:33,922 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 327 msecs
2015-12-05 19:22:34,105 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-05 19:22:34,113 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-05 19:22:34,128 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-05 19:22:34,192 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-05 19:22:34,208 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-05 19:22:34,208 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-05 19:22:34,208 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-05 19:22:34,211 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2015-12-05 19:22:34,211 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2015-12-05 19:22:34,211 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-12-05 19:22:34,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2015-12-05 19:22:34,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-05 19:22:34,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-12-05 19:22:34,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-05 19:22:34,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-05 19:22:34,231 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 18 msec
2015-12-05 19:22:34,249 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-05 19:22:34,250 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-05 19:22:34,252 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-05 19:22:34,253 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-05 19:22:34,257 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-05 19:22:34,257 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 12330845 milliseconds
2015-12-05 19:22:34,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-05 19:22:40,694 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=70e40ee2-5188-4a05-ac21-5d90b743dd79, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-4c805491-133b-45fe-ad2f-46e6c9c45a4a;nsid=208897031;c=0) storage 70e40ee2-5188-4a05-ac21-5d90b743dd79
2015-12-05 19:22:40,708 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-05 19:22:40,709 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-05 19:22:40,804 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-05 19:22:40,804 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-54135721-87d9-4976-9bbd-05ba3b9871ae for DN 127.0.0.1:50010
2015-12-05 19:22:40,836 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-54135721-87d9-4976-9bbd-05ba3b9871ae,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-12-05 19:22:40,836 INFO BlockStateChange: BLOCK* processReport: from storage DS-54135721-87d9-4976-9bbd-05ba3b9871ae node DatanodeRegistration(127.0.0.1, datanodeUuid=70e40ee2-5188-4a05-ac21-5d90b743dd79, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-4c805491-133b-45fe-ad2f-46e6c9c45a4a;nsid=208897031;c=0), blocks: 0, hasStaleStorages: false, processing time: 1 msecs
2015-12-05 19:22:54,416 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-05 19:23:04,257 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:23:04,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:23:34,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:23:34,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:23:48,296 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-05 19:23:48,296 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-05 19:23:48,296 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 8
2015-12-05 19:23:48,296 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 5 
2015-12-05 19:23:48,298 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 7 
2015-12-05 19:23:48,299 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-parallels/dfs/name/current/edits_inprogress_0000000000000000008 -> /tmp/hadoop-parallels/dfs/name/current/edits_0000000000000000008-0000000000000000009
2015-12-05 19:23:48,299 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 10
2015-12-05 19:23:48,903 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-05 19:23:48,903 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000009 size 622 bytes.
2015-12-05 19:23:48,906 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2015-12-05 19:23:48,906 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-parallels/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2015-12-05 19:24:04,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:24:04,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:24:34,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:24:34,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:25:04,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:25:04,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:25:34,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:25:34,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-05 19:26:04,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:26:04,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:26:34,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:26:34,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:27:04,263 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:27:04,263 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:27:34,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:27:34,263 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:28:04,263 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:28:04,263 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:28:34,263 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:28:34,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:29:04,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:29:04,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:29:34,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:29:34,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:30:04,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:30:04,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:30:34,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:30:34,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:31:04,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:31:04,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:31:34,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:31:34,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:32:04,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:32:04,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:32:34,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:32:34,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:33:04,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:33:04,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:33:34,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:33:34,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:34:04,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:34:04,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:34:34,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:34:34,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:35:04,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:35:04,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:35:34,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:35:34,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:36:04,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:36:04,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:36:34,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:36:34,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:37:04,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:37:04,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:37:34,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:37:34,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:38:04,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:38:04,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:38:34,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:38:34,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:39:04,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:39:04,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:39:34,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:39:34,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:40:04,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:40:04,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:40:34,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:40:34,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:41:04,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:41:04,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:41:34,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:41:34,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:42:04,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:42:04,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:42:34,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:42:34,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:43:04,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:43:04,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:43:34,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:43:34,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:44:04,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:44:04,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:44:34,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:44:34,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:45:04,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:45:04,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:45:21,827 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-05 19:45:34,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:45:34,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:46:04,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:46:04,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:46:34,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:46:34,281 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:47:04,281 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:47:04,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:47:34,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:47:34,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:48:04,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:48:04,284 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:48:34,284 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:48:34,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:49:04,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-05 19:49:04,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:49:34,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:49:34,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:50:04,288 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:50:04,288 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:50:34,288 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:50:34,288 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:51:04,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:51:04,288 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:51:34,288 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:51:34,288 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:51:43,871 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 6 
2015-12-05 19:51:43,946 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/file01._COPYING_. BP-1164591051-127.0.1.1-1449371237756 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-54135721-87d9-4976-9bbd-05ba3b9871ae:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 19:51:44,416 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-54135721-87d9-4976-9bbd-05ba3b9871ae:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-05 19:51:44,416 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2015-12-05 19:51:44,423 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-54135721-87d9-4976-9bbd-05ba3b9871ae:NORMAL:127.0.0.1:50010|RBW]]} size 22
2015-12-05 19:51:44,831 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/file01._COPYING_ is closed by DFSClient_NONMAPREDUCE_682048098_1
2015-12-05 19:51:52,749 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/file02._COPYING_. BP-1164591051-127.0.1.1-1449371237756 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-54135721-87d9-4976-9bbd-05ba3b9871ae:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 19:51:52,902 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-54135721-87d9-4976-9bbd-05ba3b9871ae:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 19:51:52,907 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/file02._COPYING_ is closed by DFSClient_NONMAPREDUCE_1010199021_1
2015-12-05 19:52:04,289 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:52:04,289 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:52:34,289 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:52:34,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:53:04,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:53:04,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:53:34,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:53:34,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:54:04,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:54:04,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:54:34,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:54:34,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 4 millisecond(s).
2015-12-05 19:55:04,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:55:04,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:55:34,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:55:34,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:56:04,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:56:04,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:56:34,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:56:34,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:57:04,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:57:04,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:57:34,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:57:34,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:58:04,296 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:58:04,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:58:34,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 19:58:34,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 19:59:04,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:59:04,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 19:59:34,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 19:59:34,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 20:00:04,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 20:00:04,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 20:00:34,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 20:00:34,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:01:04,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 20:01:04,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:01:34,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 20:01:34,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 20:02:04,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 20:02:04,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 20:02:34,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 20:02:34,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 20:03:04,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 20:03:04,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:03:34,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 20:03:34,300 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:04:04,300 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 20:04:04,300 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:04:34,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 20:04:34,302 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:05:04,302 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 20:05:04,303 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:05:34,303 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 20:05:34,304 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:06:04,304 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 20:06:04,304 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 20:06:34,303 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 20:06:34,304 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:07:04,304 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 20:07:04,304 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:07:34,305 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 20:07:34,306 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:08:04,307 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-05 20:08:04,307 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:08:34,308 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 20:08:34,308 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 20:09:04,307 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 20:09:04,308 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:09:34,309 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-05 20:09:34,310 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:10:04,310 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 20:10:04,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 20:10:34,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 20:10:34,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:11:04,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 20:11:04,313 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:11:34,313 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 20:11:34,314 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:12:04,314 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 20:12:04,315 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:12:34,315 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 20:12:34,316 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 20:12:37,454 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 20:12:37,461 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 20:16:30,036 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 20:16:30,056 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 20:16:30,059 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-05 20:16:30,371 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 20:16:30,521 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 20:16:30,521 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-05 20:16:30,523 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-05 20:16:30,524 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-05 20:16:30,762 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-05 20:16:30,810 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 20:16:30,813 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-05 20:16:30,833 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 20:16:30,835 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-05 20:16:30,835 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 20:16:30,836 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 20:16:30,881 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-05 20:16:30,884 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 20:16:30,914 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-05 20:16:30,914 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 20:16:31,167 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 20:16:31,276 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 20:16:31,276 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 20:16:31,320 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 20:16:31,336 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 20:16:31,384 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 20:16:31,384 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 20:16:31,389 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 20:16:31,390 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 20:16:31
2015-12-05 20:16:31,393 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 20:16:31,393 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:16:31,396 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 20:16:31,396 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 20:16:31,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 20:16:31,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 20:16:31,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 20:16:31,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 20:16:31,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 20:16:31,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 20:16:31,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 20:16:31,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 20:16:31,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 20:16:31,433 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 20:16:31,433 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 20:16:31,433 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 20:16:31,434 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 20:16:31,437 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 20:16:31,688 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 20:16:31,688 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:16:31,688 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 20:16:31,688 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 20:16:31,690 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 20:16:31,704 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 20:16:31,704 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:16:31,704 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 20:16:31,704 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 20:16:31,706 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 20:16:31,706 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 20:16:31,706 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 20:16:31,707 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-05 20:16:31,707 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-05 20:16:31,709 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-05 20:16:31,709 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:16:31,709 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-05 20:16:31,709 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-05 20:16:31,713 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 20:16:31,713 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 20:16:31,714 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 20:16:31,715 WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory /usr/local/hadoop/tmp/dfs/name does not exist
2015-12-05 20:16:31,716 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /usr/local/hadoop/tmp/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:313)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1020)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:739)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:536)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:595)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2015-12-05 20:16:31,722 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 20:16:31,724 WARN org.apache.hadoop.http.HttpServer2: HttpServer Acceptor: isRunning is false. Rechecking.
2015-12-05 20:16:31,724 WARN org.apache.hadoop.http.HttpServer2: HttpServer Acceptor: isRunning is false
2015-12-05 20:16:31,823 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2015-12-05 20:16:31,823 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2015-12-05 20:16:31,824 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2015-12-05 20:16:31,824 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /usr/local/hadoop/tmp/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:313)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1020)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:739)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:536)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:595)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2015-12-05 20:16:31,826 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 20:16:31,827 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 20:24:57,920 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 20:24:57,931 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 20:24:57,933 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-05 20:24:58,197 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 20:24:58,276 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 20:24:58,276 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-05 20:24:58,278 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-05 20:24:58,279 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-05 20:24:58,494 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-05 20:24:58,536 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 20:24:58,541 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-05 20:24:58,555 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 20:24:58,561 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-05 20:24:58,561 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 20:24:58,561 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 20:24:58,589 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-05 20:24:58,590 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 20:24:58,606 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-05 20:24:58,606 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 20:24:58,794 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 20:24:58,837 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 20:24:58,837 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 20:24:58,873 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 20:24:58,879 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 20:24:58,913 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 20:24:58,913 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 20:24:58,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 20:24:58,916 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 20:24:58
2015-12-05 20:24:58,918 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 20:24:58,918 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:24:58,919 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 20:24:58,919 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 20:24:58,949 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 20:24:58,949 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 20:24:58,949 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 20:24:58,949 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 20:24:58,949 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 20:24:58,949 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 20:24:58,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 20:24:58,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 20:24:58,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 20:24:58,954 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 20:24:58,954 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 20:24:58,954 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 20:24:58,954 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 20:24:58,956 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 20:24:59,103 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 20:24:59,103 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:24:59,103 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 20:24:59,103 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 20:24:59,106 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 20:24:59,119 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 20:24:59,119 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:24:59,119 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 20:24:59,119 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 20:24:59,120 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 20:24:59,120 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 20:24:59,120 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 20:24:59,121 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-05 20:24:59,122 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-05 20:24:59,124 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-05 20:24:59,124 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:24:59,124 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-05 20:24:59,124 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-05 20:24:59,129 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 20:24:59,129 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 20:24:59,129 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 20:24:59,130 WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory /usr/local/hadoop/tmp/dfs/name does not exist
2015-12-05 20:24:59,131 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /usr/local/hadoop/tmp/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:313)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1020)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:739)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:536)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:595)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2015-12-05 20:24:59,138 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 20:24:59,239 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2015-12-05 20:24:59,240 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2015-12-05 20:24:59,240 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2015-12-05 20:24:59,240 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /usr/local/hadoop/tmp/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:313)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1020)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:739)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:536)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:595)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2015-12-05 20:24:59,242 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 20:24:59,245 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 20:34:56,850 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 20:34:56,860 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 20:34:56,862 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-05 20:34:57,151 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 20:34:57,244 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 20:34:57,244 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-05 20:34:57,245 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-05 20:34:57,246 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-05 20:34:57,458 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-05 20:34:57,497 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 20:34:57,502 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-05 20:34:57,518 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 20:34:57,520 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-05 20:34:57,520 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 20:34:57,522 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 20:34:57,541 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-05 20:34:57,542 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 20:34:57,557 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-05 20:34:57,557 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 20:34:57,731 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 20:34:57,777 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 20:34:57,777 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 20:34:57,812 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 20:34:57,819 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 20:34:57,852 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 20:34:57,852 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 20:34:57,855 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 20:34:57,856 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 20:34:57
2015-12-05 20:34:57,857 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 20:34:57,858 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:34:57,859 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 20:34:57,859 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 20:34:57,883 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 20:34:57,883 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 20:34:57,883 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 20:34:57,883 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 20:34:57,883 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 20:34:57,883 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 20:34:57,883 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 20:34:57,883 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 20:34:57,883 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 20:34:57,888 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 20:34:57,888 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 20:34:57,888 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 20:34:57,888 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 20:34:57,889 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 20:34:58,057 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 20:34:58,057 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:34:58,057 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 20:34:58,057 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 20:34:58,060 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 20:34:58,071 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 20:34:58,071 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:34:58,071 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 20:34:58,071 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 20:34:58,073 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 20:34:58,073 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 20:34:58,073 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 20:34:58,074 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-05 20:34:58,074 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-05 20:34:58,076 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-05 20:34:58,076 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:34:58,076 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-05 20:34:58,076 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-05 20:34:58,080 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 20:34:58,080 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 20:34:58,080 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 20:34:58,084 WARN org.apache.hadoop.hdfs.server.common.Storage: Cannot access storage directory /usr/local/hadoop/tmp/dfs/name
2015-12-05 20:34:58,086 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /usr/local/hadoop/tmp/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:313)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1020)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:739)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:536)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:595)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2015-12-05 20:34:58,101 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 20:34:58,203 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2015-12-05 20:34:58,204 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2015-12-05 20:34:58,204 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2015-12-05 20:34:58,204 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /usr/local/hadoop/tmp/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:313)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1020)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:739)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:536)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:595)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2015-12-05 20:34:58,206 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 20:34:58,208 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 20:45:46,601 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 20:45:46,617 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 20:45:46,620 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-05 20:45:46,883 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 20:45:46,975 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 20:45:46,975 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-05 20:45:46,976 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-05 20:45:46,977 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-05 20:45:47,154 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-05 20:45:47,188 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 20:45:47,191 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-05 20:45:47,204 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 20:45:47,206 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-05 20:45:47,206 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 20:45:47,206 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 20:45:47,226 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-05 20:45:47,227 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 20:45:47,248 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-05 20:45:47,249 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 20:45:47,441 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 20:45:47,491 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 20:45:47,491 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 20:45:47,527 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 20:45:47,534 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 20:45:47,568 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 20:45:47,568 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 20:45:47,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 20:45:47,572 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 20:45:47
2015-12-05 20:45:47,574 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 20:45:47,574 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:45:47,575 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 20:45:47,575 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 20:45:47,607 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 20:45:47,607 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 20:45:47,607 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 20:45:47,607 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 20:45:47,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 20:45:47,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 20:45:47,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 20:45:47,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 20:45:47,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 20:45:47,614 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 20:45:47,614 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 20:45:47,614 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 20:45:47,614 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 20:45:47,617 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 20:45:47,783 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 20:45:47,783 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:45:47,783 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 20:45:47,783 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 20:45:47,786 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 20:45:47,820 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 20:45:47,820 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:45:47,820 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 20:45:47,820 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 20:45:47,822 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 20:45:47,822 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 20:45:47,822 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 20:45:47,823 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-05 20:45:47,823 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-05 20:45:47,825 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-05 20:45:47,825 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:45:47,825 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-05 20:45:47,825 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-05 20:45:47,829 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 20:45:47,830 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 20:45:47,830 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 20:45:47,834 WARN org.apache.hadoop.hdfs.server.common.Storage: Cannot access storage directory /usr/local/hadoop/tmp/dfs/name
2015-12-05 20:45:47,836 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /usr/local/hadoop/tmp/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:313)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1020)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:739)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:536)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:595)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2015-12-05 20:45:47,850 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 20:45:47,951 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2015-12-05 20:45:47,952 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2015-12-05 20:45:47,952 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2015-12-05 20:45:47,952 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /usr/local/hadoop/tmp/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:313)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1020)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:739)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:536)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:595)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2015-12-05 20:45:47,954 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 20:45:47,956 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 20:49:22,063 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 20:49:22,075 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 20:49:22,077 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-05 20:49:22,362 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 20:49:22,447 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 20:49:22,448 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-05 20:49:22,449 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-05 20:49:22,450 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-05 20:49:22,665 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-05 20:49:22,702 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 20:49:22,705 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-05 20:49:22,717 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 20:49:22,719 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-05 20:49:22,719 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 20:49:22,719 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 20:49:22,750 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-05 20:49:22,752 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 20:49:22,768 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-05 20:49:22,768 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 20:49:22,948 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 20:49:22,992 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 20:49:22,993 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 20:49:23,028 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 20:49:23,034 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 20:49:23,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 20:49:23,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 20:49:23,069 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 20:49:23,070 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 20:49:23
2015-12-05 20:49:23,072 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 20:49:23,072 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:49:23,073 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 20:49:23,073 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 20:49:23,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 20:49:23,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 20:49:23,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 20:49:23,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 20:49:23,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 20:49:23,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 20:49:23,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 20:49:23,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 20:49:23,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 20:49:23,105 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 20:49:23,105 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 20:49:23,105 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 20:49:23,105 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 20:49:23,108 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 20:49:23,267 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 20:49:23,267 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:49:23,267 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 20:49:23,267 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 20:49:23,270 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 20:49:23,280 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 20:49:23,280 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:49:23,280 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 20:49:23,280 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 20:49:23,281 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 20:49:23,281 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 20:49:23,281 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 20:49:23,283 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-05 20:49:23,283 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-05 20:49:23,285 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-05 20:49:23,285 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 20:49:23,285 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-05 20:49:23,285 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-05 20:49:23,289 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 20:49:23,289 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 20:49:23,289 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 20:49:23,293 WARN org.apache.hadoop.hdfs.server.common.Storage: Cannot access storage directory /usr/local/hadoop/tmp/dfs/name
2015-12-05 20:49:23,294 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /usr/local/hadoop/tmp/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:313)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1020)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:739)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:536)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:595)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2015-12-05 20:49:23,299 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 20:49:23,400 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2015-12-05 20:49:23,401 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2015-12-05 20:49:23,401 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2015-12-05 20:49:23,401 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /usr/local/hadoop/tmp/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:313)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1020)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:739)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:536)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:595)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:762)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:746)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1438)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1504)
2015-12-05 20:49:23,404 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 20:49:23,406 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 21:13:35,410 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 21:13:35,425 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 21:13:35,427 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-05 21:13:35,742 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 21:13:35,856 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 21:13:35,856 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-05 21:13:35,858 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-05 21:13:35,859 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-05 21:13:36,056 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-05 21:13:36,101 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 21:13:36,104 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-05 21:13:36,117 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 21:13:36,120 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-05 21:13:36,120 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 21:13:36,121 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 21:13:36,151 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-05 21:13:36,153 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 21:13:36,174 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-05 21:13:36,174 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 21:13:36,392 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 21:13:36,441 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 21:13:36,441 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 21:13:36,474 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 21:13:36,480 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 21:13:36,518 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 21:13:36,518 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 21:13:36,520 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 21:13:36,522 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 21:13:36
2015-12-05 21:13:36,525 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 21:13:36,525 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 21:13:36,527 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 21:13:36,527 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 21:13:36,554 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 21:13:36,554 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 21:13:36,554 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 21:13:36,554 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 21:13:36,554 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 21:13:36,554 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 21:13:36,554 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 21:13:36,554 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 21:13:36,554 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 21:13:36,559 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 21:13:36,559 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 21:13:36,559 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 21:13:36,560 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 21:13:36,561 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 21:13:36,774 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 21:13:36,774 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 21:13:36,774 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 21:13:36,774 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 21:13:36,777 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 21:13:36,790 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 21:13:36,790 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 21:13:36,790 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 21:13:36,790 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 21:13:36,791 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 21:13:36,791 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 21:13:36,791 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 21:13:36,792 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-05 21:13:36,792 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-05 21:13:36,794 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-05 21:13:36,794 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 21:13:36,794 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-05 21:13:36,794 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-05 21:13:36,798 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 21:13:36,799 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 21:13:36,799 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 21:13:36,815 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/name/in_use.lock acquired by nodename 19379@ubuntu
2015-12-05 21:13:36,893 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/parallels/hadoop/tmp/dfs/name/current
2015-12-05 21:13:36,894 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2015-12-05 21:13:36,926 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-05 21:13:36,946 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-05 21:13:36,947 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000000
2015-12-05 21:13:36,954 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-12-05 21:13:36,955 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2015-12-05 21:13:37,070 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-05 21:13:37,071 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 272 msecs
2015-12-05 21:13:37,364 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-05 21:13:37,371 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-05 21:13:37,385 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-05 21:13:37,419 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-05 21:13:37,428 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-05 21:13:37,428 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-05 21:13:37,428 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-05 21:13:37,431 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2015-12-05 21:13:37,431 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2015-12-05 21:13:37,431 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-12-05 21:13:37,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2015-12-05 21:13:37,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-05 21:13:37,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-12-05 21:13:37,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-05 21:13:37,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-05 21:13:37,447 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 16 msec
2015-12-05 21:13:37,460 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-05 21:13:37,461 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-05 21:13:37,468 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-05 21:13:37,468 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-05 21:13:37,473 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-05 21:13:37,474 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 3614497 milliseconds
2015-12-05 21:13:37,476 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-05 21:13:43,771 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=3ad84b8a-5322-4d61-8b83-0f8f71d68d89, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-64431893-233f-4618-b6db-9870599dcfca;nsid=1710165876;c=0) storage 3ad84b8a-5322-4d61-8b83-0f8f71d68d89
2015-12-05 21:13:43,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-05 21:13:43,774 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-05 21:13:43,898 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-05 21:13:43,904 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9b06fc1e-467a-4a84-a86a-5325a62808ff for DN 127.0.0.1:50010
2015-12-05 21:13:43,939 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-9b06fc1e-467a-4a84-a86a-5325a62808ff,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-12-05 21:13:43,939 INFO BlockStateChange: BLOCK* processReport: from storage DS-9b06fc1e-467a-4a84-a86a-5325a62808ff node DatanodeRegistration(127.0.0.1, datanodeUuid=3ad84b8a-5322-4d61-8b83-0f8f71d68d89, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-64431893-233f-4618-b6db-9870599dcfca;nsid=1710165876;c=0), blocks: 0, hasStaleStorages: false, processing time: 1 msecs
2015-12-05 21:14:07,474 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:14:07,475 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:14:10,227 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-05 21:14:37,474 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:14:37,475 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:14:50,948 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-05 21:14:50,948 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-05 21:14:50,948 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2015-12-05 21:14:50,948 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 6 
2015-12-05 21:14:50,950 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 8 
2015-12-05 21:14:50,951 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000000001 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000001-0000000000000000002
2015-12-05 21:14:50,956 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2015-12-05 21:14:51,778 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-05 21:14:51,778 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 356 bytes.
2015-12-05 21:14:51,782 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2015-12-05 21:15:07,475 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:15:07,475 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:15:37,476 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:15:37,476 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:16:07,477 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:16:07,477 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:16:37,477 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:16:37,477 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:17:03,821 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 14 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 4 
2015-12-05 21:17:07,478 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:17:07,479 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:17:37,478 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:17:37,479 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:18:07,478 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:18:07,479 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:18:29,312 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 15 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 38 
2015-12-05 21:18:37,478 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:18:37,479 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:19:07,479 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:19:07,480 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:19:37,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-05 21:19:37,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:19:53,151 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 5 Total time for transactions(ms): 17 Number of transactions batched in Syncs: 0 Number of syncs: 5 SyncTimes(ms): 42 
2015-12-05 21:19:53,237 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/file01._COPYING_. BP-1777036329-127.0.1.1-1449378795593 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 21:19:53,640 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-05 21:19:53,640 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2015-12-05 21:19:53,660 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]} size 22
2015-12-05 21:19:54,051 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/file01._COPYING_ is closed by DFSClient_NONMAPREDUCE_1046888939_1
2015-12-05 21:20:06,027 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/file02._COPYING_. BP-1777036329-127.0.1.1-1449378795593 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 21:20:06,190 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 21:20:06,197 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/file02._COPYING_ is closed by DFSClient_NONMAPREDUCE_626213952_1
2015-12-05 21:20:07,482 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:20:07,482 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:20:22,924 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 21:20:22,936 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 21:22:17,275 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 21:22:17,290 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 21:22:17,293 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-05 21:22:17,595 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 21:22:17,749 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 21:22:17,749 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-05 21:22:17,751 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-05 21:22:17,752 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-05 21:22:17,990 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-05 21:22:18,037 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 21:22:18,043 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-05 21:22:18,056 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 21:22:18,060 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-05 21:22:18,060 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 21:22:18,061 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 21:22:18,106 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-05 21:22:18,109 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 21:22:18,140 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-05 21:22:18,140 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 21:22:18,396 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 21:22:18,508 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 21:22:18,508 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 21:22:18,552 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 21:22:18,568 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 21:22:18,621 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 21:22:18,621 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 21:22:18,626 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 21:22:18,628 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 21:22:18
2015-12-05 21:22:18,630 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 21:22:18,630 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 21:22:18,633 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 21:22:18,633 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 21:22:18,667 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 21:22:18,670 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 21:22:18,670 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 21:22:18,670 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 21:22:18,670 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 21:22:18,670 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 21:22:18,670 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 21:22:18,670 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 21:22:18,670 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 21:22:18,677 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 21:22:18,677 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 21:22:18,677 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 21:22:18,678 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 21:22:18,682 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 21:22:18,917 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 21:22:18,917 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 21:22:18,917 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 21:22:18,918 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 21:22:18,920 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 21:22:18,938 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 21:22:18,938 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 21:22:18,938 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 21:22:18,938 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 21:22:18,940 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 21:22:18,940 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 21:22:18,940 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 21:22:18,941 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-05 21:22:18,941 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-05 21:22:18,943 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-05 21:22:18,943 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 21:22:18,943 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-05 21:22:18,943 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-05 21:22:18,947 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 21:22:18,947 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 21:22:18,947 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 21:22:18,959 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/name/in_use.lock acquired by nodename 3014@ubuntu
2015-12-05 21:22:19,034 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/parallels/hadoop/tmp/dfs/name/current
2015-12-05 21:22:19,138 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000000003 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000003-0000000000000000018
2015-12-05 21:22:19,184 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-05 21:22:19,206 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-05 21:22:19,206 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 2 from /home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000002
2015-12-05 21:22:19,207 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1755e85b expecting start txid #3
2015-12-05 21:22:19,207 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000003-0000000000000000018
2015-12-05 21:22:19,208 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000003-0000000000000000018' to transaction ID 3
2015-12-05 21:22:19,231 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000003-0000000000000000018 of size 1048576 edits # 16 loaded in 0 seconds
2015-12-05 21:22:19,231 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-12-05 21:22:19,233 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 19
2015-12-05 21:22:19,326 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-05 21:22:19,327 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 379 msecs
2015-12-05 21:22:19,607 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-05 21:22:19,615 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-05 21:22:19,631 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-05 21:22:19,663 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-05 21:22:19,680 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-05 21:22:19,681 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-05 21:22:19,681 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 2.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2015-12-05 21:22:19,717 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-05 21:22:19,718 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-05 21:22:19,720 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-05 21:22:19,720 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-05 21:22:19,725 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-05 21:22:19,725 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 64775 milliseconds
2015-12-05 21:22:19,731 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 7 millisecond(s).
2015-12-05 21:22:25,746 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=3ad84b8a-5322-4d61-8b83-0f8f71d68d89, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-64431893-233f-4618-b6db-9870599dcfca;nsid=1710165876;c=0) storage 3ad84b8a-5322-4d61-8b83-0f8f71d68d89
2015-12-05 21:22:25,750 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-05 21:22:25,752 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-05 21:22:25,891 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-05 21:22:25,893 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9b06fc1e-467a-4a84-a86a-5325a62808ff for DN 127.0.0.1:50010
2015-12-05 21:22:25,944 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 1 has reached the threshold 0.9990 of total blocks 2. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2015-12-05 21:22:25,944 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-05 21:22:25,946 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-9b06fc1e-467a-4a84-a86a-5325a62808ff,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-12-05 21:22:25,946 INFO BlockStateChange: BLOCK* processReport: from storage DS-9b06fc1e-467a-4a84-a86a-5325a62808ff node DatanodeRegistration(127.0.0.1, datanodeUuid=3ad84b8a-5322-4d61-8b83-0f8f71d68d89, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-64431893-233f-4618-b6db-9870599dcfca;nsid=1710165876;c=0), blocks: 2, hasStaleStorages: false, processing time: 11 msecs
2015-12-05 21:22:25,961 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 2
2015-12-05 21:22:25,961 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-05 21:22:25,961 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-12-05 21:22:25,961 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-05 21:22:25,961 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-05 21:22:25,961 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 12 msec
2015-12-05 21:22:45,955 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 2 has reached the threshold 0.9990 of total blocks 2. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2015-12-05 21:22:49,725 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:22:49,725 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:22:49,865 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-05 21:22:55,958 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 37 secs
2015-12-05 21:22:55,958 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2015-12-05 21:22:55,959 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2015-12-05 21:22:55,959 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-12-05 21:23:19,725 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:23:19,725 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:23:49,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:23:49,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:24:19,725 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:24:19,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:24:49,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:24:49,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:25:19,725 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:25:19,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:25:49,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:25:49,727 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:26:19,727 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:26:19,728 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:26:49,728 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:26:49,729 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:27:19,729 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:27:19,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:27:49,731 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-05 21:27:49,731 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:28:19,733 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:28:19,734 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:28:49,733 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:28:49,733 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:29:19,734 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:29:19,735 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:29:49,735 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:29:49,735 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:30:19,735 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:30:19,735 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:30:49,734 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:30:49,735 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:31:19,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:31:19,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:31:37,206 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-05 21:31:43,096 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-05 21:31:49,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:31:49,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:32:19,738 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:32:19,738 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:32:49,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:32:49,738 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:32:57,657 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 7 
2015-12-05 21:32:57,686 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741825_1001 127.0.0.1:50010 
2015-12-05 21:32:57,686 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741826_1002 127.0.0.1:50010 
2015-12-05 21:32:58,835 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741825_1001, blk_1073741826_1002]
2015-12-05 21:33:19,738 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:33:19,739 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:33:49,739 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:33:49,740 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:34:19,740 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:34:19,741 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:34:42,857 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 5 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 0 Number of syncs: 5 SyncTimes(ms): 36 
2015-12-05 21:34:42,918 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/wordcount/input/file01._COPYING_. BP-1777036329-127.0.1.1-1449378795593 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 21:34:43,205 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741827_1003{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-05 21:34:43,205 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2015-12-05 21:34:43,219 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741827_1003{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]} size 22
2015-12-05 21:34:43,614 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/wordcount/input/file01._COPYING_ is closed by DFSClient_NONMAPREDUCE_-216749264_1
2015-12-05 21:34:49,740 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:34:49,741 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:34:49,996 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/wordcount/input/file02._COPYING_. BP-1777036329-127.0.1.1-1449378795593 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 21:34:50,197 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 21:34:50,206 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/wordcount/input/file02._COPYING_ is closed by DFSClient_NONMAPREDUCE_-847673488_1
2015-12-05 21:35:19,741 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:35:19,742 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-05 21:35:49,740 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:35:49,741 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:36:19,742 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:36:19,742 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:36:49,742 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:36:49,742 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:37:19,743 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:37:19,743 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:37:49,743 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:37:49,746 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 4 millisecond(s).
2015-12-05 21:38:19,743 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:38:19,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:38:49,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:38:49,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:39:19,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:39:19,745 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:39:49,746 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:39:49,746 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:40:19,745 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:40:19,746 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:40:29,016 INFO BlockStateChange: BLOCK* processReport: from storage DS-9b06fc1e-467a-4a84-a86a-5325a62808ff node DatanodeRegistration(127.0.0.1, datanodeUuid=3ad84b8a-5322-4d61-8b83-0f8f71d68d89, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-64431893-233f-4618-b6db-9870599dcfca;nsid=1710165876;c=0), blocks: 2, hasStaleStorages: false, processing time: 7 msecs
2015-12-05 21:40:49,746 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:40:49,747 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:41:19,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:41:19,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:41:49,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:41:49,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:42:19,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:42:19,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:42:49,747 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:42:49,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:43:19,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:43:19,749 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:43:49,749 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:43:49,750 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-05 21:44:19,749 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:44:19,749 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:44:49,750 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:44:49,750 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:45:19,750 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:45:19,750 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:45:49,750 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:45:49,751 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:46:19,751 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:46:19,752 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:46:49,752 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:46:49,753 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:47:19,755 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-05 21:47:19,755 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:47:49,754 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:47:49,755 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:48:19,756 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:48:19,756 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:48:49,756 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:48:49,756 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:49:19,756 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:49:19,757 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:49:49,757 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:49:49,758 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:50:19,759 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-05 21:50:19,760 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:50:49,761 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:50:49,761 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:51:19,761 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:51:19,761 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:51:49,760 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:51:49,761 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:52:19,761 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:52:19,761 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:52:49,762 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:52:49,762 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:53:19,762 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:53:19,762 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:53:49,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-05 21:53:49,764 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:54:19,764 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:54:19,765 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:54:49,765 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:54:49,765 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:55:19,764 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:55:19,765 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:55:49,766 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-05 21:55:49,766 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:56:19,767 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:56:19,767 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:56:49,766 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 21:56:49,767 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:57:19,767 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:57:19,768 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:57:49,768 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:57:49,769 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:58:19,770 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:58:19,770 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 21:58:49,770 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:58:49,770 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:59:19,771 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:59:19,772 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 21:59:49,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 21:59:49,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:00:19,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:00:19,774 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:00:49,774 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:00:49,774 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:01:19,774 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:01:19,775 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:01:49,775 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:01:49,775 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:02:19,774 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:02:19,775 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:02:49,775 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:02:49,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:03:19,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:03:19,777 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:03:49,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:03:49,777 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:04:19,777 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:04:19,777 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:04:49,778 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:04:49,778 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:05:19,777 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:05:19,778 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:05:49,778 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:05:49,779 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:06:19,779 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:06:19,780 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:06:49,781 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:06:49,781 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:07:19,781 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:07:19,782 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:07:49,782 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:07:49,783 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:08:19,783 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:08:19,784 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:08:49,784 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:08:49,785 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:09:19,785 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:09:19,785 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:09:49,786 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:09:49,786 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:10:19,785 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:10:19,786 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:10:49,786 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:10:49,787 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:11:19,787 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:11:19,787 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:11:49,787 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:11:49,788 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:12:19,788 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:12:19,789 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:12:49,790 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:12:49,790 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:13:19,789 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:13:19,790 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:13:49,791 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:13:49,791 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:14:19,790 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:14:19,791 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:14:49,791 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:14:49,791 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:15:19,791 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:15:19,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:15:49,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:15:49,793 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:16:19,793 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:16:19,794 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:16:49,794 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:16:49,794 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:17:19,794 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:17:19,794 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:17:49,794 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:17:49,794 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:18:19,793 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:18:19,793 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:18:49,794 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:18:49,795 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:19:19,795 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:19:19,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:19:49,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:19:49,797 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:20:19,797 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:20:19,798 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:20:49,798 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:20:49,798 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:21:19,798 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:21:19,798 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:21:36,044 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-05 22:21:36,044 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-05 22:21:36,044 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 19
2015-12-05 22:21:36,052 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 17 Total time for transactions(ms): 11 Number of transactions batched in Syncs: 0 Number of syncs: 14 SyncTimes(ms): 47 
2015-12-05 22:21:36,057 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 17 Total time for transactions(ms): 11 Number of transactions batched in Syncs: 0 Number of syncs: 15 SyncTimes(ms): 52 
2015-12-05 22:21:36,069 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000000019 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000019-0000000000000000035
2015-12-05 22:21:36,070 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 36
2015-12-05 22:21:37,277 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2015-12-05 22:21:37,278 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000035 size 770 bytes.
2015-12-05 22:21:37,287 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2015-12-05 22:21:37,287 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2015-12-05 22:21:49,797 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:21:49,798 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:22:19,798 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:22:19,798 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:22:49,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:22:49,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:23:19,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:23:19,800 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:23:49,800 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:23:49,800 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:24:19,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:24:19,800 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:24:49,800 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:24:49,801 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:25:19,802 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:25:19,802 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:25:49,802 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:25:49,803 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:26:19,803 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:26:19,804 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:26:49,805 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:26:49,805 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:27:19,805 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:27:19,805 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:27:49,806 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-05 22:27:49,807 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:28:19,808 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:28:19,808 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:28:49,807 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:28:49,808 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:29:19,807 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:29:19,808 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:29:49,809 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:29:49,809 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:30:19,808 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:30:19,809 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:30:46,131 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 6 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 8 
2015-12-05 22:30:49,809 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:30:49,810 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:31:19,810 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:31:19,810 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:31:49,810 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:31:49,811 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:32:19,811 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:32:19,811 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:32:49,812 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:32:49,813 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:33:19,814 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-05 22:33:19,815 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:33:49,815 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:33:49,816 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:34:19,816 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:34:19,816 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:34:37,424 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 22:34:37,426 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 22:42:13,397 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 22:42:13,413 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 22:42:13,417 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-05 22:42:13,738 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 22:42:13,891 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 22:42:13,891 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-05 22:42:13,894 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-05 22:42:13,895 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-05 22:42:14,116 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-05 22:42:14,178 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 22:42:14,182 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-05 22:42:14,194 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 22:42:14,196 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-05 22:42:14,196 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 22:42:14,196 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 22:42:14,244 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-05 22:42:14,247 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 22:42:14,280 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-05 22:42:14,280 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 22:42:14,555 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-05 22:42:14,618 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 22:42:14,619 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-05 22:42:14,669 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-05 22:42:14,680 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-05 22:42:14,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-05 22:42:14,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-05 22:42:14,740 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-05 22:42:14,742 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 05 22:42:14
2015-12-05 22:42:14,746 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-05 22:42:14,746 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 22:42:14,748 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-05 22:42:14,748 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-05 22:42:14,779 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-05 22:42:14,779 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-05 22:42:14,779 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-05 22:42:14,779 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-05 22:42:14,779 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-05 22:42:14,779 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-05 22:42:14,779 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-05 22:42:14,779 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-05 22:42:14,779 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-05 22:42:14,785 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-05 22:42:14,785 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-05 22:42:14,785 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-05 22:42:14,786 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-05 22:42:14,787 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-05 22:42:15,068 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-05 22:42:15,068 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 22:42:15,068 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-05 22:42:15,069 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-05 22:42:15,071 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-05 22:42:15,082 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-05 22:42:15,082 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 22:42:15,082 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-05 22:42:15,082 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-05 22:42:15,084 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-05 22:42:15,084 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-05 22:42:15,084 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-05 22:42:15,085 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-05 22:42:15,085 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-05 22:42:15,088 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-05 22:42:15,088 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 22:42:15,089 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-05 22:42:15,089 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-05 22:42:15,094 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-05 22:42:15,094 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-05 22:42:15,094 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-05 22:42:15,110 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/name/in_use.lock acquired by nodename 20487@ubuntu
2015-12-05 22:42:15,205 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/parallels/hadoop/tmp/dfs/name/current
2015-12-05 22:42:15,292 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000000036 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000036-0000000000000000041
2015-12-05 22:42:15,327 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 7 INodes.
2015-12-05 22:42:15,363 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-05 22:42:15,363 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 35 from /home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000035
2015-12-05 22:42:15,363 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@2a79d4b1 expecting start txid #36
2015-12-05 22:42:15,363 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000036-0000000000000000041
2015-12-05 22:42:15,366 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000036-0000000000000000041' to transaction ID 36
2015-12-05 22:42:15,374 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000036-0000000000000000041 of size 1048576 edits # 6 loaded in 0 seconds
2015-12-05 22:42:15,375 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-12-05 22:42:15,376 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 42
2015-12-05 22:42:15,477 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-05 22:42:15,478 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 384 msecs
2015-12-05 22:42:15,698 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-05 22:42:15,706 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-05 22:42:15,734 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-05 22:42:15,759 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-05 22:42:15,774 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-05 22:42:15,774 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-05 22:42:15,774 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 2.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2015-12-05 22:42:15,811 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-05 22:42:15,812 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-05 22:42:15,814 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-05 22:42:15,814 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-05 22:42:15,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-05 22:42:15,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 4860869 milliseconds
2015-12-05 22:42:15,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 8 millisecond(s).
2015-12-05 22:42:19,381 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=3ad84b8a-5322-4d61-8b83-0f8f71d68d89, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-64431893-233f-4618-b6db-9870599dcfca;nsid=1710165876;c=0) storage 3ad84b8a-5322-4d61-8b83-0f8f71d68d89
2015-12-05 22:42:19,384 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-05 22:42:19,384 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-05 22:42:19,510 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-05 22:42:19,512 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9b06fc1e-467a-4a84-a86a-5325a62808ff for DN 127.0.0.1:50010
2015-12-05 22:42:19,554 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 1 has reached the threshold 0.9990 of total blocks 2. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2015-12-05 22:42:19,554 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-05 22:42:19,556 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-9b06fc1e-467a-4a84-a86a-5325a62808ff,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-12-05 22:42:19,556 INFO BlockStateChange: BLOCK* processReport: from storage DS-9b06fc1e-467a-4a84-a86a-5325a62808ff node DatanodeRegistration(127.0.0.1, datanodeUuid=3ad84b8a-5322-4d61-8b83-0f8f71d68d89, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-64431893-233f-4618-b6db-9870599dcfca;nsid=1710165876;c=0), blocks: 2, hasStaleStorages: false, processing time: 8 msecs
2015-12-05 22:42:19,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 2
2015-12-05 22:42:19,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-05 22:42:19,571 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-12-05 22:42:19,571 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-05 22:42:19,571 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-05 22:42:19,571 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 12 msec
2015-12-05 22:42:39,569 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 2 has reached the threshold 0.9990 of total blocks 2. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2015-12-05 22:42:45,818 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:42:45,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:42:49,575 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 34 secs
2015-12-05 22:42:49,575 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2015-12-05 22:42:49,576 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2015-12-05 22:42:49,576 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-12-05 22:43:15,818 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:43:15,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:43:24,164 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-05 22:43:24,164 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-05 22:43:24,164 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 42
2015-12-05 22:43:24,164 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 7 
2015-12-05 22:43:24,171 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 14 
2015-12-05 22:43:24,172 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000000042 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000042-0000000000000000043
2015-12-05 22:43:24,172 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 44
2015-12-05 22:43:25,147 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 500.00 KB/s
2015-12-05 22:43:25,148 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000043 size 1096 bytes.
2015-12-05 22:43:25,156 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 35
2015-12-05 22:43:25,156 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000002, cpktTxId=0000000000000000002)
2015-12-05 22:43:45,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:43:45,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:44:12,434 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449384245710_0001/job.jar. BP-1777036329-127.0.1.1-1449378795593 blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 22:44:12,718 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741829_1005{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-05 22:44:12,718 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2015-12-05 22:44:12,741 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741829_1005{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]} size 3074
2015-12-05 22:44:13,129 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449384245710_0001/job.jar is closed by DFSClient_NONMAPREDUCE_329235791_1
2015-12-05 22:44:13,147 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449384245710_0001/job.jar
2015-12-05 22:44:13,337 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449384245710_0001/job.split
2015-12-05 22:44:13,351 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449384245710_0001/job.split. BP-1777036329-127.0.1.1-1449378795593 blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 22:44:13,362 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 22:44:13,366 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449384245710_0001/job.split is closed by DFSClient_NONMAPREDUCE_329235791_1
2015-12-05 22:44:13,404 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449384245710_0001/job.splitmetainfo. BP-1777036329-127.0.1.1-1449378795593 blk_1073741831_1007{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 22:44:13,425 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741831_1007{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 22:44:13,431 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449384245710_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_329235791_1
2015-12-05 22:44:13,687 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449384245710_0001/job.xml. BP-1777036329-127.0.1.1-1449378795593 blk_1073741832_1008{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 22:44:13,705 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741832_1008{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 22:44:13,716 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449384245710_0001/job.xml is closed by DFSClient_NONMAPREDUCE_329235791_1
2015-12-05 22:44:15,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:44:15,820 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:44:24,332 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 39 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 0 Number of syncs: 29 SyncTimes(ms): 216 
2015-12-05 22:44:24,593 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449384245710_0001/job_1449384245710_0001_1_conf.xml. BP-1777036329-127.0.1.1-1449378795593 blk_1073741833_1009{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 22:44:24,939 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741833_1009{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 22:44:24,952 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449384245710_0001/job_1449384245710_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1394527982_1
2015-12-05 22:44:43,900 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449384245710_0001/job_1449384245710_0001_1.jhist. BP-1777036329-127.0.1.1-1449378795593 blk_1073741834_1010{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 22:44:44,297 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449384245710_0001/job_1449384245710_0001_1.jhist for DFSClient_NONMAPREDUCE_-1394527982_1
2015-12-05 22:44:45,820 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:44:45,828 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 8 millisecond(s).
2015-12-05 22:44:53,766 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/wordcount/output/_temporary/1/_temporary/attempt_1449384245710_0001_r_000000_0/part-r-00000. BP-1777036329-127.0.1.1-1449378795593 blk_1073741835_1011{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 22:44:53,934 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741835_1011{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 22:44:53,952 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/wordcount/output/_temporary/1/_temporary/attempt_1449384245710_0001_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449384245710_0001_r_000000_0_-268524602_1
2015-12-05 22:44:54,315 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449384245710_0001/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1394527982_1
2015-12-05 22:44:54,435 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/wordcount/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1394527982_1
2015-12-05 22:44:54,443 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449384245710_0001/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1394527982_1
2015-12-05 22:44:54,481 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741834_1010{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]} size 13692
2015-12-05 22:44:54,491 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449384245710_0001/job_1449384245710_0001_1.jhist is closed by DFSClient_NONMAPREDUCE_-1394527982_1
2015-12-05 22:44:54,511 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449384245710_0001.summary_tmp. BP-1777036329-127.0.1.1-1449378795593 blk_1073741836_1012{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 22:44:54,533 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741836_1012{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 22:44:54,542 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449384245710_0001.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1394527982_1
2015-12-05 22:44:54,618 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449384245710_0001-1449384254143-parallels-word+count-1449384294446-2-1-SUCCEEDED-default-1449384264128.jhist_tmp. BP-1777036329-127.0.1.1-1449378795593 blk_1073741837_1013{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 22:44:54,641 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741837_1013{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 22:44:54,646 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449384245710_0001-1449384254143-parallels-word+count-1449384294446-2-1-SUCCEEDED-default-1449384264128.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1394527982_1
2015-12-05 22:44:54,694 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449384245710_0001_conf.xml_tmp. BP-1777036329-127.0.1.1-1449378795593 blk_1073741838_1014{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-05 22:44:54,719 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741838_1014{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-9b06fc1e-467a-4a84-a86a-5325a62808ff:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-05 22:44:54,727 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449384245710_0001_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1394527982_1
2015-12-05 22:44:55,826 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741829_1005 127.0.0.1:50010 
2015-12-05 22:44:55,827 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741830_1006 127.0.0.1:50010 
2015-12-05 22:44:55,827 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741831_1007 127.0.0.1:50010 
2015-12-05 22:44:55,827 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741832_1008 127.0.0.1:50010 
2015-12-05 22:44:55,827 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741834_1010 127.0.0.1:50010 
2015-12-05 22:44:55,827 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741833_1009 127.0.0.1:50010 
2015-12-05 22:44:57,851 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741829_1005, blk_1073741830_1006, blk_1073741831_1007, blk_1073741832_1008, blk_1073741833_1009, blk_1073741834_1010]
2015-12-05 22:45:15,820 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:45:15,821 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:45:45,821 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:45:45,823 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:46:15,822 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:46:15,823 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:46:45,824 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:46:45,825 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-05 22:47:15,824 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:47:15,824 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:47:45,823 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-05 22:47:45,824 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-05 22:48:15,824 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:48:15,825 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:48:45,825 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:48:45,828 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-05 22:49:15,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-05 22:49:15,827 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-05 22:49:43,053 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 22:49:43,056 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-06 18:11:45,874 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-06 18:11:45,893 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-06 18:11:45,896 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-06 18:11:46,244 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-06 18:11:46,351 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-06 18:11:46,351 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-06 18:11:46,353 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-06 18:11:46,353 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-06 18:11:46,714 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-06 18:11:46,778 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-06 18:11:46,783 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-06 18:11:46,793 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-06 18:11:46,795 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-06 18:11:46,795 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-06 18:11:46,795 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-06 18:11:46,836 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-06 18:11:46,838 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-06 18:11:46,874 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-06 18:11:46,874 INFO org.mortbay.log: jetty-6.1.26
2015-12-06 18:11:47,278 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-06 18:11:47,336 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-06 18:11:47,336 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-06 18:11:47,375 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-06 18:11:47,381 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-06 18:11:47,416 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-06 18:11:47,416 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-06 18:11:47,418 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-06 18:11:47,419 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 06 18:11:47
2015-12-06 18:11:47,421 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-06 18:11:47,421 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:11:47,422 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-06 18:11:47,422 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-06 18:11:47,461 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-06 18:11:47,461 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-06 18:11:47,461 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-06 18:11:47,461 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-06 18:11:47,461 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-06 18:11:47,461 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-06 18:11:47,461 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-06 18:11:47,461 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-06 18:11:47,461 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-06 18:11:47,466 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-06 18:11:47,466 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-06 18:11:47,466 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-06 18:11:47,467 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-06 18:11:47,468 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-06 18:11:47,640 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-06 18:11:47,640 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:11:47,640 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-06 18:11:47,640 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-06 18:11:47,643 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-06 18:11:47,654 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-06 18:11:47,654 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:11:47,654 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-06 18:11:47,654 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-06 18:11:47,656 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-06 18:11:47,656 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-06 18:11:47,656 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-06 18:11:47,657 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-06 18:11:47,657 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-06 18:11:47,659 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-06 18:11:47,659 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:11:47,659 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-06 18:11:47,659 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-06 18:11:47,662 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-06 18:11:47,662 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-06 18:11:47,662 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-06 18:11:47,676 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/name/in_use.lock acquired by nodename 3426@ubuntu
2015-12-06 18:11:47,798 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/parallels/hadoop/tmp/dfs/name/current
2015-12-06 18:11:47,799 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2015-12-06 18:11:47,841 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-06 18:11:48,403 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-06 18:11:48,403 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000000
2015-12-06 18:11:48,408 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-12-06 18:11:48,408 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2015-12-06 18:11:48,474 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-06 18:11:48,474 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 812 msecs
2015-12-06 18:11:48,813 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-06 18:11:48,821 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-06 18:11:48,837 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-06 18:11:48,873 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-06 18:11:48,884 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-06 18:11:48,885 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-06 18:11:48,885 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-06 18:11:48,885 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2015-12-06 18:11:48,885 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2015-12-06 18:11:48,885 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-12-06 18:11:48,898 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2015-12-06 18:11:48,898 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-06 18:11:48,898 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-12-06 18:11:48,898 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-06 18:11:48,898 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-06 18:11:48,898 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 13 msec
2015-12-06 18:11:48,933 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-06 18:11:48,934 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-06 18:11:48,941 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-06 18:11:48,941 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-06 18:11:48,948 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-06 18:11:48,948 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 149079 milliseconds
2015-12-06 18:11:48,955 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 7 millisecond(s).
2015-12-06 18:12:14,481 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-06 18:12:18,948 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:12:18,949 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:12:48,948 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:12:48,948 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:12:54,016 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 19 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 50 
2015-12-06 18:13:18,948 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:13:18,949 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:13:48,948 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:13:48,949 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:14:18,949 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:14:18,949 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:14:48,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:14:48,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:15:18,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:15:18,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:15:48,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:15:48,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:16:18,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:16:18,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:16:48,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:16:48,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:17:04,129 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 19 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 51 
2015-12-06 18:17:18,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:17:18,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:17:48,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:17:48,952 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:18:18,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:18:18,952 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:18:48,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:18:48,952 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:19:18,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:19:18,952 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:19:48,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:19:48,952 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:20:18,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:20:18,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:20:48,952 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:20:48,952 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:21:18,952 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:21:18,953 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:21:48,952 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:21:48,953 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:22:18,952 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:22:18,952 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:22:20,654 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 5 Total time for transactions(ms): 20 Number of transactions batched in Syncs: 0 Number of syncs: 5 SyncTimes(ms): 52 
2015-12-06 18:22:20,753 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:59543 Call#5 Retry#0
java.io.IOException: File /user/parallels/input/file01._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1549)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3200)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)
2015-12-06 18:22:48,953 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:22:48,954 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:23:05,190 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:59545 Call#5 Retry#0
java.io.IOException: File /user/parallels/input/file01._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1549)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3200)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)
2015-12-06 18:23:18,953 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:23:18,953 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:23:48,954 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:23:48,954 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:24:18,955 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:24:18,955 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:24:48,956 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:24:48,957 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:25:18,957 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:25:18,957 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:25:47,060 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 9 Total time for transactions(ms): 21 Number of transactions batched in Syncs: 0 Number of syncs: 9 SyncTimes(ms): 108 
2015-12-06 18:25:47,098 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 127.0.0.1:59550 Call#5 Retry#0
java.io.IOException: File /user/parallels/input/file01._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1549)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3200)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)
2015-12-06 18:25:48,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:25:48,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:26:18,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:26:18,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:26:48,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:26:48,959 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:27:18,959 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:27:18,959 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:27:48,959 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:27:48,960 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:28:18,959 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:28:18,960 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:28:48,959 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:28:48,959 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:28:50,915 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-06 18:28:50,916 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-06 18:29:12,877 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-06 18:29:12,891 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-06 18:29:12,894 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-06 18:29:13,259 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-06 18:29:13,385 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-06 18:29:13,385 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-06 18:29:13,387 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-06 18:29:13,387 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-06 18:29:13,814 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-06 18:29:13,889 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-06 18:29:13,895 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-06 18:29:13,905 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-06 18:29:13,907 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-06 18:29:13,907 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-06 18:29:13,907 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-06 18:29:13,966 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-06 18:29:13,969 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-06 18:29:13,996 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-06 18:29:13,996 INFO org.mortbay.log: jetty-6.1.26
2015-12-06 18:29:14,356 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-06 18:29:14,418 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-06 18:29:14,418 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-06 18:29:14,470 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-06 18:29:14,484 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-06 18:29:14,533 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-06 18:29:14,533 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-06 18:29:14,535 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-06 18:29:14,537 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 06 18:29:14
2015-12-06 18:29:14,539 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-06 18:29:14,539 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:29:14,542 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-06 18:29:14,542 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-06 18:29:14,578 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-06 18:29:14,579 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-06 18:29:14,579 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-06 18:29:14,579 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-06 18:29:14,579 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-06 18:29:14,579 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-06 18:29:14,579 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-06 18:29:14,579 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-06 18:29:14,579 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-06 18:29:14,588 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-06 18:29:14,588 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-06 18:29:14,588 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-06 18:29:14,588 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-06 18:29:14,589 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-06 18:29:14,820 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-06 18:29:14,820 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:29:14,820 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-06 18:29:14,820 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-06 18:29:14,822 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-06 18:29:14,835 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-06 18:29:14,835 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:29:14,835 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-06 18:29:14,835 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-06 18:29:14,837 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-06 18:29:14,837 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-06 18:29:14,837 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-06 18:29:14,838 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-06 18:29:14,838 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-06 18:29:14,840 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-06 18:29:14,840 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:29:14,840 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-06 18:29:14,840 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-06 18:29:14,844 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-06 18:29:14,845 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-06 18:29:14,845 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-06 18:29:14,857 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/name/in_use.lock acquired by nodename 7900@ubuntu
2015-12-06 18:29:15,041 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/parallels/hadoop/tmp/dfs/name/current
2015-12-06 18:29:15,132 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000000001 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000001-0000000000000000010
2015-12-06 18:29:15,163 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-06 18:29:15,191 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-06 18:29:15,192 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000000
2015-12-06 18:29:15,192 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@20c12662 expecting start txid #1
2015-12-06 18:29:15,192 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000001-0000000000000000010
2015-12-06 18:29:15,194 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000001-0000000000000000010' to transaction ID 1
2015-12-06 18:29:15,211 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000001-0000000000000000010 of size 1048576 edits # 10 loaded in 0 seconds
2015-12-06 18:29:15,212 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-12-06 18:29:15,212 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 11
2015-12-06 18:29:15,251 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-06 18:29:15,251 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 406 msecs
2015-12-06 18:29:15,464 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-06 18:29:15,472 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-06 18:29:15,488 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-06 18:29:15,528 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-06 18:29:15,537 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-06 18:29:15,537 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-06 18:29:15,537 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-06 18:29:15,537 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2015-12-06 18:29:15,537 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2015-12-06 18:29:15,537 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-12-06 18:29:15,551 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2015-12-06 18:29:15,551 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-06 18:29:15,551 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-12-06 18:29:15,551 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-06 18:29:15,551 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-06 18:29:15,551 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 14 msec
2015-12-06 18:29:15,571 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-06 18:29:15,572 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-06 18:29:15,574 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-06 18:29:15,574 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-06 18:29:15,579 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-06 18:29:15,579 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 1195710 milliseconds
2015-12-06 18:29:15,586 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 7 millisecond(s).
2015-12-06 18:29:45,579 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:29:45,580 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:30:15,579 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:30:15,580 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:30:45,579 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:30:45,579 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:30:51,321 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-06 18:30:51,323 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-06 18:33:19,362 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-06 18:33:19,369 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-06 18:33:19,371 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-06 18:33:19,697 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-06 18:33:19,780 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-06 18:33:19,780 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-06 18:33:19,781 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-06 18:33:19,782 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-06 18:33:20,116 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-06 18:33:20,165 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-06 18:33:20,168 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-06 18:33:20,178 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-06 18:33:20,180 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-06 18:33:20,181 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-06 18:33:20,181 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-06 18:33:20,205 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-06 18:33:20,209 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-06 18:33:20,222 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-06 18:33:20,222 INFO org.mortbay.log: jetty-6.1.26
2015-12-06 18:33:20,457 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-06 18:33:20,546 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-06 18:33:20,546 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-06 18:33:20,623 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-06 18:33:20,636 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-06 18:33:20,675 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-06 18:33:20,675 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-06 18:33:20,677 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-06 18:33:20,679 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 06 18:33:20
2015-12-06 18:33:20,681 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-06 18:33:20,681 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:33:20,682 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-06 18:33:20,682 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-06 18:33:20,724 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-06 18:33:20,725 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-06 18:33:20,725 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-06 18:33:20,725 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-06 18:33:20,725 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-06 18:33:20,725 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-06 18:33:20,725 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-06 18:33:20,725 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-06 18:33:20,725 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-06 18:33:20,729 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-06 18:33:20,729 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-06 18:33:20,729 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-06 18:33:20,730 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-06 18:33:20,731 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-06 18:33:20,893 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-06 18:33:20,893 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:33:20,893 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-06 18:33:20,893 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-06 18:33:20,897 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-06 18:33:20,911 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-06 18:33:20,912 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:33:20,912 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-06 18:33:20,912 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-06 18:33:20,913 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-06 18:33:20,913 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-06 18:33:20,913 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-06 18:33:20,915 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-06 18:33:20,915 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-06 18:33:20,919 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-06 18:33:20,919 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:33:20,920 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-06 18:33:20,920 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-06 18:33:20,925 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-06 18:33:20,925 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-06 18:33:20,925 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-06 18:33:20,940 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/name/in_use.lock acquired by nodename 9683@ubuntu
2015-12-06 18:33:21,063 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/parallels/hadoop/tmp/dfs/name/current
2015-12-06 18:33:21,107 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000000011 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000011-0000000000000000011
2015-12-06 18:33:21,158 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-06 18:33:21,192 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-06 18:33:21,192 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000000
2015-12-06 18:33:21,192 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4063b180 expecting start txid #1
2015-12-06 18:33:21,193 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000001-0000000000000000010
2015-12-06 18:33:21,194 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000001-0000000000000000010' to transaction ID 1
2015-12-06 18:33:21,242 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000001-0000000000000000010 of size 1048576 edits # 10 loaded in 0 seconds
2015-12-06 18:33:21,242 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@511a6b86 expecting start txid #11
2015-12-06 18:33:21,242 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000011-0000000000000000011
2015-12-06 18:33:21,242 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000011-0000000000000000011' to transaction ID 1
2015-12-06 18:33:21,247 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000011-0000000000000000011 of size 1048576 edits # 1 loaded in 0 seconds
2015-12-06 18:33:21,247 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-12-06 18:33:21,248 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 12
2015-12-06 18:33:21,287 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-06 18:33:21,287 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 362 msecs
2015-12-06 18:33:21,454 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-06 18:33:21,459 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-06 18:33:21,470 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-06 18:33:21,496 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-06 18:33:21,503 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-06 18:33:21,503 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-06 18:33:21,503 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-06 18:33:21,503 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2015-12-06 18:33:21,503 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2015-12-06 18:33:21,503 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-12-06 18:33:21,517 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2015-12-06 18:33:21,517 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-06 18:33:21,517 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-12-06 18:33:21,518 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-06 18:33:21,518 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-06 18:33:21,518 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 13 msec
2015-12-06 18:33:21,548 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-06 18:33:21,549 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-06 18:33:21,551 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-06 18:33:21,551 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-06 18:33:21,555 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-06 18:33:21,555 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 1441686 milliseconds
2015-12-06 18:33:21,561 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 6 millisecond(s).
2015-12-06 18:33:51,556 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:33:51,556 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:34:21,556 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:34:21,557 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:34:51,556 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:34:51,557 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:35:21,556 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:35:21,557 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:35:51,556 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:35:51,557 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:36:21,557 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:36:21,557 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:36:42,222 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-06 18:36:42,223 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-06 18:41:07,027 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-06 18:41:07,035 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-06 18:41:07,041 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-06 18:41:07,373 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-06 18:41:07,457 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-06 18:41:07,457 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-06 18:41:07,459 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-06 18:41:07,459 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-06 18:41:07,799 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-06 18:41:07,844 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-06 18:41:07,850 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-06 18:41:07,863 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-06 18:41:07,866 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-06 18:41:07,866 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-06 18:41:07,866 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-06 18:41:07,902 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-06 18:41:07,903 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-06 18:41:07,931 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-06 18:41:07,931 INFO org.mortbay.log: jetty-6.1.26
2015-12-06 18:41:08,234 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-06 18:41:08,310 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-06 18:41:08,310 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-06 18:41:08,359 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-06 18:41:08,371 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-06 18:41:08,414 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-06 18:41:08,414 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-06 18:41:08,417 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-06 18:41:08,419 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 06 18:41:08
2015-12-06 18:41:08,421 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-06 18:41:08,421 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:41:08,422 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-06 18:41:08,422 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-06 18:41:08,465 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-06 18:41:08,466 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-06 18:41:08,466 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-06 18:41:08,466 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-06 18:41:08,466 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-06 18:41:08,466 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-06 18:41:08,466 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-06 18:41:08,466 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-06 18:41:08,466 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-06 18:41:08,473 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-06 18:41:08,473 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-06 18:41:08,473 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-06 18:41:08,473 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-06 18:41:08,475 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-06 18:41:08,628 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-06 18:41:08,628 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:41:08,628 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-06 18:41:08,628 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-06 18:41:08,631 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-06 18:41:08,643 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-06 18:41:08,643 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:41:08,643 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-06 18:41:08,643 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-06 18:41:08,644 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-06 18:41:08,644 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-06 18:41:08,644 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-06 18:41:08,645 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-06 18:41:08,646 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-06 18:41:08,648 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-06 18:41:08,648 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:41:08,648 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-06 18:41:08,648 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-06 18:41:08,651 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-06 18:41:08,652 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-06 18:41:08,652 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-06 18:41:08,660 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/name/in_use.lock acquired by nodename 12139@ubuntu
2015-12-06 18:41:08,782 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/parallels/hadoop/tmp/dfs/name/current
2015-12-06 18:41:08,782 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2015-12-06 18:41:08,822 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-12-06 18:41:08,853 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-06 18:41:08,853 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000000
2015-12-06 18:41:08,859 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-12-06 18:41:08,860 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2015-12-06 18:41:08,949 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-06 18:41:08,949 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 297 msecs
2015-12-06 18:41:09,129 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-06 18:41:09,135 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-06 18:41:09,146 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-06 18:41:09,175 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-06 18:41:09,182 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-06 18:41:09,182 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-06 18:41:09,182 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-06 18:41:09,183 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2015-12-06 18:41:09,183 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2015-12-06 18:41:09,183 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-12-06 18:41:09,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2015-12-06 18:41:09,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-06 18:41:09,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-12-06 18:41:09,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-06 18:41:09,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-06 18:41:09,196 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 13 msec
2015-12-06 18:41:09,216 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-06 18:41:09,217 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-06 18:41:09,227 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-06 18:41:09,227 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-06 18:41:09,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-06 18:41:09,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 1909363 milliseconds
2015-12-06 18:41:09,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 6 millisecond(s).
2015-12-06 18:41:13,417 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0) storage 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-06 18:41:13,420 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-06 18:41:13,421 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-06 18:41:13,496 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-06 18:41:13,496 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-75e73aff-3510-410a-91be-881360516f94 for DN 127.0.0.1:50010
2015-12-06 18:41:13,528 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-75e73aff-3510-410a-91be-881360516f94,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-12-06 18:41:13,528 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 0, hasStaleStorages: false, processing time: 1 msecs
2015-12-06 18:41:39,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:41:39,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:42:09,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:42:09,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:42:39,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:42:39,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:43:09,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-06 18:43:09,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:43:39,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:43:39,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:43:57,035 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 9 
2015-12-06 18:44:09,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:44:09,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:44:39,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:44:39,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:44:45,758 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input._COPYING_. BP-1841603009-127.0.1.1-1449456040486 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-06 18:44:46,045 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-06 18:44:46,045 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2015-12-06 18:44:46,052 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 22
2015-12-06 18:44:46,463 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1868091400_1
2015-12-06 18:45:09,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:45:09,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:45:16,834 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 10 Total time for transactions(ms): 14 Number of transactions batched in Syncs: 0 Number of syncs: 9 SyncTimes(ms): 37 
2015-12-06 18:45:16,835 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741825_1001 127.0.0.1:50010 
2015-12-06 18:45:18,244 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741825_1001]
2015-12-06 18:45:39,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:45:39,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:45:58,468 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/file01._COPYING_. BP-1841603009-127.0.1.1-1449456040486 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-06 18:45:58,618 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-06 18:45:58,622 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/file01._COPYING_ is closed by DFSClient_NONMAPREDUCE_914896958_1
2015-12-06 18:46:09,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:46:09,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:46:11,594 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/input/file02._COPYING_. BP-1841603009-127.0.1.1-1449456040486 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-06 18:46:11,756 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-06 18:46:11,766 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/input/file02._COPYING_ is closed by DFSClient_NONMAPREDUCE_-263158760_1
2015-12-06 18:46:39,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:46:39,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:47:09,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:47:09,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:47:39,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:47:39,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:48:09,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:48:09,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:48:39,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:48:39,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:49:09,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:49:09,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:49:38,295 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 28 Total time for transactions(ms): 14 Number of transactions batched in Syncs: 0 Number of syncs: 19 SyncTimes(ms): 58 
2015-12-06 18:49:39,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:49:39,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:50:06,617 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449456599213_0001/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-06 18:50:06,816 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-06 18:50:06,824 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449456599213_0001/job.jar is closed by DFSClient_NONMAPREDUCE_289117872_1
2015-12-06 18:50:06,834 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449456599213_0001/job.jar
2015-12-06 18:50:06,945 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449456599213_0001/job.split
2015-12-06 18:50:06,965 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449456599213_0001/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-06 18:50:06,985 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-06 18:50:06,988 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449456599213_0001/job.split is closed by DFSClient_NONMAPREDUCE_289117872_1
2015-12-06 18:50:06,998 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449456599213_0001/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-06 18:50:07,009 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-06 18:50:07,012 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449456599213_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_289117872_1
2015-12-06 18:50:07,278 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449456599213_0001/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741831_1007{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-06 18:50:07,314 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741831_1007{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-06 18:50:07,317 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449456599213_0001/job.xml is closed by DFSClient_NONMAPREDUCE_289117872_1
2015-12-06 18:50:09,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:50:09,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:50:17,747 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449456599213_0001/job_1449456599213_0001_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741832_1008{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-06 18:50:17,886 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741832_1008{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-06 18:50:17,892 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449456599213_0001/job_1449456599213_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1284525877_1
2015-12-06 18:50:28,291 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449456599213_0001/job_1449456599213_0001_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073741833_1009{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-06 18:50:29,102 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449456599213_0001/job_1449456599213_0001_1.jhist for DFSClient_NONMAPREDUCE_1284525877_1
2015-12-06 18:50:35,494 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/output/_temporary/1/_temporary/attempt_1449456599213_0001_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073741834_1010{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-06 18:50:35,722 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741834_1010{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-06 18:50:35,726 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/output/_temporary/1/_temporary/attempt_1449456599213_0001_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449456599213_0001_r_000000_0_830353187_1
2015-12-06 18:50:36,995 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449456599213_0001/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_1284525877_1
2015-12-06 18:50:37,034 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_1284525877_1
2015-12-06 18:50:37,040 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449456599213_0001/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_1284525877_1
2015-12-06 18:50:37,062 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741833_1009{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13676
2015-12-06 18:50:37,065 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449456599213_0001/job_1449456599213_0001_1.jhist is closed by DFSClient_NONMAPREDUCE_1284525877_1
2015-12-06 18:50:37,073 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449456599213_0001.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741835_1011{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-06 18:50:37,079 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741835_1011{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-06 18:50:37,082 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449456599213_0001.summary_tmp is closed by DFSClient_NONMAPREDUCE_1284525877_1
2015-12-06 18:50:37,105 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449456599213_0001-1449456607609-parallels-word+count-1449456637041-2-1-SUCCEEDED-default-1449456617299.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741836_1012{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-06 18:50:37,113 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741836_1012{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-06 18:50:37,116 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449456599213_0001-1449456607609-parallels-word+count-1449456637041-2-1-SUCCEEDED-default-1449456617299.jhist_tmp is closed by DFSClient_NONMAPREDUCE_1284525877_1
2015-12-06 18:50:37,137 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449456599213_0001_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741837_1013{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-06 18:50:37,143 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741837_1013{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-06 18:50:37,145 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449456599213_0001_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_1284525877_1
2015-12-06 18:50:38,211 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741828_1004 127.0.0.1:50010 
2015-12-06 18:50:38,211 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741829_1005 127.0.0.1:50010 
2015-12-06 18:50:38,211 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741830_1006 127.0.0.1:50010 
2015-12-06 18:50:38,211 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741831_1007 127.0.0.1:50010 
2015-12-06 18:50:38,211 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741833_1009 127.0.0.1:50010 
2015-12-06 18:50:38,211 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741832_1008 127.0.0.1:50010 
2015-12-06 18:50:39,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:50:39,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:50:39,301 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741828_1004, blk_1073741829_1005, blk_1073741830_1006, blk_1073741831_1007, blk_1073741832_1008, blk_1073741833_1009]
2015-12-06 18:51:09,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:51:09,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:51:39,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:51:39,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:52:09,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:52:09,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:52:39,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:52:39,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:53:09,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:53:09,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:53:39,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:53:39,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:54:09,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:54:09,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:54:39,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:54:39,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:55:09,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:55:09,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:55:39,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:55:39,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:56:09,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:56:09,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:56:39,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:56:39,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:57:09,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:57:09,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:57:39,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:57:39,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:58:09,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:58:09,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:58:39,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:58:39,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 18:59:09,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 18:59:09,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 18:59:39,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 18:59:39,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:00:09,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:00:09,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:00:39,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:00:39,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:01:09,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:01:09,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:01:39,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:01:39,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:02:09,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:02:09,255 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:02:39,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:02:39,255 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:03:09,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:03:09,255 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:03:39,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:03:39,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:04:09,255 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:04:09,255 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:04:39,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:04:39,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:05:09,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:05:09,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-06 19:05:39,257 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:05:39,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:06:09,257 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:06:09,257 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:06:39,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:06:39,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:07:09,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:07:09,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:07:39,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:07:39,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:08:09,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:08:09,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 4 millisecond(s).
2015-12-06 19:08:39,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:08:39,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:09:09,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:09:09,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:09:39,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:09:39,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:10:09,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:10:09,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:10:19,026 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-06 19:10:19,027 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-06 19:10:19,027 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2015-12-06 19:10:19,027 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 114 Total time for transactions(ms): 17 Number of transactions batched in Syncs: 0 Number of syncs: 80 SyncTimes(ms): 734 
2015-12-06 19:10:19,029 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 114 Total time for transactions(ms): 17 Number of transactions batched in Syncs: 0 Number of syncs: 81 SyncTimes(ms): 736 
2015-12-06 19:10:19,033 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000000001 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000001-0000000000000000114
2015-12-06 19:10:19,053 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 115
2015-12-06 19:10:22,165 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 500.00 KB/s
2015-12-06 19:10:22,166 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000114 size 1786 bytes.
2015-12-06 19:10:22,172 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2015-12-06 19:10:39,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:10:39,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:11:09,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:11:09,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:11:39,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:11:39,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:12:09,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:12:09,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:12:39,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:12:39,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:13:09,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:13:09,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:13:39,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:13:39,263 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:14:09,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:14:09,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:14:39,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:14:39,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:15:09,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:15:09,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:15:39,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:15:39,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:16:09,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:16:09,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:16:39,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:16:39,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:17:09,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:17:09,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:17:39,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:17:39,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:18:09,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:18:09,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:18:39,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:18:39,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:19:09,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:19:09,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:19:39,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:19:39,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:20:09,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:20:09,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:20:39,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:20:39,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:21:09,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:21:09,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:21:39,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:21:39,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:22:09,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:22:09,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:22:39,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:22:39,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:23:09,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:23:09,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:23:39,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:23:39,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:24:09,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:24:09,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:24:39,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:24:39,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:25:09,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:25:09,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:25:39,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:25:39,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:26:09,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:26:09,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:26:39,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:26:39,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:27:09,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-06 19:27:09,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:27:39,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-06 19:27:39,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:28:09,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:28:09,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-06 19:28:39,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-06 19:28:39,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-06 19:29:08,791 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-06 19:29:08,847 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-10 17:08:42,288 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-10 17:08:42,300 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-10 17:08:42,303 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-10 17:08:42,659 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-10 17:08:42,788 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-10 17:08:42,788 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-10 17:08:42,790 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-10 17:08:42,790 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-10 17:08:43,201 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-10 17:08:43,266 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-10 17:08:43,283 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-10 17:08:43,309 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-10 17:08:43,311 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-10 17:08:43,311 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-10 17:08:43,311 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-10 17:08:43,368 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-10 17:08:43,371 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-10 17:08:43,406 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-10 17:08:43,406 INFO org.mortbay.log: jetty-6.1.26
2015-12-10 17:08:43,745 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-10 17:08:43,827 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-10 17:08:43,827 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-10 17:08:43,876 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-10 17:08:43,900 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-10 17:08:43,954 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-10 17:08:43,954 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-10 17:08:43,959 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-10 17:08:43,962 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 10 17:08:43
2015-12-10 17:08:43,964 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-10 17:08:43,965 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-10 17:08:43,968 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-10 17:08:43,968 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-10 17:08:44,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-10 17:08:44,014 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-10 17:08:44,014 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-10 17:08:44,014 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-10 17:08:44,014 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-10 17:08:44,014 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-10 17:08:44,016 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-10 17:08:44,016 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-10 17:08:44,016 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-10 17:08:44,021 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-10 17:08:44,021 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-10 17:08:44,021 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-10 17:08:44,021 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-10 17:08:44,024 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-10 17:08:44,263 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-10 17:08:44,263 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-10 17:08:44,263 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-10 17:08:44,263 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-10 17:08:44,265 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-10 17:08:44,280 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-10 17:08:44,280 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-10 17:08:44,280 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-10 17:08:44,280 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-10 17:08:44,282 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-10 17:08:44,282 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-10 17:08:44,282 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-10 17:08:44,283 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-10 17:08:44,283 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-10 17:08:44,286 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-10 17:08:44,286 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-10 17:08:44,286 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-10 17:08:44,286 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-10 17:08:44,290 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-10 17:08:44,290 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-10 17:08:44,290 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-10 17:08:44,308 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/name/in_use.lock acquired by nodename 26758@ubuntu
2015-12-10 17:08:44,432 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/parallels/hadoop/tmp/dfs/name/current
2015-12-10 17:08:44,491 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000000115 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000115-0000000000000000115
2015-12-10 17:08:44,553 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 20 INodes.
2015-12-10 17:08:44,635 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-10 17:08:44,635 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 114 from /home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000114
2015-12-10 17:08:44,635 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1ebd94c6 expecting start txid #115
2015-12-10 17:08:44,635 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000115-0000000000000000115
2015-12-10 17:08:44,637 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000115-0000000000000000115' to transaction ID 115
2015-12-10 17:08:44,639 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000115-0000000000000000115 of size 1048576 edits # 1 loaded in 0 seconds
2015-12-10 17:08:44,641 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2015-12-10 17:08:44,642 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2015-12-10 17:08:44,696 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 114
2015-12-10 17:08:44,696 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2015-12-10 17:08:44,706 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 116
2015-12-10 17:08:44,746 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-10 17:08:44,746 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 456 msecs
2015-12-10 17:08:45,090 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-10 17:08:45,099 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-10 17:08:45,111 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-10 17:08:45,154 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-10 17:08:45,171 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-10 17:08:45,171 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-10 17:08:45,171 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 6 blocks to reach the threshold 0.9990 of total blocks 6.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2015-12-10 17:08:45,205 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-10 17:08:45,206 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-10 17:08:45,209 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-10 17:08:45,209 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-10 17:08:45,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-10 17:08:45,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 28405453 milliseconds
2015-12-10 17:08:45,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:08:49,140 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0) storage 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-10 17:08:49,143 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-10 17:08:49,146 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-10 17:08:49,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-10 17:08:49,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-75e73aff-3510-410a-91be-881360516f94 for DN 127.0.0.1:50010
2015-12-10 17:08:49,361 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 5 has reached the threshold 0.9990 of total blocks 6. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2015-12-10 17:08:49,362 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-10 17:08:49,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-75e73aff-3510-410a-91be-881360516f94,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-12-10 17:08:49,363 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 6, hasStaleStorages: false, processing time: 7 msecs
2015-12-10 17:08:49,364 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 6
2015-12-10 17:08:49,364 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-10 17:08:49,364 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-12-10 17:08:49,364 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-10 17:08:49,364 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-10 17:08:49,364 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 2 msec
2015-12-10 17:09:09,368 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 6 has reached the threshold 0.9990 of total blocks 6. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2015-12-10 17:09:15,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:09:15,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:09:19,376 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 35 secs
2015-12-10 17:09:19,376 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2015-12-10 17:09:19,376 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2015-12-10 17:09:19,376 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-12-10 17:09:45,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:09:45,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:09:55,819 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-10 17:09:55,819 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-10 17:09:55,819 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 116
2015-12-10 17:09:55,822 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 6 
2015-12-10 17:09:55,842 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 26 
2015-12-10 17:09:55,847 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000000116 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000116-0000000000000000117
2015-12-10 17:09:55,848 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 118
2015-12-10 17:09:57,628 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 1000.00 KB/s
2015-12-10 17:09:57,628 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000117 size 1786 bytes.
2015-12-10 17:09:57,631 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 115
2015-12-10 17:09:57,631 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000114, cpktTxId=0000000000000000114)
2015-12-10 17:09:59,971 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-10 17:10:01,850 INFO logs: Aliases are enabled
2015-12-10 17:10:15,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:10:15,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-10 17:10:45,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:10:45,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:11:15,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:11:15,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:11:30,806 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 18 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 15 
2015-12-10 17:11:30,835 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741826_1002 127.0.0.1:50010 
2015-12-10 17:11:30,835 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741827_1003 127.0.0.1:50010 
2015-12-10 17:11:33,201 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741826_1002, blk_1073741827_1003]
2015-12-10 17:11:43,576 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741834_1010 127.0.0.1:50010 
2015-12-10 17:11:45,203 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741834_1010]
2015-12-10 17:11:45,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:11:45,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:12:15,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:12:15,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:12:35,612 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 5 Total time for transactions(ms): 28 Number of transactions batched in Syncs: 0 Number of syncs: 5 SyncTimes(ms): 124 
2015-12-10 17:12:45,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:12:45,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:13:15,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:13:15,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:13:45,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:13:45,226 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:14:15,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:14:15,226 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:14:45,226 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:14:45,226 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:15:08,420 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 6 Total time for transactions(ms): 28 Number of transactions batched in Syncs: 0 Number of syncs: 6 SyncTimes(ms): 135 
2015-12-10 17:15:08,595 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/input/Chess_Test_Data/ficsgamesdb_small_201501.pgn._COPYING_. BP-1841603009-127.0.1.1-1449456040486 blk_1073741838_1014{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:15:09,108 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741838_1014{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-10 17:15:09,161 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741838_1014{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 7560294
2015-12-10 17:15:09,535 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/input/Chess_Test_Data/ficsgamesdb_small_201501.pgn._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1425481013_1
2015-12-10 17:15:09,580 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/input/Chess_Test_Data/ficsgamesdb_small_201502.pgn._COPYING_. BP-1841603009-127.0.1.1-1449456040486 blk_1073741839_1015{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:15:09,634 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741839_1015{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:15:09,646 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/input/Chess_Test_Data/ficsgamesdb_small_201502.pgn._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1425481013_1
2015-12-10 17:15:09,678 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/input/Chess_Test_Data/ficsgamesdb_small_201503.pgn._COPYING_. BP-1841603009-127.0.1.1-1449456040486 blk_1073741840_1016{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:15:09,783 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741840_1016{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:15:09,800 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/input/Chess_Test_Data/ficsgamesdb_small_201503.pgn._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1425481013_1
2015-12-10 17:15:15,226 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:15:15,227 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:15:45,227 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:15:45,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:15:45,978 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741838_1014 127.0.0.1:50010 
2015-12-10 17:15:45,979 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741839_1015 127.0.0.1:50010 
2015-12-10 17:15:45,979 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741840_1016 127.0.0.1:50010 
2015-12-10 17:15:48,236 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741840_1016, blk_1073741838_1014, blk_1073741839_1015]
2015-12-10 17:16:15,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:16:15,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:16:19,110 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 27 Total time for transactions(ms): 30 Number of transactions batched in Syncs: 1 Number of syncs: 21 SyncTimes(ms): 220 
2015-12-10 17:16:19,155 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/input/ficsgamesdb_small_201501.pgn._COPYING_. BP-1841603009-127.0.1.1-1449456040486 blk_1073741841_1017{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:16:19,383 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741841_1017{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:16:19,413 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/input/ficsgamesdb_small_201501.pgn._COPYING_ is closed by DFSClient_NONMAPREDUCE_1770453338_1
2015-12-10 17:16:19,436 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/input/ficsgamesdb_small_201502.pgn._COPYING_. BP-1841603009-127.0.1.1-1449456040486 blk_1073741842_1018{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:16:19,485 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741842_1018{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:16:19,501 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/input/ficsgamesdb_small_201502.pgn._COPYING_ is closed by DFSClient_NONMAPREDUCE_1770453338_1
2015-12-10 17:16:19,519 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/input/ficsgamesdb_small_201503.pgn._COPYING_. BP-1841603009-127.0.1.1-1449456040486 blk_1073741843_1019{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:16:19,614 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741843_1019{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:16:19,646 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/input/ficsgamesdb_small_201503.pgn._COPYING_ is closed by DFSClient_NONMAPREDUCE_1770453338_1
2015-12-10 17:16:45,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:16:45,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:17:15,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:17:15,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:17:45,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:17:45,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:18:15,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:18:15,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:18:45,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:18:45,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:19:15,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:19:15,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:19:45,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:19:45,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:20:15,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:20:15,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:20:38,406 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 45 Total time for transactions(ms): 30 Number of transactions batched in Syncs: 1 Number of syncs: 33 SyncTimes(ms): 307 
2015-12-10 17:20:38,597 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0001/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073741844_1020{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:20:38,772 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741844_1020{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:20:38,778 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0001/job.jar is closed by DFSClient_NONMAPREDUCE_-1251220522_1
2015-12-10 17:20:38,793 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0001/job.jar
2015-12-10 17:20:38,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0001/job.split
2015-12-10 17:20:38,978 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0001/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073741845_1021{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:20:38,992 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741845_1021{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:20:38,996 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0001/job.split is closed by DFSClient_NONMAPREDUCE_-1251220522_1
2015-12-10 17:20:39,007 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0001/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073741846_1022{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:20:39,015 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741846_1022{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:20:39,018 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1251220522_1
2015-12-10 17:20:39,266 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0001/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741847_1023{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:20:39,282 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741847_1023{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-10 17:20:39,290 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741847_1023{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 87255
2015-12-10 17:20:39,687 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0001/job.xml is closed by DFSClient_NONMAPREDUCE_-1251220522_1
2015-12-10 17:20:45,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:20:45,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:20:50,318 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0001/job_1449785342086_0001_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741848_1024{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:20:50,487 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741848_1024{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:20:50,494 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0001/job_1449785342086_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-480449843_1
2015-12-10 17:21:03,253 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0001/job_1449785342086_0001_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073741849_1025{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:21:03,467 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0001/job_1449785342086_0001_1.jhist for DFSClient_NONMAPREDUCE_-480449843_1
2015-12-10 17:21:15,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30037 milliseconds
2015-12-10 17:21:15,307 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 37 millisecond(s).
2015-12-10 17:21:21,144 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449785342086_0001_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073741850_1026{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:21:21,298 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741850_1026{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:21:21,305 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449785342086_0001_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449785342086_0001_r_000000_0_-2036151038_1
2015-12-10 17:21:21,674 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0001/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-480449843_1
2015-12-10 17:21:21,721 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-480449843_1
2015-12-10 17:21:21,727 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0001/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-480449843_1
2015-12-10 17:21:21,766 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741849_1025{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13136
2015-12-10 17:21:21,769 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0001/job_1449785342086_0001_1.jhist is closed by DFSClient_NONMAPREDUCE_-480449843_1
2015-12-10 17:21:21,782 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0001.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741851_1027{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:21:21,794 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741851_1027{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:21:21,798 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0001.summary_tmp is closed by DFSClient_NONMAPREDUCE_-480449843_1
2015-12-10 17:21:21,863 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0001-1449786040101-parallels-word+count-1449786081730-3-1-SUCCEEDED-default-1449786049658.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741852_1028{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:21:21,874 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741852_1028{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:21:21,878 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0001-1449786040101-parallels-word+count-1449786081730-3-1-SUCCEEDED-default-1449786049658.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-480449843_1
2015-12-10 17:21:21,907 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0001_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741853_1029{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:21:21,930 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741853_1029{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:21:21,933 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0001_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-480449843_1
2015-12-10 17:21:23,052 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741844_1020 127.0.0.1:50010 
2015-12-10 17:21:23,053 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741845_1021 127.0.0.1:50010 
2015-12-10 17:21:23,053 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741846_1022 127.0.0.1:50010 
2015-12-10 17:21:23,053 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741847_1023 127.0.0.1:50010 
2015-12-10 17:21:23,054 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741849_1025 127.0.0.1:50010 
2015-12-10 17:21:23,054 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741848_1024 127.0.0.1:50010 
2015-12-10 17:21:24,307 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741844_1020, blk_1073741845_1021, blk_1073741846_1022, blk_1073741847_1023, blk_1073741848_1024, blk_1073741849_1025]
2015-12-10 17:21:45,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:21:45,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:22:15,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-10 17:22:15,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:22:45,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:22:45,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:23:15,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:23:15,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-10 17:23:45,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:23:45,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-10 17:24:15,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:24:15,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:24:28,872 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 125 Total time for transactions(ms): 41 Number of transactions batched in Syncs: 2 Number of syncs: 89 SyncTimes(ms): 750 
2015-12-10 17:24:28,873 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741850_1026 127.0.0.1:50010 
2015-12-10 17:24:30,343 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741850_1026]
2015-12-10 17:24:35,739 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0002/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073741854_1030{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:24:35,897 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741854_1030{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:24:35,901 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0002/job.jar is closed by DFSClient_NONMAPREDUCE_-782895968_1
2015-12-10 17:24:35,906 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0002/job.jar
2015-12-10 17:24:35,994 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0002/job.split
2015-12-10 17:24:36,022 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0002/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073741855_1031{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:24:36,035 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741855_1031{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:24:36,037 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0002/job.split is closed by DFSClient_NONMAPREDUCE_-782895968_1
2015-12-10 17:24:36,053 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0002/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073741856_1032{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:24:36,059 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741856_1032{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:24:36,062 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0002/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-782895968_1
2015-12-10 17:24:36,327 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0002/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741857_1033{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:24:36,348 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741857_1033{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:24:36,350 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0002/job.xml is closed by DFSClient_NONMAPREDUCE_-782895968_1
2015-12-10 17:24:44,599 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0002/job_1449785342086_0002_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741858_1034{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:24:44,759 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741858_1034{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:24:44,766 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0002/job_1449785342086_0002_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1071761326_1
2015-12-10 17:24:45,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:24:45,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:24:55,833 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0002/job_1449785342086_0002_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073741859_1035{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:24:56,068 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0002/job_1449785342086_0002_1.jhist for DFSClient_NONMAPREDUCE_1071761326_1
2015-12-10 17:25:03,179 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449785342086_0002_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073741860_1036{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:25:03,380 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741860_1036{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:25:03,387 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449785342086_0002_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449785342086_0002_r_000000_0_-150447708_1
2015-12-10 17:25:03,650 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0002/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_1071761326_1
2015-12-10 17:25:03,723 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_1071761326_1
2015-12-10 17:25:03,730 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0002/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_1071761326_1
2015-12-10 17:25:03,761 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741859_1035{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13087
2015-12-10 17:25:03,765 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0002/job_1449785342086_0002_1.jhist is closed by DFSClient_NONMAPREDUCE_1071761326_1
2015-12-10 17:25:03,774 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0002.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741861_1037{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:25:03,785 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741861_1037{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-10 17:25:03,786 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741861_1037{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 352
2015-12-10 17:25:04,191 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0002.summary_tmp is closed by DFSClient_NONMAPREDUCE_1071761326_1
2015-12-10 17:25:04,222 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0002-1449786276636-parallels-word+count-1449786303732-1-1-SUCCEEDED-default-1449786283897.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741862_1038{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:25:04,232 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741862_1038{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:25:04,235 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0002-1449786276636-parallels-word+count-1449786303732-1-1-SUCCEEDED-default-1449786283897.jhist_tmp is closed by DFSClient_NONMAPREDUCE_1071761326_1
2015-12-10 17:25:04,261 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0002_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741863_1039{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:25:04,293 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741863_1039{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-10 17:25:04,295 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741863_1039{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 103830
2015-12-10 17:25:04,697 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0002_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_1071761326_1
2015-12-10 17:25:05,805 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741854_1030 127.0.0.1:50010 
2015-12-10 17:25:05,805 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741855_1031 127.0.0.1:50010 
2015-12-10 17:25:05,805 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741856_1032 127.0.0.1:50010 
2015-12-10 17:25:05,805 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741857_1033 127.0.0.1:50010 
2015-12-10 17:25:05,806 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741859_1035 127.0.0.1:50010 
2015-12-10 17:25:05,806 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741858_1034 127.0.0.1:50010 
2015-12-10 17:25:06,350 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741856_1032, blk_1073741857_1033, blk_1073741858_1034, blk_1073741859_1035, blk_1073741854_1030, blk_1073741855_1031]
2015-12-10 17:25:15,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:25:15,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:25:45,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:25:45,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:26:15,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:26:15,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:26:45,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:26:45,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:27:15,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:27:15,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:27:45,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:27:45,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:28:15,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:28:15,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:28:45,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:28:45,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:29:15,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:29:15,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:29:45,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:29:45,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:30:15,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:30:15,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:30:45,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:30:45,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:30:49,114 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 206 Total time for transactions(ms): 49 Number of transactions batched in Syncs: 4 Number of syncs: 146 SyncTimes(ms): 968 
2015-12-10 17:30:49,115 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741860_1036 127.0.0.1:50010 
2015-12-10 17:30:51,431 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741860_1036]
2015-12-10 17:31:15,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-10 17:31:15,281 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:31:45,281 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:31:45,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 17:32:15,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 17:32:15,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:32:45,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:32:45,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-10 17:33:15,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:33:15,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:33:22,873 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 207 Total time for transactions(ms): 50 Number of transactions batched in Syncs: 4 Number of syncs: 147 SyncTimes(ms): 968 
2015-12-10 17:33:22,983 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0003/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073741864_1040{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:33:23,140 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741864_1040{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:33:23,146 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0003/job.jar is closed by DFSClient_NONMAPREDUCE_-309811808_1
2015-12-10 17:33:23,152 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0003/job.jar
2015-12-10 17:33:23,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0003/job.split
2015-12-10 17:33:23,247 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0003/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073741865_1041{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:33:23,254 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741865_1041{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:33:23,258 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0003/job.split is closed by DFSClient_NONMAPREDUCE_-309811808_1
2015-12-10 17:33:23,274 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0003/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073741866_1042{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:33:23,282 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741866_1042{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:33:23,284 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0003/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-309811808_1
2015-12-10 17:33:23,480 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0003/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741867_1043{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:33:23,501 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741867_1043{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:33:23,505 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0003/job.xml is closed by DFSClient_NONMAPREDUCE_-309811808_1
2015-12-10 17:33:29,880 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0003/job_1449785342086_0003_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741868_1044{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:33:30,117 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741868_1044{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:33:30,131 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0003/job_1449785342086_0003_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-204081518_1
2015-12-10 17:33:43,403 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0003/job_1449785342086_0003_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073741869_1045{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:33:43,623 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0003/job_1449785342086_0003_1.jhist for DFSClient_NONMAPREDUCE_-204081518_1
2015-12-10 17:33:45,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30006 milliseconds
2015-12-10 17:33:45,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-10 17:33:52,814 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449785342086_0003_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073741870_1046{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:33:53,107 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741870_1046{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:33:53,115 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449785342086_0003_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449785342086_0003_r_000000_0_-1391354756_1
2015-12-10 17:33:54,428 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0003/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-204081518_1
2015-12-10 17:33:54,480 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-204081518_1
2015-12-10 17:33:54,487 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0003/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-204081518_1
2015-12-10 17:33:54,533 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741869_1045{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12220
2015-12-10 17:33:54,537 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449785342086_0003/job_1449785342086_0003_1.jhist is closed by DFSClient_NONMAPREDUCE_-204081518_1
2015-12-10 17:33:54,551 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0003.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741871_1047{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:33:54,559 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741871_1047{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:33:54,561 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0003.summary_tmp is closed by DFSClient_NONMAPREDUCE_-204081518_1
2015-12-10 17:33:54,619 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0003-1449786803761-parallels-word+count-1449786834489-3-1-SUCCEEDED-default-1449786809418.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741872_1048{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:33:54,637 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741872_1048{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:33:54,640 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0003-1449786803761-parallels-word+count-1449786834489-3-1-SUCCEEDED-default-1449786809418.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-204081518_1
2015-12-10 17:33:54,678 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0003_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741873_1049{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 17:33:54,687 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741873_1049{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 17:33:54,690 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449785342086_0003_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-204081518_1
2015-12-10 17:33:55,739 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741864_1040 127.0.0.1:50010 
2015-12-10 17:33:55,739 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741865_1041 127.0.0.1:50010 
2015-12-10 17:33:55,739 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741866_1042 127.0.0.1:50010 
2015-12-10 17:33:55,739 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741867_1043 127.0.0.1:50010 
2015-12-10 17:33:55,739 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741869_1045 127.0.0.1:50010 
2015-12-10 17:33:55,739 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741868_1044 127.0.0.1:50010 
2015-12-10 17:33:57,470 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741864_1040, blk_1073741865_1041, blk_1073741866_1042, blk_1073741867_1043, blk_1073741868_1044, blk_1073741869_1045]
2015-12-10 17:34:15,289 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 17:34:15,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 17:34:41,006 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-10 17:34:41,078 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-10 21:55:36,086 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-10 21:55:36,094 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-10 21:55:36,096 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-10 21:55:36,444 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-10 21:55:36,559 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-10 21:55:36,559 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-10 21:55:36,561 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-10 21:55:36,561 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-10 21:55:36,924 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-10 21:55:36,980 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-10 21:55:36,983 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-10 21:55:36,995 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-10 21:55:36,998 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-10 21:55:36,998 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-10 21:55:36,998 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-10 21:55:37,029 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-10 21:55:37,031 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-10 21:55:37,055 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-10 21:55:37,055 INFO org.mortbay.log: jetty-6.1.26
2015-12-10 21:55:37,364 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-10 21:55:37,431 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-10 21:55:37,431 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-10 21:55:37,472 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-10 21:55:37,484 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-10 21:55:37,525 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-10 21:55:37,525 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-10 21:55:37,527 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-10 21:55:37,528 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 10 21:55:37
2015-12-10 21:55:37,530 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-10 21:55:37,530 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-10 21:55:37,533 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-10 21:55:37,534 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-10 21:55:37,568 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-10 21:55:37,568 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-10 21:55:37,568 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-10 21:55:37,568 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-10 21:55:37,568 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-10 21:55:37,568 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-10 21:55:37,568 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-10 21:55:37,568 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-10 21:55:37,568 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-10 21:55:37,573 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-10 21:55:37,573 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-10 21:55:37,573 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-10 21:55:37,573 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-10 21:55:37,575 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-10 21:55:37,796 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-10 21:55:37,796 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-10 21:55:37,796 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-10 21:55:37,796 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-10 21:55:37,798 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-10 21:55:37,809 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-10 21:55:37,809 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-10 21:55:37,809 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-10 21:55:37,809 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-10 21:55:37,810 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-10 21:55:37,811 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-10 21:55:37,811 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-10 21:55:37,812 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-10 21:55:37,812 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-10 21:55:37,813 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-10 21:55:37,813 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-10 21:55:37,814 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-10 21:55:37,814 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-10 21:55:37,818 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-10 21:55:37,818 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-10 21:55:37,818 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-10 21:55:37,828 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/name/in_use.lock acquired by nodename 26266@ubuntu
2015-12-10 21:55:37,946 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/parallels/hadoop/tmp/dfs/name/current
2015-12-10 21:55:38,081 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000000118 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000118-0000000000000000403
2015-12-10 21:55:38,119 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 20 INodes.
2015-12-10 21:55:38,156 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-10 21:55:38,156 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 117 from /home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000117
2015-12-10 21:55:38,156 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@53ccbdb5 expecting start txid #118
2015-12-10 21:55:38,157 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000118-0000000000000000403
2015-12-10 21:55:38,158 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000118-0000000000000000403' to transaction ID 118
2015-12-10 21:55:38,227 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000118-0000000000000000403 of size 1048576 edits # 286 loaded in 0 seconds
2015-12-10 21:55:38,228 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2015-12-10 21:55:38,228 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2015-12-10 21:55:38,283 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 117
2015-12-10 21:55:38,283 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000115, cpktTxId=0000000000000000115)
2015-12-10 21:55:38,291 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 404
2015-12-10 21:55:38,330 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-10 21:55:38,330 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 512 msecs
2015-12-10 21:55:38,489 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-10 21:55:38,494 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-10 21:55:38,504 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-10 21:55:38,541 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-10 21:55:38,551 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-10 21:55:38,551 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-10 21:55:38,552 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 16 blocks to reach the threshold 0.9990 of total blocks 16.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2015-12-10 21:55:38,587 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-10 21:55:38,588 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-10 21:55:38,602 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-10 21:55:38,602 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-10 21:55:38,607 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-10 21:55:38,607 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 37484477 milliseconds
2015-12-10 21:55:38,609 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 21:55:42,653 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0) storage 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-10 21:55:42,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-10 21:55:42,661 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-10 21:55:42,811 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-10 21:55:42,811 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-75e73aff-3510-410a-91be-881360516f94 for DN 127.0.0.1:50010
2015-12-10 21:55:42,872 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 15 has reached the threshold 0.9990 of total blocks 16. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2015-12-10 21:55:42,872 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-10 21:55:42,873 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-75e73aff-3510-410a-91be-881360516f94,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-12-10 21:55:42,874 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 16, hasStaleStorages: false, processing time: 7 msecs
2015-12-10 21:55:42,881 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 16
2015-12-10 21:55:42,881 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-10 21:55:42,881 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-12-10 21:55:42,881 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-10 21:55:42,881 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-10 21:55:42,881 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 2 msec
2015-12-10 21:56:02,886 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 16 has reached the threshold 0.9990 of total blocks 16. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2015-12-10 21:56:08,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 21:56:08,609 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 21:56:12,891 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 35 secs
2015-12-10 21:56:12,892 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2015-12-10 21:56:12,892 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2015-12-10 21:56:12,892 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-12-10 21:56:28,632 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741870_1046 127.0.0.1:50010 
2015-12-10 21:56:29,564 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741870_1046]
2015-12-10 21:56:38,609 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 21:56:38,609 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 21:56:45,756 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 16 
2015-12-10 21:56:45,992 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0001/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073741874_1050{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 21:56:46,344 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741874_1050{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-10 21:56:46,344 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2015-12-10 21:56:46,364 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741874_1050{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 4272
2015-12-10 21:56:46,761 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0001/job.jar is closed by DFSClient_NONMAPREDUCE_2131924496_1
2015-12-10 21:56:46,770 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0001/job.jar
2015-12-10 21:56:46,865 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0001/job.split
2015-12-10 21:56:46,879 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0001/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073741875_1051{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 21:56:46,894 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741875_1051{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 21:56:46,901 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0001/job.split is closed by DFSClient_NONMAPREDUCE_2131924496_1
2015-12-10 21:56:46,911 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0001/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073741876_1052{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 21:56:46,918 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741876_1052{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 21:56:46,921 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_2131924496_1
2015-12-10 21:56:47,199 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0001/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741877_1053{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 21:56:47,237 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741877_1053{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 21:56:47,246 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0001/job.xml is closed by DFSClient_NONMAPREDUCE_2131924496_1
2015-12-10 21:56:48,311 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-10 21:56:48,311 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-10 21:56:48,311 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 404
2015-12-10 21:56:48,329 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 31 Total time for transactions(ms): 14 Number of transactions batched in Syncs: 0 Number of syncs: 25 SyncTimes(ms): 137 
2015-12-10 21:56:48,332 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000000404 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000404-0000000000000000434
2015-12-10 21:56:48,338 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 435
2015-12-10 21:56:50,439 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 333.33 KB/s
2015-12-10 21:56:50,439 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000434 size 3244 bytes.
2015-12-10 21:56:50,448 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 403
2015-12-10 21:56:50,449 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000117, cpktTxId=0000000000000000117)
2015-12-10 21:56:59,328 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0001/job_1449802555556_0001_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741878_1054{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 21:56:59,564 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741878_1054{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 21:56:59,574 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0001/job_1449802555556_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_474703413_1
2015-12-10 21:57:08,644 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30035 milliseconds
2015-12-10 21:57:08,645 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 21:57:15,289 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0001/job_1449802555556_0001_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073741879_1055{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 21:57:15,563 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0001/job_1449802555556_0001_1.jhist for DFSClient_NONMAPREDUCE_474703413_1
2015-12-10 21:57:38,650 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30006 milliseconds
2015-12-10 21:57:38,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 5 millisecond(s).
2015-12-10 21:57:56,020 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 18 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 0 Number of syncs: 9 SyncTimes(ms): 125 
2015-12-10 21:57:56,168 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741879_1055{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13843
2015-12-10 21:57:56,214 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0001/job_1449802555556_0001_1.jhist is closed by DFSClient_NONMAPREDUCE_474703413_1
2015-12-10 21:57:56,295 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0001.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741880_1056{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 21:57:56,328 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741880_1056{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 21:57:56,351 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0001.summary_tmp is closed by DFSClient_NONMAPREDUCE_474703413_1
2015-12-10 21:57:56,554 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0001-1449802607565-parallels-chess+data-1449802675965-0-0-FAILED-default-1449802618801.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741881_1057{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 21:57:56,645 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741881_1057{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 21:57:56,692 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0001-1449802607565-parallels-chess+data-1449802675965-0-0-FAILED-default-1449802618801.jhist_tmp is closed by DFSClient_NONMAPREDUCE_474703413_1
2015-12-10 21:57:56,782 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0001_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741882_1058{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 21:57:56,819 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741882_1058{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 21:57:56,835 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0001_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_474703413_1
2015-12-10 21:57:58,734 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741874_1050 127.0.0.1:50010 
2015-12-10 21:57:58,737 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741875_1051 127.0.0.1:50010 
2015-12-10 21:57:58,737 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741876_1052 127.0.0.1:50010 
2015-12-10 21:57:58,737 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741877_1053 127.0.0.1:50010 
2015-12-10 21:57:58,737 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741879_1055 127.0.0.1:50010 
2015-12-10 21:57:58,737 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741878_1054 127.0.0.1:50010 
2015-12-10 21:57:59,604 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741874_1050, blk_1073741875_1051, blk_1073741876_1052, blk_1073741877_1053, blk_1073741878_1054, blk_1073741879_1055]
2015-12-10 21:58:08,653 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-10 21:58:08,654 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 21:58:38,653 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 21:58:38,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 21:59:08,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-10 21:59:08,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 21:59:38,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 21:59:38,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:00:08,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:00:08,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-10 22:00:38,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:00:38,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:01:08,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:01:08,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:01:38,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:01:38,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:02:08,659 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-10 22:02:08,659 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:02:38,661 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:02:38,661 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:03:08,661 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:03:08,661 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:03:38,661 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:03:38,662 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:04:08,662 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:04:08,663 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:04:38,662 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:04:38,663 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:19:39,587 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* removeDeadDatanode: lost heartbeat from 127.0.0.1:50010
2015-12-10 22:19:39,614 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/127.0.0.1:50010
2015-12-10 22:19:39,901 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0) storage 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-10 22:19:39,902 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/127.0.0.1:50010
2015-12-10 22:19:39,902 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-10 22:19:39,902 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-10 22:19:39,902 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: [DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010 failed.
2015-12-10 22:19:39,907 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-10 22:19:39,919 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 18, hasStaleStorages: false, processing time: 0 msecs
2015-12-10 22:19:59,636 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:19:59,637 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-10 22:20:29,636 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:20:29,637 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:20:59,638 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:20:59,638 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:21:29,638 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:21:29,639 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:21:59,639 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:21:59,640 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:22:29,641 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:22:29,641 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:22:59,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:22:59,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:23:29,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:23:29,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:23:59,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:23:59,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:24:29,643 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:24:29,644 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:24:59,643 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:24:59,643 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:25:29,643 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:25:29,643 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:25:59,644 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:25:59,644 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:26:29,644 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:26:29,645 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:26:59,645 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:26:59,646 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:27:29,645 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:27:29,646 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:27:59,646 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:27:59,647 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:28:29,647 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:28:29,647 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:28:59,647 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:28:59,648 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:29:29,647 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:29:29,648 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:29:59,648 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:29:59,648 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:30:29,649 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:30:29,649 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:30:59,650 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:30:59,650 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:31:29,650 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:31:29,650 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:31:59,650 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:31:59,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:32:29,650 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:32:29,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:32:59,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:32:59,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:33:29,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:33:29,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:33:59,650 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:33:59,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:34:29,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:34:29,652 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:34:59,653 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:34:59,653 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:35:29,654 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:35:29,654 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:35:59,654 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:35:59,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:36:29,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:36:29,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:36:59,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:36:59,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:37:29,654 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:37:29,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:37:36,673 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 42 Total time for transactions(ms): 23 Number of transactions batched in Syncs: 0 Number of syncs: 27 SyncTimes(ms): 328 
2015-12-10 22:37:59,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:37:59,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:38:19,278 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0002/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073741883_1059{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:38:19,439 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741883_1059{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:38:19,444 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0002/job.jar is closed by DFSClient_NONMAPREDUCE_-205624779_1
2015-12-10 22:38:19,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0002/job.jar
2015-12-10 22:38:19,540 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0002/job.split
2015-12-10 22:38:19,557 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0002/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073741884_1060{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:38:19,569 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741884_1060{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:38:19,572 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0002/job.split is closed by DFSClient_NONMAPREDUCE_-205624779_1
2015-12-10 22:38:19,584 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0002/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073741885_1061{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:38:19,591 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741885_1061{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:38:19,594 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0002/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-205624779_1
2015-12-10 22:38:19,848 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0002/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741886_1062{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:38:19,862 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741886_1062{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:38:19,865 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0002/job.xml is closed by DFSClient_NONMAPREDUCE_-205624779_1
2015-12-10 22:38:27,375 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0002/job_1449802555556_0002_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741887_1063{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:38:27,503 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741887_1063{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:38:27,509 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0002/job_1449802555556_0002_1_conf.xml is closed by DFSClient_NONMAPREDUCE_73091126_1
2015-12-10 22:38:29,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:38:29,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:38:40,803 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0002/job_1449802555556_0002_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073741888_1064{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:38:40,804 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 82 Total time for transactions(ms): 28 Number of transactions batched in Syncs: 0 Number of syncs: 53 SyncTimes(ms): 353 
2015-12-10 22:38:41,274 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0002/job_1449802555556_0002_1.jhist for DFSClient_NONMAPREDUCE_73091126_1
2015-12-10 22:38:59,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:38:59,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:39:05,431 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741888_1064{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13449
2015-12-10 22:39:05,439 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0002/job_1449802555556_0002_1.jhist is closed by DFSClient_NONMAPREDUCE_73091126_1
2015-12-10 22:39:05,458 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0002.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741889_1065{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:39:05,475 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741889_1065{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-10 22:39:05,476 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741889_1065{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 338
2015-12-10 22:39:05,883 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0002.summary_tmp is closed by DFSClient_NONMAPREDUCE_73091126_1
2015-12-10 22:39:05,946 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0002-1449805100145-parallels-chess+data-1449805145244-0-0-FAILED-default-1449805106871.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741890_1066{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:39:05,964 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741890_1066{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:39:05,975 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0002-1449805100145-parallels-chess+data-1449805145244-0-0-FAILED-default-1449805106871.jhist_tmp is closed by DFSClient_NONMAPREDUCE_73091126_1
2015-12-10 22:39:06,004 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0002_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741891_1067{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:39:06,018 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741891_1067{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:39:06,021 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0002_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_73091126_1
2015-12-10 22:39:07,106 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741883_1059 127.0.0.1:50010 
2015-12-10 22:39:07,108 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741884_1060 127.0.0.1:50010 
2015-12-10 22:39:07,109 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741885_1061 127.0.0.1:50010 
2015-12-10 22:39:07,109 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741886_1062 127.0.0.1:50010 
2015-12-10 22:39:07,109 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741888_1064 127.0.0.1:50010 
2015-12-10 22:39:07,109 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741887_1063 127.0.0.1:50010 
2015-12-10 22:39:08,902 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741888_1064, blk_1073741883_1059, blk_1073741884_1060, blk_1073741885_1061, blk_1073741886_1062, blk_1073741887_1063]
2015-12-10 22:39:29,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:39:29,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:39:59,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:39:59,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:40:29,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:40:29,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:40:59,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:40:59,658 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:41:29,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:41:29,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:41:59,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:41:59,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:42:29,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:42:29,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:42:59,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:42:59,658 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:43:29,658 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:43:29,659 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:43:59,659 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:43:59,660 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:44:29,659 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:44:29,659 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:44:59,659 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:44:59,659 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:45:29,658 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:45:29,659 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:45:59,659 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:45:59,660 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:46:29,659 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:46:29,660 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:46:59,661 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:46:59,661 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:47:29,661 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:47:29,661 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:47:59,661 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:47:59,662 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:48:29,662 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:48:29,663 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:48:59,664 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:48:59,664 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:49:29,665 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:49:29,665 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:49:59,665 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:49:59,667 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-10 22:50:24,265 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 108 Total time for transactions(ms): 38 Number of transactions batched in Syncs: 1 Number of syncs: 73 SyncTimes(ms): 697 
2015-12-10 22:50:29,666 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:50:29,666 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:50:35,643 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0003/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073741892_1068{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:50:35,781 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741892_1068{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:50:35,790 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0003/job.jar is closed by DFSClient_NONMAPREDUCE_-1365974731_1
2015-12-10 22:50:35,800 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0003/job.jar
2015-12-10 22:50:35,874 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0003/job.split
2015-12-10 22:50:35,888 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0003/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073741893_1069{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:50:35,900 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741893_1069{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:50:35,903 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0003/job.split is closed by DFSClient_NONMAPREDUCE_-1365974731_1
2015-12-10 22:50:35,920 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0003/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073741894_1070{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:50:35,931 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741894_1070{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:50:35,933 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0003/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1365974731_1
2015-12-10 22:50:36,152 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0003/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741895_1071{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:50:36,167 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741895_1071{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:50:36,171 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0003/job.xml is closed by DFSClient_NONMAPREDUCE_-1365974731_1
2015-12-10 22:50:42,117 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0003/job_1449802555556_0003_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741896_1072{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:50:42,248 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741896_1072{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:50:42,255 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0003/job_1449802555556_0003_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-427384879_1
2015-12-10 22:50:55,259 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0003/job_1449802555556_0003_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073741897_1073{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:50:55,409 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0003/job_1449802555556_0003_1.jhist for DFSClient_NONMAPREDUCE_-427384879_1
2015-12-10 22:50:59,673 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30008 milliseconds
2015-12-10 22:50:59,674 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:51:29,712 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30038 milliseconds
2015-12-10 22:51:29,717 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 5 millisecond(s).
2015-12-10 22:51:31,865 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 150 Total time for transactions(ms): 48 Number of transactions batched in Syncs: 1 Number of syncs: 101 SyncTimes(ms): 793 
2015-12-10 22:51:32,276 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741897_1073{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-10 22:51:32,277 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741897_1073{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 44640
2015-12-10 22:51:32,685 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0003/job_1449802555556_0003_1.jhist is closed by DFSClient_NONMAPREDUCE_-427384879_1
2015-12-10 22:51:32,727 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0003.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741898_1074{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:51:32,753 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741898_1074{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:51:32,770 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0003.summary_tmp is closed by DFSClient_NONMAPREDUCE_-427384879_1
2015-12-10 22:51:32,868 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0003-1449805836441-parallels-chess+data-1449805892078-0-0-FAILED-default-1449805841623.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741899_1075{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:51:32,890 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741899_1075{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-10 22:51:32,901 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741899_1075{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 44640
2015-12-10 22:51:33,296 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0003-1449805836441-parallels-chess+data-1449805892078-0-0-FAILED-default-1449805841623.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-427384879_1
2015-12-10 22:51:33,327 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0003_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741900_1076{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:51:33,347 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741900_1076{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:51:33,364 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0003_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-427384879_1
2015-12-10 22:51:34,463 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741892_1068 127.0.0.1:50010 
2015-12-10 22:51:34,464 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741893_1069 127.0.0.1:50010 
2015-12-10 22:51:34,465 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741894_1070 127.0.0.1:50010 
2015-12-10 22:51:34,465 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741895_1071 127.0.0.1:50010 
2015-12-10 22:51:34,465 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741897_1073 127.0.0.1:50010 
2015-12-10 22:51:34,466 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741896_1072 127.0.0.1:50010 
2015-12-10 22:51:36,292 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741892_1068, blk_1073741893_1069, blk_1073741894_1070, blk_1073741895_1071, blk_1073741896_1072, blk_1073741897_1073]
2015-12-10 22:51:59,713 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:51:59,714 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:52:29,713 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:52:29,715 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-10 22:52:59,714 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:52:59,715 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:53:29,714 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:53:29,715 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:53:59,715 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-10 22:53:59,716 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:54:29,717 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:54:29,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-10 22:54:59,718 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:54:59,718 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:55:29,718 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:55:29,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:55:59,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:55:59,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:56:18,831 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 174 Total time for transactions(ms): 52 Number of transactions batched in Syncs: 3 Number of syncs: 119 SyncTimes(ms): 910 
2015-12-10 22:56:18,944 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0004/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073741901_1077{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:56:19,080 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741901_1077{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:56:19,085 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0004/job.jar is closed by DFSClient_NONMAPREDUCE_-1798564717_1
2015-12-10 22:56:19,091 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0004/job.jar
2015-12-10 22:56:19,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0004/job.split
2015-12-10 22:56:19,174 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0004/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073741902_1078{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:56:19,187 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741902_1078{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:56:19,190 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0004/job.split is closed by DFSClient_NONMAPREDUCE_-1798564717_1
2015-12-10 22:56:19,206 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0004/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073741903_1079{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:56:19,214 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741903_1079{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:56:19,218 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0004/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1798564717_1
2015-12-10 22:56:19,471 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0004/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741904_1080{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:56:19,506 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741904_1080{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:56:19,509 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0004/job.xml is closed by DFSClient_NONMAPREDUCE_-1798564717_1
2015-12-10 22:56:25,449 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0004/job_1449802555556_0004_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741905_1081{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:56:25,646 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741905_1081{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:56:25,654 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0004/job_1449802555556_0004_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1672126040_1
2015-12-10 22:56:29,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:56:29,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:56:40,069 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0004/job_1449802555556_0004_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073741906_1082{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:56:40,268 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0004/job_1449802555556_0004_1.jhist for DFSClient_NONMAPREDUCE_-1672126040_1
2015-12-10 22:56:51,310 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0004_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073741907_1083{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:56:51,470 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741907_1083{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:56:51,478 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0004_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449802555556_0004_r_000000_0_1093380799_1
2015-12-10 22:56:51,769 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0004/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1672126040_1
2015-12-10 22:56:51,815 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1672126040_1
2015-12-10 22:56:51,824 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0004/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1672126040_1
2015-12-10 22:56:51,859 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741906_1082{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12220
2015-12-10 22:56:51,862 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0004/job_1449802555556_0004_1.jhist is closed by DFSClient_NONMAPREDUCE_-1672126040_1
2015-12-10 22:56:51,871 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0004.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741908_1084{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:56:51,878 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741908_1084{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:56:51,882 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0004.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1672126040_1
2015-12-10 22:56:51,937 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0004-1449806179742-parallels-chess+data-1449806211826-3-1-SUCCEEDED-default-1449806184988.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741909_1085{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:56:51,949 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741909_1085{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:56:51,952 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0004-1449806179742-parallels-chess+data-1449806211826-3-1-SUCCEEDED-default-1449806184988.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1672126040_1
2015-12-10 22:56:51,976 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0004_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741910_1086{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 22:56:51,995 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741910_1086{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 22:56:52,006 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0004_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1672126040_1
2015-12-10 22:56:53,063 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741901_1077 127.0.0.1:50010 
2015-12-10 22:56:53,063 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741902_1078 127.0.0.1:50010 
2015-12-10 22:56:53,064 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741903_1079 127.0.0.1:50010 
2015-12-10 22:56:53,064 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741904_1080 127.0.0.1:50010 
2015-12-10 22:56:53,064 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741906_1082 127.0.0.1:50010 
2015-12-10 22:56:53,064 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741905_1081 127.0.0.1:50010 
2015-12-10 22:56:54,368 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741904_1080, blk_1073741905_1081, blk_1073741906_1082, blk_1073741901_1077, blk_1073741902_1078, blk_1073741903_1079]
2015-12-10 22:56:59,728 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30009 milliseconds
2015-12-10 22:56:59,728 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:57:29,728 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:57:29,728 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:57:59,729 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:57:59,729 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:58:29,729 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:58:29,729 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:58:59,729 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:58:59,729 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 22:59:29,729 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 22:59:29,729 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 22:59:59,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 22:59:59,731 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:00:29,731 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:00:29,731 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:00:59,731 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:00:59,731 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:01:29,731 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:01:29,731 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:01:59,731 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:01:59,731 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:02:29,732 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:02:29,732 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:02:59,733 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:02:59,733 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:03:29,733 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:03:29,733 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:03:48,677 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 254 Total time for transactions(ms): 56 Number of transactions batched in Syncs: 3 Number of syncs: 175 SyncTimes(ms): 979 
2015-12-10 23:03:48,682 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741907_1083 127.0.0.1:50010 
2015-12-10 23:03:51,454 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741907_1083]
2015-12-10 23:03:54,741 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0005/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073741911_1087{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:03:54,878 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741911_1087{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:03:54,889 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0005/job.jar is closed by DFSClient_NONMAPREDUCE_275638596_1
2015-12-10 23:03:54,894 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0005/job.jar
2015-12-10 23:03:54,960 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0005/job.split
2015-12-10 23:03:54,975 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0005/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073741912_1088{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:03:54,987 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741912_1088{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:03:54,994 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0005/job.split is closed by DFSClient_NONMAPREDUCE_275638596_1
2015-12-10 23:03:55,011 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0005/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073741913_1089{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:03:55,018 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741913_1089{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:03:55,021 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0005/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_275638596_1
2015-12-10 23:03:55,204 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0005/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741914_1090{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:03:55,229 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741914_1090{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:03:55,234 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0005/job.xml is closed by DFSClient_NONMAPREDUCE_275638596_1
2015-12-10 23:03:59,733 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:03:59,733 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:04:00,450 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0005/job_1449802555556_0005_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741915_1091{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:04:00,542 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741915_1091{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:04:00,550 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0005/job_1449802555556_0005_1_conf.xml is closed by DFSClient_NONMAPREDUCE_27097967_1
2015-12-10 23:04:14,024 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0005/job_1449802555556_0005_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073741916_1092{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:04:14,195 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0005/job_1449802555556_0005_1.jhist for DFSClient_NONMAPREDUCE_27097967_1
2015-12-10 23:04:25,094 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0005_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073741917_1093{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:04:25,349 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741917_1093{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:04:25,357 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0005_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449802555556_0005_r_000000_0_-467405944_1
2015-12-10 23:04:25,597 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0005/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_27097967_1
2015-12-10 23:04:25,651 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_27097967_1
2015-12-10 23:04:25,659 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0005/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_27097967_1
2015-12-10 23:04:25,722 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741916_1092{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14410
2015-12-10 23:04:25,725 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0005/job_1449802555556_0005_1.jhist is closed by DFSClient_NONMAPREDUCE_27097967_1
2015-12-10 23:04:25,750 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0005.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741918_1094{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:04:25,775 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741918_1094{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:04:25,779 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0005.summary_tmp is closed by DFSClient_NONMAPREDUCE_27097967_1
2015-12-10 23:04:25,851 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0005-1449806635463-parallels-chess+data-1449806665661-3-1-SUCCEEDED-default-1449806640111.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741919_1095{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:04:25,860 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741919_1095{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:04:25,863 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0005-1449806635463-parallels-chess+data-1449806665661-3-1-SUCCEEDED-default-1449806640111.jhist_tmp is closed by DFSClient_NONMAPREDUCE_27097967_1
2015-12-10 23:04:25,890 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0005_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741920_1096{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:04:25,901 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741920_1096{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:04:25,903 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0005_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_27097967_1
2015-12-10 23:04:26,973 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741911_1087 127.0.0.1:50010 
2015-12-10 23:04:26,973 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741912_1088 127.0.0.1:50010 
2015-12-10 23:04:26,973 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741913_1089 127.0.0.1:50010 
2015-12-10 23:04:26,973 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741914_1090 127.0.0.1:50010 
2015-12-10 23:04:26,973 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741916_1092 127.0.0.1:50010 
2015-12-10 23:04:26,974 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741915_1091 127.0.0.1:50010 
2015-12-10 23:04:27,460 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741911_1087, blk_1073741912_1088, blk_1073741913_1089, blk_1073741914_1090, blk_1073741915_1091, blk_1073741916_1092]
2015-12-10 23:04:29,735 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-10 23:04:29,735 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:04:59,735 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:04:59,735 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:05:29,735 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:05:29,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:05:59,735 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:05:59,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:06:29,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:06:29,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:06:59,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:06:59,738 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-10 23:07:29,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:07:29,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:07:59,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:07:59,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:08:29,738 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:08:29,738 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:08:59,738 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:08:59,739 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:09:29,740 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:09:29,740 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:09:59,740 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:09:59,740 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:10:01,851 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 338 Total time for transactions(ms): 72 Number of transactions batched in Syncs: 3 Number of syncs: 232 SyncTimes(ms): 1105 
2015-12-10 23:10:01,853 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741917_1093 127.0.0.1:50010 
2015-12-10 23:10:03,542 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741917_1093]
2015-12-10 23:10:07,509 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0006/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073741921_1097{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:10:07,628 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741921_1097{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:10:07,633 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0006/job.jar is closed by DFSClient_NONMAPREDUCE_-632804457_1
2015-12-10 23:10:07,639 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0006/job.jar
2015-12-10 23:10:07,711 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0006/job.split
2015-12-10 23:10:07,727 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0006/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073741922_1098{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:10:07,739 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741922_1098{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:10:07,743 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0006/job.split is closed by DFSClient_NONMAPREDUCE_-632804457_1
2015-12-10 23:10:07,759 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0006/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073741923_1099{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:10:07,769 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741923_1099{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:10:07,773 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0006/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-632804457_1
2015-12-10 23:10:08,017 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0006/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741924_1100{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:10:08,034 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741924_1100{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:10:08,036 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0006/job.xml is closed by DFSClient_NONMAPREDUCE_-632804457_1
2015-12-10 23:10:13,243 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0006/job_1449802555556_0006_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741925_1101{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:10:13,366 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741925_1101{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:10:13,368 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0006/job_1449802555556_0006_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1536706009_1
2015-12-10 23:10:28,684 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0006/job_1449802555556_0006_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073741926_1102{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:10:29,141 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0006/job_1449802555556_0006_1.jhist for DFSClient_NONMAPREDUCE_1536706009_1
2015-12-10 23:10:29,741 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:10:29,813 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 73 millisecond(s).
2015-12-10 23:10:39,117 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0006_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073741927_1103{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:10:39,356 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741927_1103{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:10:39,368 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0006_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449802555556_0006_r_000000_0_1562035676_1
2015-12-10 23:10:40,614 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0006/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_1536706009_1
2015-12-10 23:10:40,686 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_1536706009_1
2015-12-10 23:10:40,701 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0006/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_1536706009_1
2015-12-10 23:10:40,732 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741926_1102{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12232
2015-12-10 23:10:40,734 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0006/job_1449802555556_0006_1.jhist is closed by DFSClient_NONMAPREDUCE_1536706009_1
2015-12-10 23:10:40,746 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0006.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741928_1104{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:10:40,757 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741928_1104{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:10:40,759 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0006.summary_tmp is closed by DFSClient_NONMAPREDUCE_1536706009_1
2015-12-10 23:10:40,834 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0006-1449807008275-parallels-chess+data-1449807040703-3-1-SUCCEEDED-default-1449807012928.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741929_1105{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:10:40,848 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741929_1105{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:10:40,852 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0006-1449807008275-parallels-chess+data-1449807040703-3-1-SUCCEEDED-default-1449807012928.jhist_tmp is closed by DFSClient_NONMAPREDUCE_1536706009_1
2015-12-10 23:10:40,880 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0006_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741930_1106{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:10:40,895 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741930_1106{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:10:40,898 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0006_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_1536706009_1
2015-12-10 23:10:41,953 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741921_1097 127.0.0.1:50010 
2015-12-10 23:10:41,953 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741922_1098 127.0.0.1:50010 
2015-12-10 23:10:41,953 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741923_1099 127.0.0.1:50010 
2015-12-10 23:10:41,954 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741924_1100 127.0.0.1:50010 
2015-12-10 23:10:41,954 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741926_1102 127.0.0.1:50010 
2015-12-10 23:10:41,954 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741925_1101 127.0.0.1:50010 
2015-12-10 23:10:42,677 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741921_1097, blk_1073741922_1098, blk_1073741923_1099, blk_1073741924_1100, blk_1073741925_1101, blk_1073741926_1102]
2015-12-10 23:10:59,741 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:10:59,741 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:11:29,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30033 milliseconds
2015-12-10 23:11:29,774 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:11:43,898 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-10 23:11:43,898 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-10 23:11:43,898 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 435
2015-12-10 23:11:43,898 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 419 Total time for transactions(ms): 105 Number of transactions batched in Syncs: 3 Number of syncs: 289 SyncTimes(ms): 1378 
2015-12-10 23:11:43,899 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 419 Total time for transactions(ms): 105 Number of transactions batched in Syncs: 3 Number of syncs: 290 SyncTimes(ms): 1379 
2015-12-10 23:11:43,901 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000000435 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000435-0000000000000000853
2015-12-10 23:11:43,902 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 854
2015-12-10 23:11:44,418 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 1666.67 KB/s
2015-12-10 23:11:44,423 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000853 size 5199 bytes.
2015-12-10 23:11:44,430 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 434
2015-12-10 23:11:44,430 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000403, cpktTxId=0000000000000000403)
2015-12-10 23:11:59,774 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:11:59,775 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:12:29,775 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:12:29,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:12:59,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:12:59,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:13:29,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:13:29,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:13:59,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:13:59,777 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:14:29,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:14:29,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:14:59,777 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:14:59,777 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:15:24,733 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 9 
2015-12-10 23:15:24,734 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741927_1103 127.0.0.1:50010 
2015-12-10 23:15:24,737 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741927_1103]
2015-12-10 23:15:29,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:15:29,777 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:15:31,155 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0007/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073741931_1107{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:15:31,281 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741931_1107{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:15:31,284 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0007/job.jar is closed by DFSClient_NONMAPREDUCE_-1464273389_1
2015-12-10 23:15:31,289 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0007/job.jar
2015-12-10 23:15:31,363 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0007/job.split
2015-12-10 23:15:31,376 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0007/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073741932_1108{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:15:31,388 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741932_1108{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:15:31,391 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0007/job.split is closed by DFSClient_NONMAPREDUCE_-1464273389_1
2015-12-10 23:15:31,406 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0007/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073741933_1109{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:15:31,413 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741933_1109{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:15:31,416 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0007/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1464273389_1
2015-12-10 23:15:31,658 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0007/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741934_1110{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:15:31,675 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741934_1110{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:15:31,678 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0007/job.xml is closed by DFSClient_NONMAPREDUCE_-1464273389_1
2015-12-10 23:15:37,539 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0007/job_1449802555556_0007_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741935_1111{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:15:37,669 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741935_1111{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:15:37,674 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0007/job_1449802555556_0007_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1000575728_1
2015-12-10 23:15:53,378 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0007/job_1449802555556_0007_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073741936_1112{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:15:53,726 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0007/job_1449802555556_0007_1.jhist for DFSClient_NONMAPREDUCE_-1000575728_1
2015-12-10 23:15:59,787 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30010 milliseconds
2015-12-10 23:15:59,892 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 105 millisecond(s).
2015-12-10 23:16:04,000 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0007_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073741937_1113{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:16:04,323 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741937_1113{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:16:04,334 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0007_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449802555556_0007_r_000000_0_-1999329940_1
2015-12-10 23:16:05,660 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0007/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1000575728_1
2015-12-10 23:16:05,712 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1000575728_1
2015-12-10 23:16:05,724 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0007/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1000575728_1
2015-12-10 23:16:05,768 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741936_1112{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-10 23:16:05,782 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741936_1112{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 49818
2015-12-10 23:16:06,174 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0007/job_1449802555556_0007_1.jhist is closed by DFSClient_NONMAPREDUCE_-1000575728_1
2015-12-10 23:16:06,193 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0007.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741938_1114{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:16:06,200 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741938_1114{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:16:06,206 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0007.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1000575728_1
2015-12-10 23:16:06,270 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0007-1449807331915-parallels-chess+data-1449807365726-3-1-SUCCEEDED-default-1449807337095.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741939_1115{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:16:06,286 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741939_1115{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:16:06,289 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0007-1449807331915-parallels-chess+data-1449807365726-3-1-SUCCEEDED-default-1449807337095.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1000575728_1
2015-12-10 23:16:06,328 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0007_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741940_1116{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:16:06,339 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741940_1116{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:16:06,342 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0007_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1000575728_1
2015-12-10 23:16:07,398 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741931_1107 127.0.0.1:50010 
2015-12-10 23:16:07,399 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741932_1108 127.0.0.1:50010 
2015-12-10 23:16:07,399 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741933_1109 127.0.0.1:50010 
2015-12-10 23:16:07,399 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741934_1110 127.0.0.1:50010 
2015-12-10 23:16:07,399 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741936_1112 127.0.0.1:50010 
2015-12-10 23:16:07,399 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741935_1111 127.0.0.1:50010 
2015-12-10 23:16:09,756 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741936_1112, blk_1073741931_1107, blk_1073741932_1108, blk_1073741933_1109, blk_1073741934_1110, blk_1073741935_1111]
2015-12-10 23:16:29,787 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:16:29,788 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:16:59,788 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:16:59,788 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-10 23:17:29,788 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:17:29,790 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-10 23:17:59,789 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-10 23:17:59,790 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:18:29,790 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:18:29,791 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:18:59,791 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:18:59,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:19:29,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:19:29,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:19:59,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:19:59,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:20:29,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:20:29,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:20:59,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:20:59,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:21:29,794 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-10 23:21:29,794 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:21:59,793 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:21:59,794 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:22:25,223 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 83 Total time for transactions(ms): 11 Number of transactions batched in Syncs: 1 Number of syncs: 59 SyncTimes(ms): 405 
2015-12-10 23:22:25,224 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741937_1113 127.0.0.1:50010 
2015-12-10 23:22:27,845 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741937_1113]
2015-12-10 23:22:29,794 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:22:29,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:22:31,707 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0008/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073741941_1117{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:22:31,850 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741941_1117{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:22:31,853 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0008/job.jar is closed by DFSClient_NONMAPREDUCE_1930240578_1
2015-12-10 23:22:31,859 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0008/job.jar
2015-12-10 23:22:31,939 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0008/job.split
2015-12-10 23:22:31,953 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0008/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073741942_1118{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:22:31,961 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741942_1118{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:22:31,969 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0008/job.split is closed by DFSClient_NONMAPREDUCE_1930240578_1
2015-12-10 23:22:31,985 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0008/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073741943_1119{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:22:31,992 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741943_1119{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:22:31,997 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0008/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1930240578_1
2015-12-10 23:22:32,251 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0008/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741944_1120{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:22:32,266 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741944_1120{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:22:32,274 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0008/job.xml is closed by DFSClient_NONMAPREDUCE_1930240578_1
2015-12-10 23:22:38,266 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0008/job_1449802555556_0008_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741945_1121{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:22:38,384 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741945_1121{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:22:38,391 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0008/job_1449802555556_0008_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1068966971_1
2015-12-10 23:22:54,986 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0008/job_1449802555556_0008_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073741946_1122{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:22:55,357 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0008/job_1449802555556_0008_1.jhist for DFSClient_NONMAPREDUCE_1068966971_1
2015-12-10 23:22:59,795 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:22:59,808 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 14 millisecond(s).
2015-12-10 23:23:06,367 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0008_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073741947_1123{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:23:06,632 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741947_1123{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:23:06,637 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0008_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449802555556_0008_r_000000_0_674795283_1
2015-12-10 23:23:07,990 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0008/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_1068966971_1
2015-12-10 23:23:08,046 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_1068966971_1
2015-12-10 23:23:08,054 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0008/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_1068966971_1
2015-12-10 23:23:08,085 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741946_1122{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12232
2015-12-10 23:23:08,090 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0008/job_1449802555556_0008_1.jhist is closed by DFSClient_NONMAPREDUCE_1068966971_1
2015-12-10 23:23:08,102 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0008.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741948_1124{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:23:08,110 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741948_1124{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:23:08,112 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0008.summary_tmp is closed by DFSClient_NONMAPREDUCE_1068966971_1
2015-12-10 23:23:08,173 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0008-1449807752486-parallels-chess+data-1449807788056-3-1-SUCCEEDED-default-1449807757645.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741949_1125{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:23:08,181 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741949_1125{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:23:08,185 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0008-1449807752486-parallels-chess+data-1449807788056-3-1-SUCCEEDED-default-1449807757645.jhist_tmp is closed by DFSClient_NONMAPREDUCE_1068966971_1
2015-12-10 23:23:08,225 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0008_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741950_1126{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:23:08,235 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741950_1126{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:23:08,239 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0008_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_1068966971_1
2015-12-10 23:23:09,303 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741941_1117 127.0.0.1:50010 
2015-12-10 23:23:09,304 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741942_1118 127.0.0.1:50010 
2015-12-10 23:23:09,304 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741943_1119 127.0.0.1:50010 
2015-12-10 23:23:09,304 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741944_1120 127.0.0.1:50010 
2015-12-10 23:23:09,304 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741946_1122 127.0.0.1:50010 
2015-12-10 23:23:09,304 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741945_1121 127.0.0.1:50010 
2015-12-10 23:23:10,140 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741941_1117, blk_1073741942_1118, blk_1073741943_1119, blk_1073741944_1120, blk_1073741945_1121, blk_1073741946_1122]
2015-12-10 23:23:20,679 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741947_1123 127.0.0.1:50010 
2015-12-10 23:23:22,142 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741947_1123]
2015-12-10 23:23:29,795 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:23:29,795 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:23:59,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:23:59,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:24:29,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:24:29,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:24:59,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:24:59,797 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:25:29,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-10 23:25:29,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:25:57,850 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 165 Total time for transactions(ms): 30 Number of transactions batched in Syncs: 1 Number of syncs: 117 SyncTimes(ms): 486 
2015-12-10 23:25:57,956 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0009/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073741951_1127{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:25:58,099 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741951_1127{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:25:58,104 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0009/job.jar is closed by DFSClient_NONMAPREDUCE_1747542676_1
2015-12-10 23:25:58,110 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0009/job.jar
2015-12-10 23:25:58,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0009/job.split
2015-12-10 23:25:58,199 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0009/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073741952_1128{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:25:58,212 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741952_1128{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:25:58,214 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0009/job.split is closed by DFSClient_NONMAPREDUCE_1747542676_1
2015-12-10 23:25:58,231 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0009/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073741953_1129{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:25:58,239 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741953_1129{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:25:58,242 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0009/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1747542676_1
2015-12-10 23:25:58,497 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0009/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741954_1130{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:25:58,524 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741954_1130{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:25:58,527 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0009/job.xml is closed by DFSClient_NONMAPREDUCE_1747542676_1
2015-12-10 23:25:59,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:25:59,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:26:04,243 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0009/job_1449802555556_0009_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741955_1131{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:26:04,362 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741955_1131{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:26:04,367 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0009/job_1449802555556_0009_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1068241642_1
2015-12-10 23:26:18,052 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0009/job_1449802555556_0009_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073741956_1132{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:26:18,221 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0009/job_1449802555556_0009_1.jhist for DFSClient_NONMAPREDUCE_-1068241642_1
2015-12-10 23:26:27,630 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0009_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073741957_1133{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:26:27,876 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741957_1133{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:26:27,885 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0009_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449802555556_0009_r_000000_0_538013692_1
2015-12-10 23:26:29,197 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0009/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1068241642_1
2015-12-10 23:26:29,243 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1068241642_1
2015-12-10 23:26:29,251 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0009/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1068241642_1
2015-12-10 23:26:29,285 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741956_1132{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12220
2015-12-10 23:26:29,463 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0009/job_1449802555556_0009_1.jhist is closed by DFSClient_NONMAPREDUCE_-1068241642_1
2015-12-10 23:26:29,472 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0009.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741958_1134{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:26:29,484 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741958_1134{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:26:29,485 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0009.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1068241642_1
2015-12-10 23:26:29,534 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0009-1449807958768-parallels-chess+data-1449807989252-3-1-SUCCEEDED-default-1449807963871.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741959_1135{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:26:29,542 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741959_1135{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:26:29,552 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0009-1449807958768-parallels-chess+data-1449807989252-3-1-SUCCEEDED-default-1449807963871.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1068241642_1
2015-12-10 23:26:29,581 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0009_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741960_1136{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:26:29,590 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741960_1136{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:26:29,592 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0009_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1068241642_1
2015-12-10 23:26:29,804 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30005 milliseconds
2015-12-10 23:26:29,818 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 15 millisecond(s).
2015-12-10 23:26:30,654 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741951_1127 127.0.0.1:50010 
2015-12-10 23:26:30,655 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741952_1128 127.0.0.1:50010 
2015-12-10 23:26:30,655 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741953_1129 127.0.0.1:50010 
2015-12-10 23:26:30,655 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741954_1130 127.0.0.1:50010 
2015-12-10 23:26:30,655 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741956_1132 127.0.0.1:50010 
2015-12-10 23:26:30,655 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741955_1131 127.0.0.1:50010 
2015-12-10 23:26:31,193 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741952_1128, blk_1073741953_1129, blk_1073741954_1130, blk_1073741955_1131, blk_1073741956_1132, blk_1073741951_1127]
2015-12-10 23:26:59,804 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:26:59,804 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:27:29,804 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:27:29,804 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:27:59,805 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-10 23:27:59,806 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:28:29,806 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:28:29,806 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:28:59,806 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:28:59,806 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:29:29,805 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:29:29,806 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:29:59,807 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:29:59,808 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:30:01,862 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 43, hasStaleStorages: false, processing time: 2 msecs
2015-12-10 23:30:29,808 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:30:29,809 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:30:59,808 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:30:59,808 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:31:29,808 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:31:29,809 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:31:59,810 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-10 23:31:59,812 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-10 23:32:29,814 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30004 milliseconds
2015-12-10 23:32:29,815 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:32:59,816 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:32:59,816 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:33:18,813 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 245 Total time for transactions(ms): 41 Number of transactions batched in Syncs: 1 Number of syncs: 173 SyncTimes(ms): 601 
2015-12-10 23:33:18,818 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741957_1133 127.0.0.1:50010 
2015-12-10 23:33:19,282 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741957_1133]
2015-12-10 23:33:26,269 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0010/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073741961_1137{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:33:26,403 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741961_1137{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:33:26,419 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0010/job.jar is closed by DFSClient_NONMAPREDUCE_1669111964_1
2015-12-10 23:33:26,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0010/job.jar
2015-12-10 23:33:26,654 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0010/job.split
2015-12-10 23:33:26,671 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0010/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073741962_1138{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:33:26,686 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741962_1138{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:33:26,688 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0010/job.split is closed by DFSClient_NONMAPREDUCE_1669111964_1
2015-12-10 23:33:26,716 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0010/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073741963_1139{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:33:26,723 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741963_1139{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:33:26,733 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0010/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1669111964_1
2015-12-10 23:33:27,026 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0010/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741964_1140{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:33:27,060 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741964_1140{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:33:27,062 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0010/job.xml is closed by DFSClient_NONMAPREDUCE_1669111964_1
2015-12-10 23:33:29,817 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:33:29,817 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:33:32,874 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0010/job_1449802555556_0010_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741965_1141{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:33:32,980 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741965_1141{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:33:32,985 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0010/job_1449802555556_0010_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1899464295_1
2015-12-10 23:33:47,222 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0010/job_1449802555556_0010_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073741966_1142{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:33:47,401 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0010/job_1449802555556_0010_1.jhist for DFSClient_NONMAPREDUCE_-1899464295_1
2015-12-10 23:33:59,823 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30008 milliseconds
2015-12-10 23:33:59,918 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 95 millisecond(s).
2015-12-10 23:34:03,868 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0010_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073741967_1143{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:34:04,256 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741967_1143{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:34:04,268 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0010_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449802555556_0010_r_000000_0_361573347_1
2015-12-10 23:34:05,746 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0010/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1899464295_1
2015-12-10 23:34:05,795 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1899464295_1
2015-12-10 23:34:05,804 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0010/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1899464295_1
2015-12-10 23:34:05,837 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741966_1142{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12220
2015-12-10 23:34:05,841 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0010/job_1449802555556_0010_1.jhist is closed by DFSClient_NONMAPREDUCE_-1899464295_1
2015-12-10 23:34:05,855 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0010.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741968_1144{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:34:05,862 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741968_1144{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:34:05,864 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0010.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1899464295_1
2015-12-10 23:34:05,914 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0010-1449808407289-parallels-chess+data-1449808445806-3-1-SUCCEEDED-default-1449808412532.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741969_1145{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:34:05,936 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741969_1145{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:34:05,944 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0010-1449808407289-parallels-chess+data-1449808445806-3-1-SUCCEEDED-default-1449808412532.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1899464295_1
2015-12-10 23:34:05,968 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0010_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741970_1146{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:34:05,980 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741970_1146{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:34:05,987 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0010_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1899464295_1
2015-12-10 23:34:07,060 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741961_1137 127.0.0.1:50010 
2015-12-10 23:34:07,060 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741962_1138 127.0.0.1:50010 
2015-12-10 23:34:07,060 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741963_1139 127.0.0.1:50010 
2015-12-10 23:34:07,060 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741964_1140 127.0.0.1:50010 
2015-12-10 23:34:07,061 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741966_1142 127.0.0.1:50010 
2015-12-10 23:34:07,061 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741965_1141 127.0.0.1:50010 
2015-12-10 23:34:07,318 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741961_1137, blk_1073741962_1138, blk_1073741963_1139, blk_1073741964_1140, blk_1073741965_1141, blk_1073741966_1142]
2015-12-10 23:34:29,825 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:34:29,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-10 23:34:59,824 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:34:59,824 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:35:29,824 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:35:29,825 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:35:59,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:35:59,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:36:29,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:36:29,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:36:59,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:36:59,827 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:37:29,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:37:29,827 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:37:59,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:37:59,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:38:29,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:38:29,827 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:38:59,827 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:38:59,828 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:39:29,828 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:39:29,829 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:39:59,828 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:39:59,829 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:40:17,523 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 326 Total time for transactions(ms): 57 Number of transactions batched in Syncs: 1 Number of syncs: 230 SyncTimes(ms): 834 
2015-12-10 23:40:17,524 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741967_1143 127.0.0.1:50010 
2015-12-10 23:40:19,416 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741967_1143]
2015-12-10 23:40:23,989 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0011/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073741971_1147{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:40:24,106 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741971_1147{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:40:24,110 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0011/job.jar is closed by DFSClient_NONMAPREDUCE_-1257412237_1
2015-12-10 23:40:24,116 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0011/job.jar
2015-12-10 23:40:24,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0011/job.split
2015-12-10 23:40:24,197 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0011/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073741972_1148{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:40:24,208 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741972_1148{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:40:24,210 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0011/job.split is closed by DFSClient_NONMAPREDUCE_-1257412237_1
2015-12-10 23:40:24,226 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0011/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073741973_1149{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:40:24,233 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741973_1149{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:40:24,236 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0011/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1257412237_1
2015-12-10 23:40:24,470 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0011/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741974_1150{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:40:24,497 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741974_1150{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:40:24,500 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0011/job.xml is closed by DFSClient_NONMAPREDUCE_-1257412237_1
2015-12-10 23:40:29,551 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0011/job_1449802555556_0011_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741975_1151{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:40:29,646 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741975_1151{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:40:29,652 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0011/job_1449802555556_0011_1_conf.xml is closed by DFSClient_NONMAPREDUCE_536831377_1
2015-12-10 23:40:29,830 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:40:29,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:40:45,850 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0011/job_1449802555556_0011_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073741976_1152{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:40:46,100 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0011/job_1449802555556_0011_1.jhist for DFSClient_NONMAPREDUCE_536831377_1
2015-12-10 23:40:55,690 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0011_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073741977_1153{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:40:55,890 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741977_1153{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:40:55,903 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0011_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449802555556_0011_r_000000_0_1728892863_1
2015-12-10 23:40:56,163 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0011/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_536831377_1
2015-12-10 23:40:56,246 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_536831377_1
2015-12-10 23:40:56,252 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0011/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_536831377_1
2015-12-10 23:40:56,281 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741976_1152{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12232
2015-12-10 23:40:56,284 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0011/job_1449802555556_0011_1.jhist is closed by DFSClient_NONMAPREDUCE_536831377_1
2015-12-10 23:40:56,308 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0011.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741978_1154{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:40:56,321 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741978_1154{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:40:56,326 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0011.summary_tmp is closed by DFSClient_NONMAPREDUCE_536831377_1
2015-12-10 23:40:56,380 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0011-1449808824712-parallels-chess+data-1449808856254-3-1-SUCCEEDED-default-1449808829219.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741979_1155{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:40:56,395 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741979_1155{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:40:56,397 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0011-1449808824712-parallels-chess+data-1449808856254-3-1-SUCCEEDED-default-1449808829219.jhist_tmp is closed by DFSClient_NONMAPREDUCE_536831377_1
2015-12-10 23:40:56,424 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0011_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741980_1156{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:40:56,448 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741980_1156{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:40:56,451 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0011_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_536831377_1
2015-12-10 23:40:57,513 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741971_1147 127.0.0.1:50010 
2015-12-10 23:40:57,514 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741972_1148 127.0.0.1:50010 
2015-12-10 23:40:57,514 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741973_1149 127.0.0.1:50010 
2015-12-10 23:40:57,515 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741974_1150 127.0.0.1:50010 
2015-12-10 23:40:57,515 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741976_1152 127.0.0.1:50010 
2015-12-10 23:40:57,515 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741975_1151 127.0.0.1:50010 
2015-12-10 23:40:58,461 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741971_1147, blk_1073741972_1148, blk_1073741973_1149, blk_1073741974_1150, blk_1073741975_1151, blk_1073741976_1152]
2015-12-10 23:40:59,832 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-10 23:40:59,838 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 7 millisecond(s).
2015-12-10 23:41:29,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:41:29,832 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:41:59,832 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:41:59,833 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:42:29,833 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:42:29,833 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:42:59,834 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:42:59,834 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:43:29,834 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:43:29,834 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:43:54,560 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-10 23:43:59,834 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:43:59,835 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:44:21,418 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 407 Total time for transactions(ms): 70 Number of transactions batched in Syncs: 1 Number of syncs: 287 SyncTimes(ms): 1158 
2015-12-10 23:44:21,421 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741977_1153 127.0.0.1:50010 
2015-12-10 23:44:22,493 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741977_1153]
2015-12-10 23:44:29,834 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:44:29,835 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:44:59,836 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:44:59,837 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-10 23:45:29,835 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:45:29,835 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:45:59,835 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:45:59,836 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:46:29,837 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:46:29,837 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:46:59,837 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:46:59,839 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:47:29,839 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-10 23:47:29,839 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:47:32,493 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 408 Total time for transactions(ms): 71 Number of transactions batched in Syncs: 1 Number of syncs: 288 SyncTimes(ms): 1160 
2015-12-10 23:47:32,644 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0012/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073741981_1157{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:47:32,794 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741981_1157{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:47:32,799 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0012/job.jar is closed by DFSClient_NONMAPREDUCE_2105500336_1
2015-12-10 23:47:32,807 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0012/job.jar
2015-12-10 23:47:32,897 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0012/job.split
2015-12-10 23:47:32,916 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0012/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073741982_1158{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:47:32,931 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741982_1158{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:47:32,933 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0012/job.split is closed by DFSClient_NONMAPREDUCE_2105500336_1
2015-12-10 23:47:32,947 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0012/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073741983_1159{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:47:32,958 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741983_1159{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:47:32,960 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0012/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_2105500336_1
2015-12-10 23:47:33,223 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0012/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741984_1160{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:47:33,252 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741984_1160{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:47:33,256 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0012/job.xml is closed by DFSClient_NONMAPREDUCE_2105500336_1
2015-12-10 23:47:41,212 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0012/job_1449802555556_0012_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741985_1161{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:47:41,394 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741985_1161{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:47:41,398 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0012/job_1449802555556_0012_1_conf.xml is closed by DFSClient_NONMAPREDUCE_631685779_1
2015-12-10 23:47:57,033 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0012/job_1449802555556_0012_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073741986_1162{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:47:57,295 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0012/job_1449802555556_0012_1.jhist for DFSClient_NONMAPREDUCE_631685779_1
2015-12-10 23:47:59,840 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:47:59,859 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 20 millisecond(s).
2015-12-10 23:48:07,762 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0012_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073741987_1163{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:48:08,004 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741987_1163{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:48:08,010 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0012_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449802555556_0012_r_000000_0_-564690695_1
2015-12-10 23:48:09,336 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0012/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_631685779_1
2015-12-10 23:48:09,394 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_631685779_1
2015-12-10 23:48:09,402 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0012/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_631685779_1
2015-12-10 23:48:09,437 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741986_1162{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14411
2015-12-10 23:48:09,440 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0012/job_1449802555556_0012_1.jhist is closed by DFSClient_NONMAPREDUCE_631685779_1
2015-12-10 23:48:09,451 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0012.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741988_1164{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:48:09,458 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741988_1164{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:48:09,460 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0012.summary_tmp is closed by DFSClient_NONMAPREDUCE_631685779_1
2015-12-10 23:48:09,517 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0012-1449809253534-parallels-chess+data-1449809289404-3-1-SUCCEEDED-default-1449809260675.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741989_1165{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:48:09,525 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741989_1165{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:48:09,529 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0012-1449809253534-parallels-chess+data-1449809289404-3-1-SUCCEEDED-default-1449809260675.jhist_tmp is closed by DFSClient_NONMAPREDUCE_631685779_1
2015-12-10 23:48:09,560 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0012_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741990_1166{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:48:09,570 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741990_1166{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:48:09,573 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0012_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_631685779_1
2015-12-10 23:48:10,628 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741981_1157 127.0.0.1:50010 
2015-12-10 23:48:10,629 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741982_1158 127.0.0.1:50010 
2015-12-10 23:48:10,629 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741983_1159 127.0.0.1:50010 
2015-12-10 23:48:10,629 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741984_1160 127.0.0.1:50010 
2015-12-10 23:48:10,629 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741986_1162 127.0.0.1:50010 
2015-12-10 23:48:10,629 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741985_1161 127.0.0.1:50010 
2015-12-10 23:48:13,536 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741984_1160, blk_1073741985_1161, blk_1073741986_1162, blk_1073741981_1157, blk_1073741982_1158, blk_1073741983_1159]
2015-12-10 23:48:29,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-10 23:48:29,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:48:59,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:48:59,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:49:25,059 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 488 Total time for transactions(ms): 78 Number of transactions batched in Syncs: 1 Number of syncs: 344 SyncTimes(ms): 1514 
2015-12-10 23:49:25,060 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741987_1163 127.0.0.1:50010 
2015-12-10 23:49:25,551 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741987_1163]
2015-12-10 23:49:29,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:49:29,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:49:59,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:49:59,842 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:50:29,842 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:50:29,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:50:45,589 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 489 Total time for transactions(ms): 79 Number of transactions batched in Syncs: 1 Number of syncs: 345 SyncTimes(ms): 1514 
2015-12-10 23:50:45,689 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0013/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073741991_1167{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:50:45,815 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741991_1167{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:50:45,820 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0013/job.jar is closed by DFSClient_NONMAPREDUCE_384772119_1
2015-12-10 23:50:45,834 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0013/job.jar
2015-12-10 23:50:45,911 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0013/job.split
2015-12-10 23:50:45,923 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0013/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073741992_1168{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:50:45,935 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741992_1168{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:50:45,937 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0013/job.split is closed by DFSClient_NONMAPREDUCE_384772119_1
2015-12-10 23:50:45,959 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0013/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073741993_1169{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:50:45,966 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741993_1169{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:50:45,969 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0013/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_384772119_1
2015-12-10 23:50:46,213 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0013/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741994_1170{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:50:46,228 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741994_1170{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:50:46,230 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0013/job.xml is closed by DFSClient_NONMAPREDUCE_384772119_1
2015-12-10 23:50:52,120 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0013/job_1449802555556_0013_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073741995_1171{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:50:52,264 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741995_1171{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:50:52,273 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0013/job_1449802555556_0013_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1988252152_1
2015-12-10 23:50:59,842 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:50:59,856 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 14 millisecond(s).
2015-12-10 23:51:05,900 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0013/job_1449802555556_0013_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073741996_1172{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:51:06,074 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0013/job_1449802555556_0013_1.jhist for DFSClient_NONMAPREDUCE_1988252152_1
2015-12-10 23:51:15,721 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0013_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073741997_1173{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:51:16,029 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741997_1173{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:51:16,044 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0013_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449802555556_0013_r_000000_0_1067719597_1
2015-12-10 23:51:17,342 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0013/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_1988252152_1
2015-12-10 23:51:17,396 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_1988252152_1
2015-12-10 23:51:17,405 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0013/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_1988252152_1
2015-12-10 23:51:17,439 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741996_1172{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12220
2015-12-10 23:51:17,444 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0013/job_1449802555556_0013_1.jhist is closed by DFSClient_NONMAPREDUCE_1988252152_1
2015-12-10 23:51:17,456 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0013.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741998_1174{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:51:17,465 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741998_1174{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:51:17,468 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0013.summary_tmp is closed by DFSClient_NONMAPREDUCE_1988252152_1
2015-12-10 23:51:17,537 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0013-1449809446504-parallels-chess+data-1449809477407-3-1-SUCCEEDED-default-1449809451681.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073741999_1175{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:51:17,553 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741999_1175{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:51:17,568 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0013-1449809446504-parallels-chess+data-1449809477407-3-1-SUCCEEDED-default-1449809451681.jhist_tmp is closed by DFSClient_NONMAPREDUCE_1988252152_1
2015-12-10 23:51:17,597 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0013_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742000_1176{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-10 23:51:17,614 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742000_1176{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-10 23:51:17,621 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0013_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_1988252152_1
2015-12-10 23:51:18,685 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741991_1167 127.0.0.1:50010 
2015-12-10 23:51:18,685 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741992_1168 127.0.0.1:50010 
2015-12-10 23:51:18,685 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741993_1169 127.0.0.1:50010 
2015-12-10 23:51:18,686 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741994_1170 127.0.0.1:50010 
2015-12-10 23:51:18,686 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741996_1172 127.0.0.1:50010 
2015-12-10 23:51:18,686 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741995_1171 127.0.0.1:50010 
2015-12-10 23:51:19,589 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741991_1167, blk_1073741992_1168, blk_1073741993_1169, blk_1073741994_1170, blk_1073741995_1171, blk_1073741996_1172]
2015-12-10 23:51:29,842 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:51:29,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:51:30,327 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741997_1173 127.0.0.1:50010 
2015-12-10 23:51:31,592 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741997_1173]
2015-12-10 23:51:59,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:51:59,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:52:29,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:52:29,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:52:59,844 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:52:59,844 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-10 23:53:29,842 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:53:29,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:53:59,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:53:59,844 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:54:29,844 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:54:29,844 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:54:59,844 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:54:59,844 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:55:29,844 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:55:29,844 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:55:59,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-10 23:55:59,846 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:56:29,847 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:56:29,847 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:56:59,849 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-10 23:56:59,849 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:57:29,849 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:57:29,850 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:57:59,851 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:57:59,851 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-10 23:58:29,851 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:58:29,852 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:58:59,853 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:58:59,854 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-10 23:59:29,852 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-10 23:59:29,853 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-10 23:59:59,853 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-10 23:59:59,854 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 00:00:29,853 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 00:00:29,853 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 00:00:53,338 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 570 Total time for transactions(ms): 91 Number of transactions batched in Syncs: 1 Number of syncs: 402 SyncTimes(ms): 1620 
2015-12-11 00:00:53,458 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0014/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742001_1177{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 00:00:53,576 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742001_1177{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 00:00:53,584 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0014/job.jar is closed by DFSClient_NONMAPREDUCE_-1136204055_1
2015-12-11 00:00:53,591 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0014/job.jar
2015-12-11 00:00:53,674 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0014/job.split
2015-12-11 00:00:53,685 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0014/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742002_1178{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 00:00:53,697 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742002_1178{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 00:00:53,702 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0014/job.split is closed by DFSClient_NONMAPREDUCE_-1136204055_1
2015-12-11 00:00:53,720 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0014/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742003_1179{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 00:00:53,727 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742003_1179{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 00:00:53,731 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0014/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1136204055_1
2015-12-11 00:00:53,985 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0014/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742004_1180{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 00:00:54,000 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742004_1180{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 00:00:54,002 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0014/job.xml is closed by DFSClient_NONMAPREDUCE_-1136204055_1
2015-12-11 00:00:59,853 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 00:00:59,853 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 00:00:59,872 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0014/job_1449802555556_0014_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742005_1181{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 00:01:00,006 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742005_1181{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 00:01:00,016 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0014/job_1449802555556_0014_1_conf.xml is closed by DFSClient_NONMAPREDUCE_785148752_1
2015-12-11 00:01:14,269 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1103ms
No GCs detected
2015-12-11 00:01:17,456 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0014/job_1449802555556_0014_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742006_1182{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 00:01:17,920 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0014/job_1449802555556_0014_1.jhist for DFSClient_NONMAPREDUCE_785148752_1
2015-12-11 00:01:29,715 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0014_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742007_1183{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 00:01:29,859 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30006 milliseconds
2015-12-11 00:01:29,902 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 43 millisecond(s).
2015-12-11 00:01:30,244 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742007_1183{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 00:01:30,253 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449802555556_0014_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449802555556_0014_r_000000_0_41215363_1
2015-12-11 00:01:30,964 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0014/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_785148752_1
2015-12-11 00:01:31,017 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_785148752_1
2015-12-11 00:01:31,033 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0014/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_785148752_1
2015-12-11 00:01:31,080 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742006_1182{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12232
2015-12-11 00:01:31,085 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449802555556_0014/job_1449802555556_0014_1.jhist is closed by DFSClient_NONMAPREDUCE_785148752_1
2015-12-11 00:01:31,097 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0014.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742008_1184{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 00:01:31,108 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742008_1184{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 00:01:31,112 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0014.summary_tmp is closed by DFSClient_NONMAPREDUCE_785148752_1
2015-12-11 00:01:31,204 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0014-1449810054271-parallels-chess+data-1449810091036-3-1-SUCCEEDED-default-1449810059372.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742009_1185{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 00:01:31,217 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742009_1185{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 00:01:31,221 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0014-1449810054271-parallels-chess+data-1449810091036-3-1-SUCCEEDED-default-1449810059372.jhist_tmp is closed by DFSClient_NONMAPREDUCE_785148752_1
2015-12-11 00:01:31,253 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0014_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742010_1186{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 00:01:31,266 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742010_1186{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 00:01:31,268 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449802555556_0014_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_785148752_1
2015-12-11 00:01:32,350 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742001_1177 127.0.0.1:50010 
2015-12-11 00:01:32,350 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742002_1178 127.0.0.1:50010 
2015-12-11 00:01:32,350 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742003_1179 127.0.0.1:50010 
2015-12-11 00:01:32,350 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742004_1180 127.0.0.1:50010 
2015-12-11 00:01:32,350 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742006_1182 127.0.0.1:50010 
2015-12-11 00:01:32,351 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742005_1181 127.0.0.1:50010 
2015-12-11 00:01:34,807 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742001_1177, blk_1073742002_1178, blk_1073742003_1179, blk_1073742004_1180, blk_1073742005_1181, blk_1073742006_1182]
2015-12-11 00:01:59,860 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 00:01:59,860 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 00:02:29,862 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 00:02:29,862 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 00:02:59,862 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 00:02:59,862 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 00:03:29,863 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 00:03:29,863 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 00:03:59,862 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 00:03:59,863 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 00:04:29,864 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 00:04:29,864 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 00:04:36,857 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-11 00:04:36,904 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-11 10:06:36,818 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-11 10:06:36,831 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-11 10:06:36,835 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-11 10:06:37,294 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-11 10:06:37,444 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-11 10:06:37,444 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-11 10:06:37,446 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-11 10:06:37,447 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-11 10:06:37,880 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-11 10:06:37,960 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-11 10:06:37,965 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-11 10:06:37,979 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-11 10:06:37,982 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-11 10:06:37,982 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-11 10:06:37,982 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-11 10:06:38,030 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-11 10:06:38,035 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-11 10:06:38,073 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-11 10:06:38,073 INFO org.mortbay.log: jetty-6.1.26
2015-12-11 10:06:38,427 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-11 10:06:38,504 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-11 10:06:38,504 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-11 10:06:38,554 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-11 10:06:38,577 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-11 10:06:38,654 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-11 10:06:38,654 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-11 10:06:38,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-11 10:06:38,662 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 11 10:06:38
2015-12-11 10:06:38,667 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-11 10:06:38,667 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-11 10:06:38,670 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-11 10:06:38,671 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-11 10:06:38,717 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-11 10:06:38,717 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-11 10:06:38,717 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-11 10:06:38,717 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-11 10:06:38,717 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-11 10:06:38,718 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-11 10:06:38,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-11 10:06:38,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-11 10:06:38,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-11 10:06:38,724 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-11 10:06:38,724 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-11 10:06:38,724 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-11 10:06:38,724 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-11 10:06:38,727 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-11 10:06:38,965 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-11 10:06:38,965 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-11 10:06:38,965 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-11 10:06:38,965 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-11 10:06:38,967 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-11 10:06:38,978 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-11 10:06:38,978 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-11 10:06:38,978 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-11 10:06:38,978 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-11 10:06:38,980 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-11 10:06:38,980 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-11 10:06:38,980 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-11 10:06:38,981 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-11 10:06:38,981 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-11 10:06:38,984 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-11 10:06:38,984 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-11 10:06:38,984 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-11 10:06:38,984 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-11 10:06:38,988 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-11 10:06:38,988 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-11 10:06:38,988 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-11 10:06:39,006 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/name/in_use.lock acquired by nodename 32408@ubuntu
2015-12-11 10:06:39,120 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/parallels/hadoop/tmp/dfs/name/current
2015-12-11 10:06:39,302 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000000854 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000854-0000000000000001506
2015-12-11 10:06:39,342 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 49 INodes.
2015-12-11 10:06:39,384 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-11 10:06:39,384 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 853 from /home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000853
2015-12-11 10:06:39,384 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@21e3e923 expecting start txid #854
2015-12-11 10:06:39,384 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000854-0000000000000001506
2015-12-11 10:06:39,386 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000854-0000000000000001506' to transaction ID 854
2015-12-11 10:06:39,489 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000000854-0000000000000001506 of size 1048576 edits # 653 loaded in 0 seconds
2015-12-11 10:06:39,490 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2015-12-11 10:06:39,490 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2015-12-11 10:06:39,547 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 853
2015-12-11 10:06:39,547 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000434, cpktTxId=0000000000000000434)
2015-12-11 10:06:39,557 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1507
2015-12-11 10:06:39,597 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-11 10:06:39,597 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 609 msecs
2015-12-11 10:06:39,870 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-11 10:06:39,877 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-11 10:06:39,897 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-11 10:06:39,936 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-11 10:06:39,948 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-11 10:06:39,948 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-11 10:06:39,948 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 58 blocks to reach the threshold 0.9990 of total blocks 58.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2015-12-11 10:06:39,980 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-11 10:06:39,981 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-11 10:06:39,982 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-11 10:06:39,982 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-11 10:06:39,987 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-11 10:06:39,987 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 46301459 milliseconds
2015-12-11 10:06:39,988 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 10:06:42,991 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0) storage 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-11 10:06:42,994 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-11 10:06:42,995 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-11 10:06:43,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-11 10:06:43,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-75e73aff-3510-410a-91be-881360516f94 for DN 127.0.0.1:50010
2015-12-11 10:06:43,178 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 57 has reached the threshold 0.9990 of total blocks 58. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2015-12-11 10:06:43,178 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-11 10:06:43,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-75e73aff-3510-410a-91be-881360516f94,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-12-11 10:06:43,179 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 58, hasStaleStorages: false, processing time: 6 msecs
2015-12-11 10:06:43,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 58
2015-12-11 10:06:43,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-11 10:06:43,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-12-11 10:06:43,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-11 10:06:43,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-11 10:06:43,181 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 3 msec
2015-12-11 10:07:03,184 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 58 has reached the threshold 0.9990 of total blocks 58. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2015-12-11 10:07:09,986 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:07:09,987 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:07:13,187 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 34 secs
2015-12-11 10:07:13,187 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2015-12-11 10:07:13,187 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2015-12-11 10:07:13,187 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-12-11 10:07:15,526 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742007_1183 127.0.0.1:50010 
2015-12-11 10:07:15,965 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742007_1183]
2015-12-11 10:07:26,108 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0001/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742011_1187{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:07:26,451 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742011_1187{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-11 10:07:26,451 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2015-12-11 10:07:26,471 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742011_1187{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 4301
2015-12-11 10:07:26,874 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0001/job.jar is closed by DFSClient_NONMAPREDUCE_-349724858_1
2015-12-11 10:07:26,882 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0001/job.jar
2015-12-11 10:07:26,978 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0001/job.split
2015-12-11 10:07:26,998 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0001/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742012_1188{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:07:27,017 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742012_1188{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:07:27,019 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0001/job.split is closed by DFSClient_NONMAPREDUCE_-349724858_1
2015-12-11 10:07:27,033 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0001/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742013_1189{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:07:27,045 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742013_1189{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:07:27,048 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-349724858_1
2015-12-11 10:07:27,310 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0001/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742014_1190{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:07:27,330 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742014_1190{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:07:27,333 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0001/job.xml is closed by DFSClient_NONMAPREDUCE_-349724858_1
2015-12-11 10:07:37,830 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0001/job_1449846414359_0001_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742015_1191{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:07:37,943 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742015_1191{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:07:37,949 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0001/job_1449846414359_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1043095702_1
2015-12-11 10:07:39,987 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:07:39,987 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:07:48,592 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-11 10:07:48,592 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-11 10:07:48,593 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1507
2015-12-11 10:07:48,596 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 40 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 0 Number of syncs: 29 SyncTimes(ms): 117 
2015-12-11 10:07:48,784 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 40 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 0 Number of syncs: 30 SyncTimes(ms): 305 
2015-12-11 10:07:48,829 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000001507 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000001507-0000000000000001546
2015-12-11 10:07:48,829 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1547
2015-12-11 10:07:55,341 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0001/job_1449846414359_0001_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742016_1192{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:07:55,533 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0001/job_1449846414359_0001_1.jhist for DFSClient_NONMAPREDUCE_-1043095702_1
2015-12-11 10:07:55,720 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 1333.33 KB/s
2015-12-11 10:07:55,720 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000001546 size 8747 bytes.
2015-12-11 10:07:55,737 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 1506
2015-12-11 10:07:55,737 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000000853, cpktTxId=0000000000000000853)
2015-12-11 10:08:06,051 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449846414359_0001_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742017_1193{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:08:06,304 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742017_1193{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:08:06,312 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449846414359_0001_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449846414359_0001_r_000000_0_-1584321040_1
2015-12-11 10:08:07,600 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0001/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1043095702_1
2015-12-11 10:08:07,652 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1043095702_1
2015-12-11 10:08:07,659 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0001/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1043095702_1
2015-12-11 10:08:07,690 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742016_1192{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14421
2015-12-11 10:08:07,693 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0001/job_1449846414359_0001_1.jhist is closed by DFSClient_NONMAPREDUCE_-1043095702_1
2015-12-11 10:08:07,705 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0001.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742018_1194{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:08:07,715 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742018_1194{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:08:07,717 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0001.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1043095702_1
2015-12-11 10:08:07,780 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0001-1449846447629-parallels-chess+data-1449846487661-3-1-SUCCEEDED-default-1449846457356.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742019_1195{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:08:07,788 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742019_1195{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:08:07,790 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0001-1449846447629-parallels-chess+data-1449846487661-3-1-SUCCEEDED-default-1449846457356.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1043095702_1
2015-12-11 10:08:07,816 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0001_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742020_1196{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:08:07,824 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742020_1196{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:08:07,827 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0001_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1043095702_1
2015-12-11 10:08:09,029 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742011_1187 127.0.0.1:50010 
2015-12-11 10:08:09,029 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742012_1188 127.0.0.1:50010 
2015-12-11 10:08:09,029 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742013_1189 127.0.0.1:50010 
2015-12-11 10:08:09,029 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742014_1190 127.0.0.1:50010 
2015-12-11 10:08:09,029 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742016_1192 127.0.0.1:50010 
2015-12-11 10:08:09,029 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742015_1191 127.0.0.1:50010 
2015-12-11 10:08:09,974 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742016_1192, blk_1073742011_1187, blk_1073742012_1188, blk_1073742013_1189, blk_1073742014_1190, blk_1073742015_1191]
2015-12-11 10:08:09,988 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:08:09,989 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:08:21,275 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742017_1193 127.0.0.1:50010 
2015-12-11 10:08:21,976 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742017_1193]
2015-12-11 10:08:39,988 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:08:39,989 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:09:09,988 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:09:09,989 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:09:39,989 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:09:39,989 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:10:09,990 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:10:09,990 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:10:39,989 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:10:39,990 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:11:09,989 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:11:09,990 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:11:39,989 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:11:39,990 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:12:09,990 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:12:09,991 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:12:39,991 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:12:39,991 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:13:09,990 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:13:09,991 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:13:39,991 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:13:39,992 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:14:09,992 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:14:09,992 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:14:39,993 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:14:39,993 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:15:09,992 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:15:09,993 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:15:39,993 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:15:39,994 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:16:09,994 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:16:09,994 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:16:29,835 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 49 Total time for transactions(ms): 41 Number of transactions batched in Syncs: 0 Number of syncs: 34 SyncTimes(ms): 202 
2015-12-11 10:16:29,945 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0002/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742021_1197{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:16:30,095 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742021_1197{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:16:30,105 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0002/job.jar is closed by DFSClient_NONMAPREDUCE_451392798_1
2015-12-11 10:16:30,112 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0002/job.jar
2015-12-11 10:16:30,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0002/job.split
2015-12-11 10:16:30,198 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0002/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742022_1198{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:16:30,210 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742022_1198{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:16:30,213 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0002/job.split is closed by DFSClient_NONMAPREDUCE_451392798_1
2015-12-11 10:16:30,228 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0002/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742023_1199{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:16:30,235 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742023_1199{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:16:30,238 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0002/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_451392798_1
2015-12-11 10:16:30,489 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0002/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742024_1200{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:16:30,507 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742024_1200{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:16:30,531 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0002/job.xml is closed by DFSClient_NONMAPREDUCE_451392798_1
2015-12-11 10:16:37,649 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0002/job_1449846414359_0002_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742025_1201{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:16:37,795 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742025_1201{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:16:37,801 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0002/job_1449846414359_0002_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-645430726_1
2015-12-11 10:16:39,993 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:16:39,994 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:16:53,662 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0002/job_1449846414359_0002_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742026_1202{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:16:53,900 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0002/job_1449846414359_0002_1.jhist for DFSClient_NONMAPREDUCE_-645430726_1
2015-12-11 10:17:04,455 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449846414359_0002_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742027_1203{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:17:04,760 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742027_1203{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:17:04,769 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449846414359_0002_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449846414359_0002_r_000000_0_145808178_1
2015-12-11 10:17:05,155 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0002/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-645430726_1
2015-12-11 10:17:05,223 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-645430726_1
2015-12-11 10:17:05,232 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0002/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-645430726_1
2015-12-11 10:17:05,272 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742026_1202{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14411
2015-12-11 10:17:05,275 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0002/job_1449846414359_0002_1.jhist is closed by DFSClient_NONMAPREDUCE_-645430726_1
2015-12-11 10:17:05,284 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0002.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742028_1204{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:17:05,292 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742028_1204{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:17:05,295 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0002.summary_tmp is closed by DFSClient_NONMAPREDUCE_-645430726_1
2015-12-11 10:17:05,347 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0002-1449846990790-parallels-chess+data-1449847025235-3-1-SUCCEEDED-default-1449846997152.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742029_1205{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:17:05,356 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742029_1205{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:17:05,365 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0002-1449846990790-parallels-chess+data-1449847025235-3-1-SUCCEEDED-default-1449846997152.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-645430726_1
2015-12-11 10:17:05,388 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0002_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742030_1206{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:17:05,400 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742030_1206{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:17:05,402 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0002_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-645430726_1
2015-12-11 10:17:06,457 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742021_1197 127.0.0.1:50010 
2015-12-11 10:17:06,457 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742022_1198 127.0.0.1:50010 
2015-12-11 10:17:06,458 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742023_1199 127.0.0.1:50010 
2015-12-11 10:17:06,458 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742024_1200 127.0.0.1:50010 
2015-12-11 10:17:06,458 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742026_1202 127.0.0.1:50010 
2015-12-11 10:17:06,458 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742025_1201 127.0.0.1:50010 
2015-12-11 10:17:07,095 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742021_1197, blk_1073742022_1198, blk_1073742023_1199, blk_1073742024_1200, blk_1073742025_1201, blk_1073742026_1202]
2015-12-11 10:17:09,995 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:17:09,995 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:17:39,995 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:17:39,996 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:18:09,996 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:18:09,996 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:18:39,998 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:18:39,998 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:19:09,998 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:19:09,998 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:19:39,997 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:19:39,998 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:20:09,998 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:20:09,999 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:20:39,999 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:20:40,000 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:21:10,001 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:21:10,001 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:21:40,001 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:21:40,001 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:21:46,354 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 129 Total time for transactions(ms): 61 Number of transactions batched in Syncs: 0 Number of syncs: 90 SyncTimes(ms): 462 
2015-12-11 10:21:46,355 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742027_1203 127.0.0.1:50010 
2015-12-11 10:21:49,141 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742027_1203]
2015-12-11 10:21:52,746 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0003/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742031_1207{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:21:52,882 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742031_1207{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:21:52,886 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0003/job.jar is closed by DFSClient_NONMAPREDUCE_-1989202495_1
2015-12-11 10:21:52,891 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0003/job.jar
2015-12-11 10:21:52,970 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0003/job.split
2015-12-11 10:21:52,985 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0003/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742032_1208{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:21:52,998 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742032_1208{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:21:53,000 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0003/job.split is closed by DFSClient_NONMAPREDUCE_-1989202495_1
2015-12-11 10:21:53,011 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0003/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742033_1209{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:21:53,018 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742033_1209{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:21:53,021 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0003/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1989202495_1
2015-12-11 10:21:53,278 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0003/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742034_1210{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:21:53,290 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742034_1210{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:21:53,293 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0003/job.xml is closed by DFSClient_NONMAPREDUCE_-1989202495_1
2015-12-11 10:21:59,708 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0003/job_1449846414359_0003_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742035_1211{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:21:59,794 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742035_1211{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:21:59,799 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0003/job_1449846414359_0003_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1868850257_1
2015-12-11 10:22:10,000 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:22:10,001 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:22:15,166 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0003/job_1449846414359_0003_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742036_1212{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:22:15,519 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0003/job_1449846414359_0003_1.jhist for DFSClient_NONMAPREDUCE_-1868850257_1
2015-12-11 10:22:24,999 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449846414359_0003_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742037_1213{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:22:25,134 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742037_1213{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:22:25,141 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449846414359_0003_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449846414359_0003_r_000000_0_-1257080540_1
2015-12-11 10:22:25,594 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0003/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1868850257_1
2015-12-11 10:22:25,673 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1868850257_1
2015-12-11 10:22:25,681 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0003/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1868850257_1
2015-12-11 10:22:25,734 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742036_1212{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12232
2015-12-11 10:22:25,738 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0003/job_1449846414359_0003_1.jhist is closed by DFSClient_NONMAPREDUCE_-1868850257_1
2015-12-11 10:22:25,752 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0003.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742038_1214{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:22:25,760 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742038_1214{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:22:25,762 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0003.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1868850257_1
2015-12-11 10:22:25,800 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0003-1449847313539-parallels-chess+data-1449847345682-3-1-SUCCEEDED-default-1449847319264.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742039_1215{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:22:25,813 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742039_1215{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:22:25,817 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0003-1449847313539-parallels-chess+data-1449847345682-3-1-SUCCEEDED-default-1449847319264.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1868850257_1
2015-12-11 10:22:25,842 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0003_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742040_1216{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:22:25,852 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742040_1216{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:22:25,857 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0003_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1868850257_1
2015-12-11 10:22:26,921 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742031_1207 127.0.0.1:50010 
2015-12-11 10:22:26,921 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742032_1208 127.0.0.1:50010 
2015-12-11 10:22:26,921 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742033_1209 127.0.0.1:50010 
2015-12-11 10:22:26,922 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742034_1210 127.0.0.1:50010 
2015-12-11 10:22:26,922 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742036_1212 127.0.0.1:50010 
2015-12-11 10:22:26,922 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742035_1211 127.0.0.1:50010 
2015-12-11 10:22:28,428 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742032_1208, blk_1073742033_1209, blk_1073742034_1210, blk_1073742035_1211, blk_1073742036_1212, blk_1073742031_1207]
2015-12-11 10:22:40,001 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:22:40,002 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:23:10,002 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:23:10,002 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:23:40,002 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:23:40,003 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:24:10,002 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:24:10,004 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:24:40,003 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:24:40,003 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:24:40,848 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 214 Total time for transactions(ms): 72 Number of transactions batched in Syncs: 0 Number of syncs: 148 SyncTimes(ms): 719 
2015-12-11 10:24:40,850 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742037_1213 127.0.0.1:50010 
2015-12-11 10:24:43,451 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742037_1213]
2015-12-11 10:24:47,091 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0004/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742041_1217{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:24:47,229 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742041_1217{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:24:47,233 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0004/job.jar is closed by DFSClient_NONMAPREDUCE_712849395_1
2015-12-11 10:24:47,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0004/job.jar
2015-12-11 10:24:47,304 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0004/job.split
2015-12-11 10:24:47,318 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0004/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742042_1218{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:24:47,327 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742042_1218{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:24:47,330 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0004/job.split is closed by DFSClient_NONMAPREDUCE_712849395_1
2015-12-11 10:24:47,345 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0004/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742043_1219{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:24:47,356 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742043_1219{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:24:47,359 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0004/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_712849395_1
2015-12-11 10:24:47,546 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0004/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742044_1220{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:24:47,580 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742044_1220{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:24:47,583 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0004/job.xml is closed by DFSClient_NONMAPREDUCE_712849395_1
2015-12-11 10:24:52,898 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0004/job_1449846414359_0004_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742045_1221{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:24:53,043 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742045_1221{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:24:53,056 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0004/job_1449846414359_0004_1_conf.xml is closed by DFSClient_NONMAPREDUCE_640598033_1
2015-12-11 10:25:09,957 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0004/job_1449846414359_0004_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742046_1222{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:25:10,025 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30021 milliseconds
2015-12-11 10:25:10,027 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 4 millisecond(s).
2015-12-11 10:25:10,112 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0004/job_1449846414359_0004_1.jhist for DFSClient_NONMAPREDUCE_640598033_1
2015-12-11 10:25:20,070 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449846414359_0004_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742047_1223{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:25:20,337 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742047_1223{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:25:20,342 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449846414359_0004_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449846414359_0004_r_000000_0_1434525805_1
2015-12-11 10:25:21,597 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0004/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_640598033_1
2015-12-11 10:25:21,657 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_640598033_1
2015-12-11 10:25:21,664 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0004/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_640598033_1
2015-12-11 10:25:21,704 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742046_1222{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14425
2015-12-11 10:25:21,707 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0004/job_1449846414359_0004_1.jhist is closed by DFSClient_NONMAPREDUCE_640598033_1
2015-12-11 10:25:21,718 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0004.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742048_1224{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:25:21,725 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742048_1224{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:25:21,728 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0004.summary_tmp is closed by DFSClient_NONMAPREDUCE_640598033_1
2015-12-11 10:25:21,785 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0004-1449847487826-parallels-chess+data-1449847521666-3-1-SUCCEEDED-default-1449847492567.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742049_1225{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:25:21,794 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742049_1225{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:25:21,799 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0004-1449847487826-parallels-chess+data-1449847521666-3-1-SUCCEEDED-default-1449847492567.jhist_tmp is closed by DFSClient_NONMAPREDUCE_640598033_1
2015-12-11 10:25:21,821 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0004_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742050_1226{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:25:21,838 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742050_1226{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:25:21,842 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0004_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_640598033_1
2015-12-11 10:25:22,892 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742041_1217 127.0.0.1:50010 
2015-12-11 10:25:22,893 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742042_1218 127.0.0.1:50010 
2015-12-11 10:25:22,893 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742043_1219 127.0.0.1:50010 
2015-12-11 10:25:22,893 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742044_1220 127.0.0.1:50010 
2015-12-11 10:25:22,893 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742046_1222 127.0.0.1:50010 
2015-12-11 10:25:22,893 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742045_1221 127.0.0.1:50010 
2015-12-11 10:25:25,464 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742041_1217, blk_1073742042_1218, blk_1073742043_1219, blk_1073742044_1220, blk_1073742045_1221, blk_1073742046_1222]
2015-12-11 10:25:40,026 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:25:40,027 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 10:26:10,026 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 10:26:10,026 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:26:40,027 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:26:40,027 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:27:10,027 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:27:10,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:27:40,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:27:40,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:28:10,027 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:28:10,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:28:40,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:28:40,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:29:10,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:29:10,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:29:40,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:29:40,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:30:10,027 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:30:10,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:30:40,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:30:40,028 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:31:10,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:31:10,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:31:40,030 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:31:40,031 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:32:10,031 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:32:10,031 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:32:40,033 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:32:40,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:33:10,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 10:33:10,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:33:40,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:33:40,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:34:10,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:34:10,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:34:40,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:34:40,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:35:10,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:35:10,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:35:40,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:35:40,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:36:10,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:36:10,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:36:40,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:36:40,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:37:10,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:37:10,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:37:40,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:37:40,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:38:10,036 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:38:10,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:38:40,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:38:40,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:39:10,038 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:39:10,038 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 10:39:40,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:39:40,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:40:10,038 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:40:10,038 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:40:33,042 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 295 Total time for transactions(ms): 88 Number of transactions batched in Syncs: 1 Number of syncs: 204 SyncTimes(ms): 794 
2015-12-11 10:40:33,043 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742047_1223 127.0.0.1:50010 
2015-12-11 10:40:34,694 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742047_1223]
2015-12-11 10:40:40,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:40:40,038 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:41:10,039 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:41:10,039 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:41:40,038 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:41:40,039 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:41:43,493 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 298 Total time for transactions(ms): 88 Number of transactions batched in Syncs: 1 Number of syncs: 207 SyncTimes(ms): 797 
2015-12-11 10:41:43,617 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0006/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742051_1227{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:41:43,781 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742051_1227{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:41:43,784 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0006/job.jar is closed by DFSClient_NONMAPREDUCE_-483856251_1
2015-12-11 10:41:43,790 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0006/job.jar
2015-12-11 10:41:43,863 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0006/job.split
2015-12-11 10:41:43,878 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0006/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742052_1228{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:41:43,889 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742052_1228{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:41:43,893 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0006/job.split is closed by DFSClient_NONMAPREDUCE_-483856251_1
2015-12-11 10:41:43,908 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0006/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742053_1229{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:41:43,915 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742053_1229{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:41:43,918 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0006/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-483856251_1
2015-12-11 10:41:44,181 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0006/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742054_1230{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:41:44,205 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742054_1230{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:41:44,208 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0006/job.xml is closed by DFSClient_NONMAPREDUCE_-483856251_1
2015-12-11 10:41:50,818 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0006/job_1449846414359_0006_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742055_1231{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:41:50,944 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742055_1231{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:41:50,951 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0006/job_1449846414359_0006_1_conf.xml is closed by DFSClient_NONMAPREDUCE_2066648004_1
2015-12-11 10:42:04,623 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1427ms
No GCs detected
2015-12-11 10:42:07,199 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0006/job_1449846414359_0006_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742056_1232{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:42:07,247 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0006/job_1449846414359_0006_1.jhist for DFSClient_NONMAPREDUCE_2066648004_1
2015-12-11 10:42:10,040 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:42:10,041 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:42:17,551 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449846414359_0006_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742057_1233{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:42:17,792 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742057_1233{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:42:17,796 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_temporary/1/_temporary/attempt_1449846414359_0006_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449846414359_0006_r_000000_0_-2002347730_1
2015-12-11 10:42:19,120 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0006/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_2066648004_1
2015-12-11 10:42:19,164 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_2066648004_1
2015-12-11 10:42:19,171 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0006/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_2066648004_1
2015-12-11 10:42:19,206 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742056_1232{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14262
2015-12-11 10:42:19,209 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0006/job_1449846414359_0006_1.jhist is closed by DFSClient_NONMAPREDUCE_2066648004_1
2015-12-11 10:42:19,224 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0006.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742058_1234{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:42:19,235 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742058_1234{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:42:19,238 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0006.summary_tmp is closed by DFSClient_NONMAPREDUCE_2066648004_1
2015-12-11 10:42:19,290 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0006-1449848504464-parallels-chess+data-1449848539173-3-1-SUCCEEDED-default-1449848510357.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742059_1235{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:42:19,302 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742059_1235{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:42:19,304 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0006-1449848504464-parallels-chess+data-1449848539173-3-1-SUCCEEDED-default-1449848510357.jhist_tmp is closed by DFSClient_NONMAPREDUCE_2066648004_1
2015-12-11 10:42:19,333 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0006_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742060_1236{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 10:42:19,351 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742060_1236{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 10:42:19,355 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0006_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_2066648004_1
2015-12-11 10:42:20,412 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742051_1227 127.0.0.1:50010 
2015-12-11 10:42:20,412 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742052_1228 127.0.0.1:50010 
2015-12-11 10:42:20,412 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742053_1229 127.0.0.1:50010 
2015-12-11 10:42:20,412 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742054_1230 127.0.0.1:50010 
2015-12-11 10:42:20,412 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742056_1232 127.0.0.1:50010 
2015-12-11 10:42:20,412 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742055_1231 127.0.0.1:50010 
2015-12-11 10:42:22,845 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742051_1227, blk_1073742052_1228, blk_1073742053_1229, blk_1073742054_1230, blk_1073742055_1231, blk_1073742056_1232]
2015-12-11 10:42:26,619 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742057_1233 127.0.0.1:50010 
2015-12-11 10:42:28,847 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742057_1233]
2015-12-11 10:42:40,040 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:42:40,041 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:43:10,041 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:43:10,041 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:43:40,041 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:43:40,041 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:44:10,041 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:44:10,041 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:44:40,040 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:44:40,041 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:45:10,041 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:45:10,042 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:45:40,042 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:45:40,042 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:46:10,042 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:46:10,042 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:46:40,042 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:46:40,043 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:47:10,043 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:47:10,043 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:47:40,044 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:47:40,045 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:48:10,045 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:48:10,046 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:48:40,047 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 10:48:40,048 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:49:10,049 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:49:10,049 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:49:40,049 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:49:40,049 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:50:10,049 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:50:10,049 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:50:40,048 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:50:40,049 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:51:10,050 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:51:10,050 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:51:40,050 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:51:40,050 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:52:10,050 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:52:10,050 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:52:40,050 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:52:40,050 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:53:10,050 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:53:10,050 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:53:40,051 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:53:40,052 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:54:10,052 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:54:10,052 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 10:54:40,053 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:54:40,054 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:55:10,054 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:55:10,054 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:55:40,054 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:55:40,054 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:56:10,054 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:56:10,054 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:56:40,054 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:56:40,055 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:57:10,054 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:57:10,055 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:57:40,055 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:57:40,055 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:58:10,055 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:58:10,055 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:58:40,055 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:58:40,055 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:59:10,054 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 10:59:10,055 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 10:59:40,055 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 10:59:40,056 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:00:10,056 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:00:10,056 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:00:40,057 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:00:40,058 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:01:10,058 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:01:10,059 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:01:40,059 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:01:40,060 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:02:10,060 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:02:10,061 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:02:40,061 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:02:40,062 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:03:10,062 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:03:10,063 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:03:40,063 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:03:40,064 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:04:10,064 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:04:10,064 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:04:40,063 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:04:40,064 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:05:10,063 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:05:10,064 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:05:40,064 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:05:40,064 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:06:10,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:06:10,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:06:40,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:06:40,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:07:10,065 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:07:10,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:07:40,065 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:07:40,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:07:57,357 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-11 11:07:57,358 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-11 11:07:57,358 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1547
2015-12-11 11:07:57,358 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 379 Total time for transactions(ms): 93 Number of transactions batched in Syncs: 1 Number of syncs: 264 SyncTimes(ms): 844 
2015-12-11 11:07:57,364 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 379 Total time for transactions(ms): 93 Number of transactions batched in Syncs: 1 Number of syncs: 265 SyncTimes(ms): 849 
2015-12-11 11:07:57,366 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000001547 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000001547-0000000000000001925
2015-12-11 11:07:57,366 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1926
2015-12-11 11:07:57,606 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 3000.00 KB/s
2015-12-11 11:07:57,606 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000001925 size 9733 bytes.
2015-12-11 11:07:57,611 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 1546
2015-12-11 11:07:57,612 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000001506, cpktTxId=0000000000000001506)
2015-12-11 11:08:10,067 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 11:08:10,068 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:08:40,068 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:08:40,068 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:09:10,069 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:09:10,070 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:09:40,071 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:09:40,071 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:10:10,071 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:10:10,071 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:10:40,072 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:10:40,073 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:11:10,073 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:11:10,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:11:40,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:11:40,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:12:10,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:12:10,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:12:40,073 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:12:40,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:13:10,075 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:13:10,075 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:13:40,076 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:13:40,076 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:14:10,076 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:14:10,077 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:14:40,077 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:14:40,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:15:10,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:15:10,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:15:40,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:15:40,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:16:10,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:16:10,079 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:16:40,079 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:16:40,079 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:17:10,079 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:17:10,079 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:17:40,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:17:40,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:18:10,081 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:18:10,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:18:40,081 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:18:40,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:19:10,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:19:10,083 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:19:40,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:19:40,083 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:20:10,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:20:10,083 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:20:40,083 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:20:40,084 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:21:10,083 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:21:10,084 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:21:40,085 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:21:40,085 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:22:10,085 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:22:10,085 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:22:31,171 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 11 
2015-12-11 11:22:31,302 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0007/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742061_1237{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:22:31,444 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742061_1237{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:22:31,448 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0007/job.jar is closed by DFSClient_NONMAPREDUCE_-1152410462_1
2015-12-11 11:22:31,454 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0007/job.jar
2015-12-11 11:22:31,548 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0007/job.split
2015-12-11 11:22:31,565 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0007/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742062_1238{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:22:31,571 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742062_1238{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:22:31,576 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0007/job.split is closed by DFSClient_NONMAPREDUCE_-1152410462_1
2015-12-11 11:22:31,588 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0007/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742063_1239{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:22:31,598 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742063_1239{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:22:31,601 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0007/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1152410462_1
2015-12-11 11:22:31,863 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0007/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742064_1240{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:22:31,879 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742064_1240{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:22:31,882 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0007/job.xml is closed by DFSClient_NONMAPREDUCE_-1152410462_1
2015-12-11 11:22:38,852 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0007/job_1449846414359_0007_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742065_1241{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:22:38,999 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742065_1241{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:22:39,005 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0007/job_1449846414359_0007_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1297431993_1
2015-12-11 11:22:40,085 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:22:40,085 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:22:53,520 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0007/job_1449846414359_0007_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742066_1242{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:22:53,722 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0007/job_1449846414359_0007_1.jhist for DFSClient_NONMAPREDUCE_1297431993_1
2015-12-11 11:23:03,790 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_a/_temporary/1/_temporary/attempt_1449846414359_0007_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742067_1243{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:23:03,982 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742067_1243{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:23:03,987 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_a/_temporary/1/_temporary/attempt_1449846414359_0007_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449846414359_0007_r_000000_0_1254106634_1
2015-12-11 11:23:05,252 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0007/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_1297431993_1
2015-12-11 11:23:05,308 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_a/_SUCCESS is closed by DFSClient_NONMAPREDUCE_1297431993_1
2015-12-11 11:23:05,316 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0007/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_1297431993_1
2015-12-11 11:23:05,366 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742066_1242{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13148
2015-12-11 11:23:05,370 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0007/job_1449846414359_0007_1.jhist is closed by DFSClient_NONMAPREDUCE_1297431993_1
2015-12-11 11:23:05,384 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0007.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742068_1244{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:23:05,393 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742068_1244{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:23:05,394 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0007.summary_tmp is closed by DFSClient_NONMAPREDUCE_1297431993_1
2015-12-11 11:23:05,442 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0007-1449850952140-parallels-chess+data-1449850985318-3-1-SUCCEEDED-default-1449850958286.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742069_1245{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:23:05,451 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742069_1245{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:23:05,457 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0007-1449850952140-parallels-chess+data-1449850985318-3-1-SUCCEEDED-default-1449850958286.jhist_tmp is closed by DFSClient_NONMAPREDUCE_1297431993_1
2015-12-11 11:23:05,494 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0007_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742070_1246{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:23:05,511 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742070_1246{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:23:05,520 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0007_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_1297431993_1
2015-12-11 11:23:06,576 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742061_1237 127.0.0.1:50010 
2015-12-11 11:23:06,578 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742062_1238 127.0.0.1:50010 
2015-12-11 11:23:06,578 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742063_1239 127.0.0.1:50010 
2015-12-11 11:23:06,578 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742064_1240 127.0.0.1:50010 
2015-12-11 11:23:06,578 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742066_1242 127.0.0.1:50010 
2015-12-11 11:23:06,578 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742065_1241 127.0.0.1:50010 
2015-12-11 11:23:08,388 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742064_1240, blk_1073742065_1241, blk_1073742066_1242, blk_1073742061_1237, blk_1073742062_1238, blk_1073742063_1239]
2015-12-11 11:23:10,088 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-11 11:23:10,089 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:23:40,091 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 11:23:40,091 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:24:10,091 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:24:10,092 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:24:40,092 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:24:40,093 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:25:10,093 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:25:10,093 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:25:40,094 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:25:40,094 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:26:10,094 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:26:10,094 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:26:40,095 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:26:40,096 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:27:10,096 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:27:10,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:27:40,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 11:27:40,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:28:10,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:28:10,100 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:28:40,100 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:28:40,100 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:29:10,100 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:29:10,100 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:29:40,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:29:40,100 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:30:10,100 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:30:10,101 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:30:40,102 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:30:40,102 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:31:10,102 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:31:10,102 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:31:40,104 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:31:40,104 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:32:10,105 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:32:10,105 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:32:40,105 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:32:40,105 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:33:10,106 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:33:10,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:33:40,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:33:40,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:34:10,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:34:10,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:34:40,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:34:40,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:35:10,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:35:10,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:35:40,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:35:40,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:36:10,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:36:10,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:36:40,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:36:40,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:37:10,110 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:37:10,111 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:37:40,110 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:37:40,113 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 11:38:10,111 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:38:10,112 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:38:40,112 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:38:40,113 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:39:10,113 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:39:10,113 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:39:40,114 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:39:40,114 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:40:10,114 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:40:10,114 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:40:40,116 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:40:40,116 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:40:58,883 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 85 Total time for transactions(ms): 36 Number of transactions batched in Syncs: 0 Number of syncs: 58 SyncTimes(ms): 133 
2015-12-11 11:40:58,889 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742067_1243 127.0.0.1:50010 
2015-12-11 11:40:59,605 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742067_1243]
2015-12-11 11:41:08,004 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0008/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742071_1247{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:41:08,153 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742071_1247{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:41:08,158 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0008/job.jar is closed by DFSClient_NONMAPREDUCE_-1380328326_1
2015-12-11 11:41:08,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0008/job.jar
2015-12-11 11:41:08,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0008/job.split
2015-12-11 11:41:08,269 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0008/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742072_1248{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:41:08,282 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742072_1248{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:41:08,285 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0008/job.split is closed by DFSClient_NONMAPREDUCE_-1380328326_1
2015-12-11 11:41:08,299 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0008/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742073_1249{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:41:08,311 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742073_1249{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:41:08,314 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0008/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1380328326_1
2015-12-11 11:41:08,572 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0008/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742074_1250{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:41:08,585 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742074_1250{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:41:08,589 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0008/job.xml is closed by DFSClient_NONMAPREDUCE_-1380328326_1
2015-12-11 11:41:10,115 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:41:10,116 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:41:14,756 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0008/job_1449846414359_0008_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742075_1251{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:41:14,867 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742075_1251{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:41:14,877 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0008/job_1449846414359_0008_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1994650406_1
2015-12-11 11:41:30,861 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0008/job_1449846414359_0008_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742076_1252{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:41:31,038 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0008/job_1449846414359_0008_1.jhist for DFSClient_NONMAPREDUCE_-1994650406_1
2015-12-11 11:41:40,116 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:41:40,118 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 11:41:41,538 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_a/_temporary/1/_temporary/attempt_1449846414359_0008_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742077_1253{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:41:41,768 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742077_1253{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:41:41,776 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_a/_temporary/1/_temporary/attempt_1449846414359_0008_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449846414359_0008_r_000000_0_-1916204941_1
2015-12-11 11:41:42,074 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0008/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1994650406_1
2015-12-11 11:41:42,123 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_a/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1994650406_1
2015-12-11 11:41:42,129 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0008/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1994650406_1
2015-12-11 11:41:42,166 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742076_1252{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12232
2015-12-11 11:41:42,169 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0008/job_1449846414359_0008_1.jhist is closed by DFSClient_NONMAPREDUCE_-1994650406_1
2015-12-11 11:41:42,180 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0008.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742078_1254{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:41:42,190 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742078_1254{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:41:42,194 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0008.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1994650406_1
2015-12-11 11:41:42,285 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0008-1449852068860-parallels-chess+data-1449852102131-3-1-SUCCEEDED-default-1449852074328.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742079_1255{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:41:42,312 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742079_1255{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:41:42,317 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0008-1449852068860-parallels-chess+data-1449852102131-3-1-SUCCEEDED-default-1449852074328.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1994650406_1
2015-12-11 11:41:42,348 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0008_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742080_1256{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 11:41:42,358 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742080_1256{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 11:41:42,361 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0008_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1994650406_1
2015-12-11 11:41:43,426 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742071_1247 127.0.0.1:50010 
2015-12-11 11:41:43,426 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742072_1248 127.0.0.1:50010 
2015-12-11 11:41:43,426 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742073_1249 127.0.0.1:50010 
2015-12-11 11:41:43,426 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742074_1250 127.0.0.1:50010 
2015-12-11 11:41:43,426 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742076_1252 127.0.0.1:50010 
2015-12-11 11:41:43,427 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742075_1251 127.0.0.1:50010 
2015-12-11 11:41:44,661 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742071_1247, blk_1073742072_1248, blk_1073742073_1249, blk_1073742074_1250, blk_1073742075_1251, blk_1073742076_1252]
2015-12-11 11:42:10,116 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:42:10,117 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:42:40,117 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:42:40,118 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:43:10,118 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:43:10,119 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:43:40,118 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:43:40,119 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:44:10,120 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 11:44:10,121 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:44:40,121 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:44:40,121 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:45:10,121 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:45:10,123 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:45:40,123 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:45:40,123 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:46:10,123 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:46:10,124 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:46:40,124 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:46:40,125 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:47:10,126 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:47:10,126 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:47:40,127 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:47:40,127 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:48:10,127 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:48:10,127 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:48:40,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:48:40,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:49:10,128 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:49:10,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:49:40,128 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:49:40,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:50:10,130 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:50:10,130 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:50:40,131 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:50:40,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-11 11:51:10,131 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:51:10,132 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:51:40,132 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:51:40,132 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:52:10,132 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:52:10,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:52:40,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:52:40,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:53:10,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:53:10,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:53:40,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:53:40,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:54:10,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:54:10,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:54:40,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:54:40,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:55:10,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:55:10,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:55:40,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:55:40,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:56:10,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:56:10,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:56:40,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:56:40,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:57:10,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:57:10,136 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:57:40,136 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:57:40,137 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:58:10,137 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:58:10,137 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 11:58:40,137 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 11:58:40,138 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:59:10,139 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 11:59:10,139 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 11:59:40,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 11:59:40,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:00:10,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:00:10,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:00:40,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:00:40,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:01:10,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:01:10,141 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:01:40,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:01:40,141 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:02:10,141 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:02:10,141 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:02:40,143 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:02:40,143 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:03:10,143 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:03:10,144 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 12:03:40,143 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:03:40,143 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:04:10,144 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:04:10,144 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:04:40,145 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:04:40,146 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 12:05:10,145 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:05:10,146 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 12:05:40,145 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:05:40,145 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:06:10,145 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:06:10,146 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:06:40,146 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:06:40,147 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:07:10,147 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:07:10,148 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:07:40,148 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:07:40,149 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:07:58,681 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-11 12:07:58,681 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-11 12:07:58,681 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1926
2015-12-11 12:07:58,682 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 166 Total time for transactions(ms): 51 Number of transactions batched in Syncs: 0 Number of syncs: 115 SyncTimes(ms): 324 
2015-12-11 12:07:58,682 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 166 Total time for transactions(ms): 51 Number of transactions batched in Syncs: 0 Number of syncs: 116 SyncTimes(ms): 324 
2015-12-11 12:07:58,683 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000001926 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000001926-0000000000000002091
2015-12-11 12:07:58,684 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 2092
2015-12-11 12:07:58,837 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 3333.33 KB/s
2015-12-11 12:07:58,838 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000002091 size 10639 bytes.
2015-12-11 12:07:58,842 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 1925
2015-12-11 12:07:58,842 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000001546, cpktTxId=0000000000000001546)
2015-12-11 12:08:10,149 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:08:10,150 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:08:40,150 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:08:40,151 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:09:10,152 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:09:10,152 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:09:40,152 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:09:40,152 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:10:10,152 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:10:10,153 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:10:40,154 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:10:40,155 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:11:10,154 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:11:10,155 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:11:40,154 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:11:40,155 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:12:10,155 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:12:10,156 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:12:40,157 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:12:40,157 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:13:10,156 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:13:10,157 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:13:40,156 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:13:40,157 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:14:10,157 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:14:10,157 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:14:40,157 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:14:40,157 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:15:10,157 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:15:10,158 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:15:40,157 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:15:40,158 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:16:10,158 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:16:10,159 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:16:40,159 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:16:40,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:17:10,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:17:10,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:17:40,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:17:40,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:18:10,162 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:18:10,162 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:18:40,163 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:18:40,163 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:19:10,163 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:19:10,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:19:40,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:19:40,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:20:10,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:20:10,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:20:40,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:20:40,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:21:10,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:21:10,167 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:21:40,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:21:40,167 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:22:10,167 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:22:10,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:22:40,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:22:40,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:23:10,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:23:10,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:23:40,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:23:40,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:24:10,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:24:10,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:24:40,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:24:40,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:25:10,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:25:10,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:25:40,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:25:40,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:26:10,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:26:10,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:26:40,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:26:40,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:27:10,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:27:10,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:27:40,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:27:40,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:28:10,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:28:10,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:28:40,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:28:40,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:29:10,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:29:10,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:29:40,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:29:40,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:30:10,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:30:10,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:30:40,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:30:40,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:31:10,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:31:10,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:31:40,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:31:40,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:32:10,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:32:10,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:32:40,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:32:40,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:33:10,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:33:10,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:33:40,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:33:40,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:34:10,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 12:34:10,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:34:40,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:34:40,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:35:10,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:35:10,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:35:40,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:35:40,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:36:10,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:36:10,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:36:40,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:36:40,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:37:10,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:37:10,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:37:40,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:37:40,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:38:10,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:38:10,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:38:40,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:38:40,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:39:10,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:39:10,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:39:40,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:39:40,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:40:10,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:40:10,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:40:40,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 12:40:40,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:41:10,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:41:10,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:41:40,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:41:40,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:42:10,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:42:10,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:42:40,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:42:40,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:43:10,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:43:10,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:43:40,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:43:40,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:44:10,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:44:10,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:44:40,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:44:40,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:45:10,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:45:10,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:45:40,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:45:40,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:46:10,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:46:10,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:46:40,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:46:40,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:47:10,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:47:10,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:47:40,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 12:47:40,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:48:10,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:48:10,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:48:40,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:48:40,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:49:10,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:49:10,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:49:40,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:49:40,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:50:10,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:50:10,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:50:40,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:50:40,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:51:10,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:51:10,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:51:40,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:51:40,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:52:10,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 12:52:10,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:52:40,203 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:52:40,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:53:10,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 12:53:10,206 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:53:40,206 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:53:40,207 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:54:10,207 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:54:10,207 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:54:40,207 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:54:40,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:55:10,209 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:55:10,209 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:55:40,209 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:55:40,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:56:10,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:56:10,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:56:40,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:56:40,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:57:10,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:57:10,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:57:40,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:57:40,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:58:10,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 12:58:10,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:58:40,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:58:40,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 12:59:10,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:59:10,216 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 12:59:40,216 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 12:59:40,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:00:10,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:00:10,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:00:40,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:00:40,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:01:10,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:01:10,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:01:40,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:01:40,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:02:10,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:02:10,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:02:40,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:02:40,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:03:10,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:03:10,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-11 13:03:40,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:03:40,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:04:10,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:04:10,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:04:40,226 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:04:40,226 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:05:10,226 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:05:10,227 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:05:40,227 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:05:40,227 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:06:10,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:06:10,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:06:40,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:06:40,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:07:10,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:07:10,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:07:40,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:07:40,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:07:59,423 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-11 13:07:59,423 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-11 13:07:59,423 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 2092
2015-12-11 13:07:59,424 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 12 
2015-12-11 13:07:59,430 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 18 
2015-12-11 13:07:59,430 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000002092 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000002092-0000000000000002093
2015-12-11 13:07:59,430 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 2094
2015-12-11 13:07:59,491 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 2000.00 KB/s
2015-12-11 13:07:59,491 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000002093 size 10639 bytes.
2015-12-11 13:07:59,494 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2091
2015-12-11 13:07:59,494 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000001925, cpktTxId=0000000000000001925)
2015-12-11 13:08:10,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:08:10,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:08:40,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:08:40,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:09:10,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:09:10,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:09:40,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:09:40,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:10:10,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:10:10,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:10:40,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:10:40,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:11:10,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:11:10,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:11:40,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:11:40,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:12:10,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:12:10,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:12:40,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:12:40,235 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:13:10,235 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:13:10,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:13:40,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:13:40,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:14:10,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:14:10,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:14:40,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:14:40,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:15:10,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:15:10,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:15:40,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:15:40,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:16:10,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:16:10,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:16:40,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:16:40,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:17:10,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:17:10,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:17:40,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:17:40,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:18:10,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 13:18:10,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:18:40,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:18:40,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:19:10,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:19:10,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:19:40,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:19:40,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 13:20:10,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:20:10,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:20:40,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:20:40,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:21:10,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:21:10,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:21:40,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:21:40,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:22:10,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:22:10,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:22:40,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:22:40,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:23:10,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:23:10,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:23:40,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:23:40,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:24:10,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:24:10,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:24:40,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:24:40,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:25:10,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 13:25:10,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:25:40,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:25:40,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:26:10,255 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:26:10,255 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:26:40,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:26:40,255 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:27:10,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:27:10,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:27:40,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:27:40,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:28:10,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:28:10,257 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:28:40,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:28:40,257 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:29:10,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 13:29:10,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:29:40,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:29:40,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:30:10,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:30:10,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:30:40,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:30:40,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:31:10,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:31:10,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:31:40,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:31:40,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:32:10,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:32:10,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:32:40,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:32:40,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:33:10,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:33:10,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:33:40,263 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:33:40,263 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:34:10,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:34:10,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:34:40,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:34:40,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:35:10,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 13:35:10,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:35:40,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:35:40,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:36:10,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:36:10,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:36:40,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:36:40,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:37:10,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:37:10,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:37:40,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:37:40,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:38:10,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:38:10,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:38:40,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:38:40,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:39:10,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:39:10,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:39:40,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:39:40,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:40:10,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:40:10,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:40:40,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:40:40,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:41:10,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:41:10,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:41:40,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:41:40,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:42:10,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:42:10,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:42:40,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:42:40,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:43:10,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:43:10,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 13:43:40,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:43:40,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:44:10,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:44:10,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:44:40,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:44:40,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:45:10,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 13:45:10,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 13:45:40,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 13:45:40,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 14:22:56,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:22:56,827 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 14:23:26,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:23:26,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:23:56,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:23:56,827 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:24:26,827 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:24:26,828 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:24:56,828 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:24:56,829 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:25:26,828 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:25:26,829 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:25:56,828 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:25:56,830 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 14:26:26,829 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:26:26,829 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:26:56,830 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:26:56,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:27:26,830 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:27:26,830 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:27:56,830 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:27:56,830 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:28:26,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:28:26,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:28:56,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:28:56,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:29:26,832 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:29:26,832 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:29:56,833 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:29:56,833 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:30:26,834 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:30:26,835 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:30:56,834 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:30:56,834 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:31:26,835 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:31:26,835 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:31:56,835 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:31:56,836 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:32:26,837 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:32:26,837 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:32:56,837 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:32:56,837 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:33:26,837 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:33:26,837 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:33:56,837 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:33:56,837 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:34:26,838 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 14:34:26,839 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:34:56,840 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:34:56,840 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:35:26,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:35:26,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:35:56,842 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:35:56,842 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:36:26,842 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:36:26,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:36:56,842 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:36:56,842 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:37:26,842 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:37:26,842 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:37:56,842 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:37:56,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:38:26,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:38:26,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:38:56,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:38:56,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:39:26,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:39:26,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:39:56,844 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:39:56,844 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:40:26,844 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:40:26,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:40:56,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:40:56,846 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:41:26,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:41:26,846 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:41:56,851 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30005 milliseconds
2015-12-11 14:41:56,855 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 4 millisecond(s).
2015-12-11 14:42:26,851 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:42:26,852 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:42:56,852 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:42:56,852 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:43:26,852 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:43:26,853 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:43:56,854 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:43:56,855 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:44:26,854 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:44:26,855 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:44:46,589 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-11 14:44:46,589 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-11 14:44:46,589 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 2094
2015-12-11 14:44:46,590 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 5 
2015-12-11 14:44:46,595 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 10 
2015-12-11 14:44:46,595 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000002094 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000002094-0000000000000002095
2015-12-11 14:44:46,596 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 2096
2015-12-11 14:44:46,649 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 3333.33 KB/s
2015-12-11 14:44:46,650 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000002095 size 10639 bytes.
2015-12-11 14:44:46,652 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2093
2015-12-11 14:44:46,653 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000002091, cpktTxId=0000000000000002091)
2015-12-11 14:44:56,855 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:44:56,856 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:45:26,855 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:45:26,855 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:45:56,855 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:45:56,855 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:46:26,855 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:46:26,856 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:46:40,819 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 5 
2015-12-11 14:46:40,821 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742077_1253 127.0.0.1:50010 
2015-12-11 14:46:42,007 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742077_1253]
2015-12-11 14:46:56,862 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30007 milliseconds
2015-12-11 14:46:56,862 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:47:26,862 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:47:26,863 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:47:56,863 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:47:56,863 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:48:26,863 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:48:26,864 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:48:56,865 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:48:56,865 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:49:26,865 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:49:26,866 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:49:56,866 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:49:56,867 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:50:26,867 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:50:26,870 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-11 14:50:56,867 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:50:56,868 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:51:26,867 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:51:26,868 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:51:56,867 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:51:56,867 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:52:26,867 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:52:26,868 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:52:56,867 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:52:56,868 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:53:26,867 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:53:26,868 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:53:56,868 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:53:56,869 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:54:26,868 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:54:26,868 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:54:56,869 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:54:56,870 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:55:26,869 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:55:26,870 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:55:56,870 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:55:56,872 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 14:56:26,871 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:56:26,872 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:56:56,871 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:56:56,872 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:57:26,871 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 14:57:26,872 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:57:56,872 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:57:56,873 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:58:26,872 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:58:26,872 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:58:56,872 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:58:56,872 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 14:59:26,872 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:59:26,872 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 14:59:56,872 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 14:59:56,873 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 15:00:26,872 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 15:00:26,874 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 15:00:56,873 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 15:00:56,874 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 15:01:26,874 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 15:01:26,875 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 15:01:56,875 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 15:01:56,876 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 15:02:26,875 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 15:02:26,875 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 15:02:56,876 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 15:02:56,876 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 15:03:26,877 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 15:03:26,877 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 15:03:56,878 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 15:03:56,879 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 15:04:26,878 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 15:04:26,878 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 15:04:56,878 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 15:04:56,878 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 15:05:26,879 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 15:05:26,879 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 15:05:56,879 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 15:05:56,879 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 15:06:26,879 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 15:06:26,880 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 15:06:56,881 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 15:06:56,881 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 15:07:26,882 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 15:07:26,883 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 15:07:56,884 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 15:07:56,884 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 15:08:26,885 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 15:08:26,886 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 15:08:56,885 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 15:08:56,886 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 15:09:26,886 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 15:09:26,886 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 15:09:56,887 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 15:09:56,887 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 15:10:26,887 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 15:10:26,887 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 15:10:56,888 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 15:10:56,889 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 15:11:26,889 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 15:11:26,890 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 15:11:56,890 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 15:11:56,891 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 15:12:26,890 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 15:12:26,890 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 15:12:56,891 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 15:12:56,892 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 15:13:26,892 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 15:13:26,892 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 15:13:56,893 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 15:13:56,894 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 15:14:26,893 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 15:14:26,894 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 15:14:56,893 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 15:14:56,893 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 15:15:26,894 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 15:15:26,895 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 17:15:44,651 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 78, hasStaleStorages: false, processing time: 54 msecs
2015-12-11 17:15:51,041 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 17:15:51,042 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 17:16:21,041 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 17:16:21,041 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 18:16:46,349 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 18:16:46,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 18:17:16,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 18:17:16,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 18:17:46,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 18:17:46,353 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 18:18:16,354 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 18:18:16,354 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 19:47:53,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 19:47:53,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 19:48:23,130 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 19:48:23,130 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 19:54:43,096 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 19:54:43,096 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 19:55:13,096 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 19:55:13,096 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 19:55:43,096 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 19:55:43,097 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 19:56:13,097 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 19:56:13,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 19:56:43,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 19:56:43,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 19:57:13,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 19:57:13,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 19:57:43,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 19:57:43,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 19:58:13,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 19:58:13,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 19:58:43,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 19:58:43,101 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 19:59:13,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 19:59:13,100 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 19:59:43,100 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 19:59:43,100 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 20:00:13,100 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 20:00:13,100 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 20:00:43,102 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 20:00:43,102 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 20:01:13,102 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 20:01:13,103 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 20:01:43,103 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 20:01:43,104 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 20:02:13,105 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 20:02:13,105 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 20:02:43,105 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 20:02:43,106 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 20:03:13,106 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 20:03:13,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 20:03:43,106 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 20:03:43,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 20:04:13,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 20:04:13,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 20:04:43,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 20:04:43,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 20:05:13,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 20:05:13,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 20:05:43,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 20:05:43,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 20:06:13,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 20:06:13,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 20:06:43,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 20:06:43,110 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 20:07:13,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 20:07:13,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 20:07:43,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 20:07:43,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 22:07:54,169 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* removeDeadDatanode: lost heartbeat from 127.0.0.1:50010
2015-12-11 22:07:54,198 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/127.0.0.1:50010
2015-12-11 22:07:55,301 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0) storage 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-11 22:07:55,301 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/127.0.0.1:50010
2015-12-11 22:07:55,301 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-11 22:07:55,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-11 22:07:55,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: [DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010 failed.
2015-12-11 22:07:55,305 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-11 22:07:55,310 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 78, hasStaleStorages: false, processing time: 2 msecs
2015-12-11 22:08:07,549 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 22:08:07,550 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 22:08:37,550 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 22:08:37,550 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 22:09:07,552 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 22:09:07,552 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 22:09:37,551 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 22:09:37,552 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:26:35,093 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* removeDeadDatanode: lost heartbeat from 127.0.0.1:50010
2015-12-11 23:26:35,096 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/127.0.0.1:50010
2015-12-11 23:26:36,222 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0) storage 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-11 23:26:36,223 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/127.0.0.1:50010
2015-12-11 23:26:36,223 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-11 23:26:36,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-11 23:26:36,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: [DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010 failed.
2015-12-11 23:26:36,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-11 23:26:36,229 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 78, hasStaleStorages: false, processing time: 3 msecs
2015-12-11 23:26:49,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:26:49,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:27:19,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:27:19,067 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:27:49,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:27:49,067 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:28:19,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:28:19,067 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:28:49,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:28:49,067 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:29:19,067 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:29:19,070 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-11 23:29:49,067 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:29:49,068 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:30:19,068 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:30:19,069 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 23:30:49,068 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:30:49,068 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:31:19,069 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:31:19,069 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:31:49,068 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:31:49,069 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:32:12,502 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 7 
2015-12-11 23:32:12,619 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0009/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742081_1257{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:32:12,750 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742081_1257{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:32:12,753 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0009/job.jar is closed by DFSClient_NONMAPREDUCE_526466774_1
2015-12-11 23:32:12,758 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0009/job.jar
2015-12-11 23:32:12,830 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0009/job.split
2015-12-11 23:32:12,846 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0009/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742082_1258{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:32:12,857 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742082_1258{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:32:12,859 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0009/job.split is closed by DFSClient_NONMAPREDUCE_526466774_1
2015-12-11 23:32:12,872 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0009/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742083_1259{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:32:12,879 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742083_1259{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:32:12,881 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0009/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_526466774_1
2015-12-11 23:32:13,133 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0009/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742084_1260{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:32:13,150 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742084_1260{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:32:13,152 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0009/job.xml is closed by DFSClient_NONMAPREDUCE_526466774_1
2015-12-11 23:32:19,068 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:32:19,071 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-11 23:32:21,432 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0009/job_1449846414359_0009_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742085_1261{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:32:21,572 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742085_1261{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:32:21,596 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0009/job_1449846414359_0009_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1082452810_1
2015-12-11 23:32:34,915 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0009/job_1449846414359_0009_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742086_1262{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:32:35,056 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0009/job_1449846414359_0009_1.jhist for DFSClient_NONMAPREDUCE_-1082452810_1
2015-12-11 23:32:49,072 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30004 milliseconds
2015-12-11 23:32:49,077 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 5 millisecond(s).
2015-12-11 23:33:05,810 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742086_1262{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13459
2015-12-11 23:33:05,817 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0009/job_1449846414359_0009_1.jhist is closed by DFSClient_NONMAPREDUCE_-1082452810_1
2015-12-11 23:33:05,864 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0009.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742087_1263{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:33:05,895 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742087_1263{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-11 23:33:05,895 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742087_1263{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 340
2015-12-11 23:33:06,300 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0009.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1082452810_1
2015-12-11 23:33:06,385 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0009-1449894733448-parallels-chess+taskA-1449894785403-0-0-FAILED-default-1449894740893.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742088_1264{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:33:06,404 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742088_1264{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:33:06,417 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0009-1449894733448-parallels-chess+taskA-1449894785403-0-0-FAILED-default-1449894740893.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1082452810_1
2015-12-11 23:33:06,439 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0009_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742089_1265{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:33:06,454 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742089_1265{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:33:06,456 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0009_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1082452810_1
2015-12-11 23:33:07,545 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742081_1257 127.0.0.1:50010 
2015-12-11 23:33:07,546 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742082_1258 127.0.0.1:50010 
2015-12-11 23:33:07,546 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742083_1259 127.0.0.1:50010 
2015-12-11 23:33:07,546 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742084_1260 127.0.0.1:50010 
2015-12-11 23:33:07,546 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742086_1262 127.0.0.1:50010 
2015-12-11 23:33:07,546 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742085_1261 127.0.0.1:50010 
2015-12-11 23:33:07,840 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742081_1257, blk_1073742082_1258, blk_1073742083_1259, blk_1073742084_1260, blk_1073742085_1261, blk_1073742086_1262]
2015-12-11 23:33:15,382 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 71 Total time for transactions(ms): 16 Number of transactions batched in Syncs: 1 Number of syncs: 48 SyncTimes(ms): 288 
2015-12-11 23:33:19,073 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:33:19,073 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:33:49,073 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:33:49,073 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:34:09,961 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0010/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742090_1266{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:34:10,086 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742090_1266{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:34:10,090 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0010/job.jar is closed by DFSClient_NONMAPREDUCE_-1170460364_1
2015-12-11 23:34:10,095 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0010/job.jar
2015-12-11 23:34:10,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0010/job.split
2015-12-11 23:34:10,194 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0010/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742091_1267{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:34:10,210 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742091_1267{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:34:10,213 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0010/job.split is closed by DFSClient_NONMAPREDUCE_-1170460364_1
2015-12-11 23:34:10,228 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0010/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742092_1268{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:34:10,239 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742092_1268{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:34:10,251 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0010/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1170460364_1
2015-12-11 23:34:10,499 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0010/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742093_1269{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:34:10,522 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742093_1269{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:34:10,525 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0010/job.xml is closed by DFSClient_NONMAPREDUCE_-1170460364_1
2015-12-11 23:34:15,452 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 100 Total time for transactions(ms): 17 Number of transactions batched in Syncs: 1 Number of syncs: 69 SyncTimes(ms): 307 
2015-12-11 23:34:15,835 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0010/job_1449846414359_0010_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742094_1270{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:34:15,938 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742094_1270{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:34:15,945 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0010/job_1449846414359_0010_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1317231136_1
2015-12-11 23:34:19,072 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:34:19,073 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:34:33,174 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4635ms
No GCs detected
2015-12-11 23:34:34,116 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0010/job_1449846414359_0010_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742095_1271{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:34:34,231 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0010/job_1449846414359_0010_1.jhist for DFSClient_NONMAPREDUCE_1317231136_1
2015-12-11 23:34:49,076 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30004 milliseconds
2015-12-11 23:34:49,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 23:35:07,658 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742095_1271{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13660
2015-12-11 23:35:07,732 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0010/job_1449846414359_0010_1.jhist is closed by DFSClient_NONMAPREDUCE_1317231136_1
2015-12-11 23:35:07,771 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0010.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742096_1272{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:35:07,798 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742096_1272{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:35:07,802 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0010.summary_tmp is closed by DFSClient_NONMAPREDUCE_1317231136_1
2015-12-11 23:35:07,870 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0010-1449894850740-parallels-chess+taskA-1449894907393-0-0-FAILED-default-1449894855433.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742097_1273{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:35:07,882 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742097_1273{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:35:07,886 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0010-1449894850740-parallels-chess+taskA-1449894907393-0-0-FAILED-default-1449894855433.jhist_tmp is closed by DFSClient_NONMAPREDUCE_1317231136_1
2015-12-11 23:35:07,912 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0010_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742098_1274{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:35:07,936 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742098_1274{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:35:07,942 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0010_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_1317231136_1
2015-12-11 23:35:09,164 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742090_1266 127.0.0.1:50010 
2015-12-11 23:35:09,164 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742091_1267 127.0.0.1:50010 
2015-12-11 23:35:09,164 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742092_1268 127.0.0.1:50010 
2015-12-11 23:35:09,164 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742093_1269 127.0.0.1:50010 
2015-12-11 23:35:09,165 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742095_1271 127.0.0.1:50010 
2015-12-11 23:35:09,165 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742094_1270 127.0.0.1:50010 
2015-12-11 23:35:12,095 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742090_1266, blk_1073742091_1267, blk_1073742092_1268, blk_1073742093_1269, blk_1073742094_1270, blk_1073742095_1271]
2015-12-11 23:35:19,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 23:35:19,079 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:35:49,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:35:49,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:36:09,685 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-11 23:36:09,686 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-11 23:36:09,686 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 2096
2015-12-11 23:36:09,686 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 137 Total time for transactions(ms): 49 Number of transactions batched in Syncs: 1 Number of syncs: 94 SyncTimes(ms): 481 
2015-12-11 23:36:09,692 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 137 Total time for transactions(ms): 49 Number of transactions batched in Syncs: 1 Number of syncs: 95 SyncTimes(ms): 486 
2015-12-11 23:36:09,696 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000002096 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000002096-0000000000000002232
2015-12-11 23:36:09,698 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 2233
2015-12-11 23:36:10,110 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 3333.33 KB/s
2015-12-11 23:36:10,112 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000002232 size 11157 bytes.
2015-12-11 23:36:10,119 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2095
2015-12-11 23:36:10,119 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000002093, cpktTxId=0000000000000002093)
2015-12-11 23:36:19,079 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:36:19,079 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:36:49,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:36:49,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:37:19,081 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:37:19,081 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:37:49,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:37:49,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:38:19,083 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:38:19,083 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:38:49,083 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:38:49,084 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:39:19,084 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:39:19,085 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:39:49,085 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:39:49,086 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:40:19,085 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:40:19,086 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:40:49,085 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:40:49,086 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:41:19,085 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:41:19,085 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:41:49,086 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:41:49,086 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:42:19,085 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:42:19,086 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:42:49,085 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:42:49,086 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:43:19,086 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:43:19,086 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:43:49,086 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:43:49,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:44:19,086 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:44:19,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:44:49,086 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:44:49,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:45:19,086 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:45:19,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:45:49,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:45:49,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:46:19,086 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:46:19,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:46:49,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:46:49,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:47:19,088 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:47:19,088 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:47:49,089 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:47:49,090 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:48:19,091 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:48:19,091 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:48:49,090 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:48:49,091 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:49:19,090 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:49:19,091 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:49:49,091 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:49:49,092 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:50:08,734 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 32 
2015-12-11 23:50:08,859 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0011/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742099_1275{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:50:08,992 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742099_1275{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:50:08,996 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0011/job.jar is closed by DFSClient_NONMAPREDUCE_1942254671_1
2015-12-11 23:50:09,000 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0011/job.jar
2015-12-11 23:50:09,071 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0011/job.split
2015-12-11 23:50:09,087 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0011/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742100_1276{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:50:09,098 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742100_1276{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:50:09,102 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0011/job.split is closed by DFSClient_NONMAPREDUCE_1942254671_1
2015-12-11 23:50:09,119 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0011/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742101_1277{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:50:09,125 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742101_1277{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:50:09,128 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0011/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1942254671_1
2015-12-11 23:50:09,392 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0011/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742102_1278{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:50:09,419 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742102_1278{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:50:09,425 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0011/job.xml is closed by DFSClient_NONMAPREDUCE_1942254671_1
2015-12-11 23:50:16,039 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0011/job_1449846414359_0011_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742103_1279{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:50:16,203 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742103_1279{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:50:16,211 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0011/job_1449846414359_0011_1_conf.xml is closed by DFSClient_NONMAPREDUCE_519660369_1
2015-12-11 23:50:19,092 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:50:19,093 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:50:29,397 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0011/job_1449846414359_0011_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742104_1280{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:50:29,477 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0011/job_1449846414359_0011_1.jhist for DFSClient_NONMAPREDUCE_519660369_1
2015-12-11 23:50:49,093 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:50:49,097 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 4 millisecond(s).
2015-12-11 23:51:04,714 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742104_1280{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13660
2015-12-11 23:51:04,730 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0011/job_1449846414359_0011_1.jhist is closed by DFSClient_NONMAPREDUCE_519660369_1
2015-12-11 23:51:04,772 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0011.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742105_1281{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:51:04,787 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742105_1281{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:51:04,792 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0011.summary_tmp is closed by DFSClient_NONMAPREDUCE_519660369_1
2015-12-11 23:51:04,884 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0011-1449895809676-parallels-chess+taskA-1449895864324-0-0-FAILED-default-1449895815423.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742106_1282{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:51:04,895 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742106_1282{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:51:04,910 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0011-1449895809676-parallels-chess+taskA-1449895864324-0-0-FAILED-default-1449895815423.jhist_tmp is closed by DFSClient_NONMAPREDUCE_519660369_1
2015-12-11 23:51:04,955 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0011_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742107_1283{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:51:04,974 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742107_1283{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:51:04,978 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0011_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_519660369_1
2015-12-11 23:51:06,080 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742099_1275 127.0.0.1:50010 
2015-12-11 23:51:06,081 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742100_1276 127.0.0.1:50010 
2015-12-11 23:51:06,081 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742101_1277 127.0.0.1:50010 
2015-12-11 23:51:06,081 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742102_1278 127.0.0.1:50010 
2015-12-11 23:51:06,081 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742104_1280 127.0.0.1:50010 
2015-12-11 23:51:06,081 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742103_1279 127.0.0.1:50010 
2015-12-11 23:51:06,302 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742099_1275, blk_1073742100_1276, blk_1073742101_1277, blk_1073742102_1278, blk_1073742103_1279, blk_1073742104_1280]
2015-12-11 23:51:19,093 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:51:19,094 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:51:49,094 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:51:49,095 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:52:19,096 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 23:52:19,096 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:52:49,097 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:52:49,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:53:03,317 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 67 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 0 Number of syncs: 47 SyncTimes(ms): 346 
2015-12-11 23:53:08,782 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0012/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742108_1284{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:53:08,914 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742108_1284{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:53:08,918 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0012/job.jar is closed by DFSClient_NONMAPREDUCE_-501274724_1
2015-12-11 23:53:08,926 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0012/job.jar
2015-12-11 23:53:09,008 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0012/job.split
2015-12-11 23:53:09,023 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0012/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742109_1285{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:53:09,032 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742109_1285{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:53:09,041 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0012/job.split is closed by DFSClient_NONMAPREDUCE_-501274724_1
2015-12-11 23:53:09,056 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0012/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742110_1286{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:53:09,063 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742110_1286{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:53:09,067 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0012/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-501274724_1
2015-12-11 23:53:09,310 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0012/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742111_1287{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:53:09,334 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742111_1287{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:53:09,337 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0012/job.xml is closed by DFSClient_NONMAPREDUCE_-501274724_1
2015-12-11 23:53:14,429 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0012/job_1449846414359_0012_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742112_1288{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:53:14,536 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742112_1288{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:53:14,541 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0012/job_1449846414359_0012_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1443065067_1
2015-12-11 23:53:19,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:53:19,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:53:28,326 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0012/job_1449846414359_0012_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742113_1289{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:53:28,363 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0012/job_1449846414359_0012_1.jhist for DFSClient_NONMAPREDUCE_1443065067_1
2015-12-11 23:53:49,103 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30005 milliseconds
2015-12-11 23:53:49,105 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 23:54:02,091 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742113_1289{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-11 23:54:02,092 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742113_1289{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 45077
2015-12-11 23:54:02,502 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0012/job_1449846414359_0012_1.jhist is closed by DFSClient_NONMAPREDUCE_1443065067_1
2015-12-11 23:54:02,547 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0012.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742114_1290{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:54:02,566 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742114_1290{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:54:02,595 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0012.summary_tmp is closed by DFSClient_NONMAPREDUCE_1443065067_1
2015-12-11 23:54:02,691 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0012-1449895989565-parallels-chess+taskA-1449896041696-0-0-FAILED-default-1449895994050.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742115_1291{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:54:02,709 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742115_1291{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-11 23:54:02,715 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0012-1449895989565-parallels-chess+taskA-1449896041696-0-0-FAILED-default-1449895994050.jhist_tmp is closed by DFSClient_NONMAPREDUCE_1443065067_1
2015-12-11 23:54:02,739 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0012_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742116_1292{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-11 23:54:02,753 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742116_1292{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-11 23:54:02,754 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742116_1292{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 103822
2015-12-11 23:54:03,157 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0012_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_1443065067_1
2015-12-11 23:54:04,222 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 132 Total time for transactions(ms): 28 Number of transactions batched in Syncs: 2 Number of syncs: 92 SyncTimes(ms): 492 
2015-12-11 23:54:04,223 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742108_1284 127.0.0.1:50010 
2015-12-11 23:54:04,223 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742109_1285 127.0.0.1:50010 
2015-12-11 23:54:04,224 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742110_1286 127.0.0.1:50010 
2015-12-11 23:54:04,224 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742111_1287 127.0.0.1:50010 
2015-12-11 23:54:04,224 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742113_1289 127.0.0.1:50010 
2015-12-11 23:54:04,224 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742112_1288 127.0.0.1:50010 
2015-12-11 23:54:06,350 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742112_1288, blk_1073742113_1289, blk_1073742108_1284, blk_1073742109_1285, blk_1073742110_1286, blk_1073742111_1287]
2015-12-11 23:54:19,105 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-11 23:54:19,106 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:54:49,106 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:54:49,106 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:55:19,106 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:55:19,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:55:49,106 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:55:49,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-11 23:56:19,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-11 23:56:19,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:56:49,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:56:49,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-11 23:57:19,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:57:19,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:57:49,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:57:49,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:58:19,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:58:19,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:58:49,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:58:49,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:59:19,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:59:19,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-11 23:59:49,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-11 23:59:49,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:00:19,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:00:19,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:00:49,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:00:49,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:01:19,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:01:19,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:01:49,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:01:49,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:02:19,110 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 00:02:19,110 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:02:49,111 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:02:49,111 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:03:19,112 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:03:19,113 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:03:49,114 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:03:49,115 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:04:19,114 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:04:19,114 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:04:49,115 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:04:49,115 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:05:19,114 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:05:19,115 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:05:49,114 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:05:49,115 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:06:19,115 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:06:19,116 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:06:49,116 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:06:49,117 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:07:19,117 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:07:19,118 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:07:49,118 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:07:49,118 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:08:19,118 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:08:19,119 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:08:49,119 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:08:49,120 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:09:19,121 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 00:09:19,122 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:09:49,122 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:09:49,123 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:10:19,124 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 00:10:19,124 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:10:49,125 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:10:49,125 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:11:19,127 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:11:19,127 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:11:49,126 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:11:49,127 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:12:19,126 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:12:19,126 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:12:49,127 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:12:49,128 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:13:19,128 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:13:19,128 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:13:49,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:13:49,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:14:19,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:14:19,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:14:49,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:14:49,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:15:19,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:15:19,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:15:49,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:15:49,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:16:19,130 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:16:19,130 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:16:49,130 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:16:49,131 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:17:19,131 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:17:19,131 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:17:49,130 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:17:49,131 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:18:19,131 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:18:19,131 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:18:49,132 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:18:49,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:19:19,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:19:19,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:19:49,132 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:19:49,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:20:19,132 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:20:19,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:20:49,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:20:49,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:21:19,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:21:19,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:21:34,453 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 133 Total time for transactions(ms): 29 Number of transactions batched in Syncs: 2 Number of syncs: 93 SyncTimes(ms): 492 
2015-12-12 00:21:49,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:21:49,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:22:19,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:22:19,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:22:49,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:22:49,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:23:19,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:23:19,136 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:23:49,136 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:23:49,136 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:24:08,898 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 134 Total time for transactions(ms): 29 Number of transactions batched in Syncs: 2 Number of syncs: 94 SyncTimes(ms): 497 
2015-12-12 00:24:09,040 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0013/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742117_1293{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:24:09,214 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742117_1293{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:24:09,218 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0013/job.jar is closed by DFSClient_NONMAPREDUCE_1171860838_1
2015-12-12 00:24:09,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0013/job.jar
2015-12-12 00:24:09,309 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0013/job.split
2015-12-12 00:24:09,328 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0013/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742118_1294{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:24:09,340 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742118_1294{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:24:09,343 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0013/job.split is closed by DFSClient_NONMAPREDUCE_1171860838_1
2015-12-12 00:24:09,354 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0013/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742119_1295{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:24:09,362 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742119_1295{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:24:09,365 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0013/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1171860838_1
2015-12-12 00:24:09,613 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0013/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742120_1296{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:24:09,630 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742120_1296{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:24:09,633 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0013/job.xml is closed by DFSClient_NONMAPREDUCE_1171860838_1
2015-12-12 00:24:15,694 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0013/job_1449846414359_0013_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742121_1297{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:24:15,838 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742121_1297{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:24:15,843 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0013/job_1449846414359_0013_1_conf.xml is closed by DFSClient_NONMAPREDUCE_2015500094_1
2015-12-12 00:24:19,136 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:24:19,136 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:24:29,119 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0013/job_1449846414359_0013_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742122_1298{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:24:29,191 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0013/job_1449846414359_0013_1.jhist for DFSClient_NONMAPREDUCE_2015500094_1
2015-12-12 00:24:49,150 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30014 milliseconds
2015-12-12 00:24:49,153 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 00:25:01,073 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742122_1298{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-12 00:25:01,078 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742122_1298{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 44683
2015-12-12 00:25:01,484 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0013/job_1449846414359_0013_1.jhist is closed by DFSClient_NONMAPREDUCE_2015500094_1
2015-12-12 00:25:01,525 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0013.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742123_1299{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:25:01,571 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742123_1299{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:25:01,594 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0013.summary_tmp is closed by DFSClient_NONMAPREDUCE_2015500094_1
2015-12-12 00:25:01,714 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0013-1449897849911-parallels-chess+taskA-1449897900759-0-0-FAILED-default-1449897855299.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742124_1300{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:25:01,734 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742124_1300{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:25:01,736 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0013-1449897849911-parallels-chess+taskA-1449897900759-0-0-FAILED-default-1449897855299.jhist_tmp is closed by DFSClient_NONMAPREDUCE_2015500094_1
2015-12-12 00:25:01,765 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0013_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742125_1301{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:25:01,779 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742125_1301{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:25:01,783 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0013_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_2015500094_1
2015-12-12 00:25:02,893 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742117_1293 127.0.0.1:50010 
2015-12-12 00:25:02,893 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742118_1294 127.0.0.1:50010 
2015-12-12 00:25:02,894 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742119_1295 127.0.0.1:50010 
2015-12-12 00:25:02,894 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742120_1296 127.0.0.1:50010 
2015-12-12 00:25:02,894 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742122_1298 127.0.0.1:50010 
2015-12-12 00:25:02,894 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742121_1297 127.0.0.1:50010 
2015-12-12 00:25:03,920 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742117_1293, blk_1073742118_1294, blk_1073742119_1295, blk_1073742120_1296, blk_1073742121_1297, blk_1073742122_1298]
2015-12-12 00:25:19,153 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-12 00:25:19,154 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:25:21,832 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 199 Total time for transactions(ms): 46 Number of transactions batched in Syncs: 3 Number of syncs: 139 SyncTimes(ms): 607 
2015-12-12 00:25:27,422 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0014/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742126_1302{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:25:27,564 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742126_1302{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:25:27,573 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0014/job.jar is closed by DFSClient_NONMAPREDUCE_-905324509_1
2015-12-12 00:25:27,578 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0014/job.jar
2015-12-12 00:25:27,640 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0014/job.split
2015-12-12 00:25:27,659 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0014/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742127_1303{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:25:27,665 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742127_1303{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:25:27,672 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0014/job.split is closed by DFSClient_NONMAPREDUCE_-905324509_1
2015-12-12 00:25:27,689 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0014/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742128_1304{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:25:27,696 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742128_1304{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-12 00:25:27,697 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742128_1304{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 52
2015-12-12 00:25:28,100 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0014/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-905324509_1
2015-12-12 00:25:28,356 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0014/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742129_1305{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:25:28,373 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742129_1305{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:25:28,376 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0014/job.xml is closed by DFSClient_NONMAPREDUCE_-905324509_1
2015-12-12 00:25:33,853 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0014/job_1449846414359_0014_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742130_1306{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:25:33,986 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742130_1306{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:25:33,990 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0014/job_1449846414359_0014_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-654609605_1
2015-12-12 00:25:46,069 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0014/job_1449846414359_0014_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742131_1307{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:25:46,108 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0014/job_1449846414359_0014_1.jhist for DFSClient_NONMAPREDUCE_-654609605_1
2015-12-12 00:25:49,154 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:25:49,154 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:26:01,471 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_a/_temporary/1/_temporary/attempt_1449846414359_0014_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742132_1308{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:26:02,038 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742132_1308{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:26:02,045 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_a/_temporary/1/_temporary/attempt_1449846414359_0014_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449846414359_0014_r_000000_0_1769165486_1
2015-12-12 00:26:02,318 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0014/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-654609605_1
2015-12-12 00:26:02,384 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_a/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-654609605_1
2015-12-12 00:26:02,392 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0014/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-654609605_1
2015-12-12 00:26:02,432 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742131_1307{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14260
2015-12-12 00:26:02,439 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0014/job_1449846414359_0014_1.jhist is closed by DFSClient_NONMAPREDUCE_-654609605_1
2015-12-12 00:26:02,474 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0014.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742133_1309{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:26:02,490 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742133_1309{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-12 00:26:02,491 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742133_1309{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 355
2015-12-12 00:26:02,897 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0014.summary_tmp is closed by DFSClient_NONMAPREDUCE_-654609605_1
2015-12-12 00:26:02,954 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0014-1449897928630-parallels-chess+taskA-1449897962394-3-1-SUCCEEDED-default-1449897933525.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742134_1310{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:26:02,975 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742134_1310{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:26:02,981 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0014-1449897928630-parallels-chess+taskA-1449897962394-3-1-SUCCEEDED-default-1449897933525.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-654609605_1
2015-12-12 00:26:03,011 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0014_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742135_1311{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:26:03,027 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742135_1311{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:26:03,031 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0014_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-654609605_1
2015-12-12 00:26:04,115 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742126_1302 127.0.0.1:50010 
2015-12-12 00:26:04,116 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742127_1303 127.0.0.1:50010 
2015-12-12 00:26:04,117 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742128_1304 127.0.0.1:50010 
2015-12-12 00:26:04,117 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742129_1305 127.0.0.1:50010 
2015-12-12 00:26:04,117 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742131_1307 127.0.0.1:50010 
2015-12-12 00:26:04,117 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742130_1306 127.0.0.1:50010 
2015-12-12 00:26:06,929 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742128_1304, blk_1073742129_1305, blk_1073742130_1306, blk_1073742131_1307, blk_1073742126_1302, blk_1073742127_1303]
2015-12-12 00:26:19,154 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:26:19,155 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:26:49,155 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:26:49,155 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:27:19,154 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:27:19,155 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:27:49,156 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:27:49,156 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:28:19,156 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:28:19,156 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:28:49,157 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:28:49,158 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:29:19,158 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:29:19,158 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:29:49,159 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:29:49,159 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:30:19,158 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:30:19,159 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:30:31,358 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 280 Total time for transactions(ms): 59 Number of transactions batched in Syncs: 5 Number of syncs: 196 SyncTimes(ms): 648 
2015-12-12 00:30:31,358 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742132_1308 127.0.0.1:50010 
2015-12-12 00:30:33,978 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742132_1308]
2015-12-12 00:30:36,900 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0015/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742136_1312{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:30:37,020 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742136_1312{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:30:37,025 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0015/job.jar is closed by DFSClient_NONMAPREDUCE_1326460505_1
2015-12-12 00:30:37,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0015/job.jar
2015-12-12 00:30:37,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0015/job.split
2015-12-12 00:30:37,258 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0015/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742137_1313{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:30:37,266 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742137_1313{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:30:37,268 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0015/job.split is closed by DFSClient_NONMAPREDUCE_1326460505_1
2015-12-12 00:30:37,283 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0015/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742138_1314{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:30:37,291 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742138_1314{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:30:37,296 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0015/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1326460505_1
2015-12-12 00:30:37,477 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0015/job.xml is closed by DFSClient_NONMAPREDUCE_1326460505_1
2015-12-12 00:30:49,159 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:30:49,159 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:31:13,255 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0016/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742139_1315{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:31:13,372 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742139_1315{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:31:13,377 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0016/job.jar is closed by DFSClient_NONMAPREDUCE_-867980133_1
2015-12-12 00:31:13,383 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0016/job.jar
2015-12-12 00:31:13,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0016/job.split
2015-12-12 00:31:13,470 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0016/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742140_1316{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:31:13,483 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742140_1316{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:31:13,486 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0016/job.split is closed by DFSClient_NONMAPREDUCE_-867980133_1
2015-12-12 00:31:13,501 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0016/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742141_1317{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:31:13,508 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742141_1317{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:31:13,510 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0016/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-867980133_1
2015-12-12 00:31:13,703 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0016/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742142_1318{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:31:13,715 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742142_1318{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:31:13,718 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0016/job.xml is closed by DFSClient_NONMAPREDUCE_-867980133_1
2015-12-12 00:31:19,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:31:19,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:31:20,314 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0016/job_1449846414359_0016_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742143_1319{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:31:20,451 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742143_1319{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:31:20,456 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0016/job_1449846414359_0016_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-237712542_1
2015-12-12 00:31:34,571 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0016/job_1449846414359_0016_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742144_1320{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:31:34,577 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 345 Total time for transactions(ms): 66 Number of transactions batched in Syncs: 5 Number of syncs: 241 SyncTimes(ms): 687 
2015-12-12 00:31:34,863 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0016/job_1449846414359_0016_1.jhist for DFSClient_NONMAPREDUCE_-237712542_1
2015-12-12 00:31:43,659 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_a/_temporary/1/_temporary/attempt_1449846414359_0016_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742145_1321{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:31:44,131 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742145_1321{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:31:44,137 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_a/_temporary/1/_temporary/attempt_1449846414359_0016_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449846414359_0016_r_000000_0_-1503813752_1
2015-12-12 00:31:44,443 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0016/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-237712542_1
2015-12-12 00:31:44,492 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_a/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-237712542_1
2015-12-12 00:31:44,500 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0016/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-237712542_1
2015-12-12 00:31:44,540 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742144_1320{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12221
2015-12-12 00:31:44,542 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0016/job_1449846414359_0016_1.jhist is closed by DFSClient_NONMAPREDUCE_-237712542_1
2015-12-12 00:31:44,557 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0016.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742146_1322{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:31:44,563 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742146_1322{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:31:44,567 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0016.summary_tmp is closed by DFSClient_NONMAPREDUCE_-237712542_1
2015-12-12 00:31:44,624 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0016-1449898273956-parallels-chess+taskA-1449898304502-3-1-SUCCEEDED-default-1449898279893.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742147_1323{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:31:44,632 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742147_1323{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:31:44,637 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0016-1449898273956-parallels-chess+taskA-1449898304502-3-1-SUCCEEDED-default-1449898279893.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-237712542_1
2015-12-12 00:31:44,665 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0016_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742148_1324{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:31:44,678 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742148_1324{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:31:44,687 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0016_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-237712542_1
2015-12-12 00:31:45,760 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742139_1315 127.0.0.1:50010 
2015-12-12 00:31:45,761 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742140_1316 127.0.0.1:50010 
2015-12-12 00:31:45,761 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742141_1317 127.0.0.1:50010 
2015-12-12 00:31:45,761 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742142_1318 127.0.0.1:50010 
2015-12-12 00:31:45,761 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742144_1320 127.0.0.1:50010 
2015-12-12 00:31:45,761 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742143_1319 127.0.0.1:50010 
2015-12-12 00:31:46,064 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742144_1320, blk_1073742139_1315, blk_1073742140_1316, blk_1073742141_1317, blk_1073742142_1318, blk_1073742143_1319]
2015-12-12 00:31:49,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:31:49,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:32:19,159 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:32:19,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:32:49,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:32:49,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:33:19,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:33:19,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:33:49,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:33:49,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:34:19,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:34:19,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:34:38,300 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 386 Total time for transactions(ms): 72 Number of transactions batched in Syncs: 5 Number of syncs: 272 SyncTimes(ms): 830 
2015-12-12 00:34:38,301 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742145_1321 127.0.0.1:50010 
2015-12-12 00:34:40,115 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742145_1321]
2015-12-12 00:34:44,066 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0017/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742149_1325{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:34:44,186 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742149_1325{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:34:44,196 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0017/job.jar is closed by DFSClient_NONMAPREDUCE_-1535496488_1
2015-12-12 00:34:44,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0017/job.jar
2015-12-12 00:34:44,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0017/job.split
2015-12-12 00:34:44,288 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0017/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742150_1326{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:34:44,295 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742150_1326{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:34:44,299 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0017/job.split is closed by DFSClient_NONMAPREDUCE_-1535496488_1
2015-12-12 00:34:44,330 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0017/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742151_1327{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:34:44,340 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742151_1327{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:34:44,343 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0017/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1535496488_1
2015-12-12 00:34:44,522 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0017/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742152_1328{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:34:44,537 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742152_1328{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:34:44,539 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0017/job.xml is closed by DFSClient_NONMAPREDUCE_-1535496488_1
2015-12-12 00:34:49,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:34:49,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:34:49,848 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0017/job_1449846414359_0017_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742153_1329{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:34:49,971 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742153_1329{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:34:49,982 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0017/job_1449846414359_0017_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1729789510_1
2015-12-12 00:35:02,197 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1118ms
No GCs detected
2015-12-12 00:35:03,892 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0017/job_1449846414359_0017_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742154_1330{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:35:04,023 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0017/job_1449846414359_0017_1.jhist for DFSClient_NONMAPREDUCE_1729789510_1
2015-12-12 00:35:19,162 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 00:35:19,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 15 millisecond(s).
2015-12-12 00:35:36,835 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742154_1330{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-12 00:35:36,841 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742154_1330{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 43189
2015-12-12 00:35:37,246 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0017/job_1449846414359_0017_1.jhist is closed by DFSClient_NONMAPREDUCE_1729789510_1
2015-12-12 00:35:37,283 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0017.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742155_1331{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:35:37,292 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742155_1331{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-12 00:35:37,292 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742155_1331{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 340
2015-12-12 00:35:37,696 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0017.summary_tmp is closed by DFSClient_NONMAPREDUCE_1729789510_1
2015-12-12 00:35:37,782 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0017-1449898484790-parallels-chess+taskA-1449898536381-0-0-FAILED-default-1449898489418.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742156_1332{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:35:37,802 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742156_1332{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:35:37,818 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0017-1449898484790-parallels-chess+taskA-1449898536381-0-0-FAILED-default-1449898489418.jhist_tmp is closed by DFSClient_NONMAPREDUCE_1729789510_1
2015-12-12 00:35:37,856 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0017_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742157_1333{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:35:37,868 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742157_1333{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:35:37,873 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0017_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_1729789510_1
2015-12-12 00:35:38,954 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 454 Total time for transactions(ms): 88 Number of transactions batched in Syncs: 7 Number of syncs: 317 SyncTimes(ms): 1124 
2015-12-12 00:35:38,955 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742149_1325 127.0.0.1:50010 
2015-12-12 00:35:38,956 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742150_1326 127.0.0.1:50010 
2015-12-12 00:35:38,956 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742151_1327 127.0.0.1:50010 
2015-12-12 00:35:38,956 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742152_1328 127.0.0.1:50010 
2015-12-12 00:35:38,956 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742154_1330 127.0.0.1:50010 
2015-12-12 00:35:38,957 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742153_1329 127.0.0.1:50010 
2015-12-12 00:35:41,165 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742149_1325, blk_1073742150_1326, blk_1073742151_1327, blk_1073742152_1328, blk_1073742153_1329, blk_1073742154_1330]
2015-12-12 00:35:49,163 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:35:49,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 00:36:11,700 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-12 00:36:11,700 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-12 00:36:11,700 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 2233
2015-12-12 00:36:11,701 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 456 Total time for transactions(ms): 88 Number of transactions batched in Syncs: 7 Number of syncs: 320 SyncTimes(ms): 1124 
2015-12-12 00:36:11,704 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000002233 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000002233-0000000000000002688
2015-12-12 00:36:11,705 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 2689
2015-12-12 00:36:12,294 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 2600.00 KB/s
2015-12-12 00:36:12,295 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000002688 size 13639 bytes.
2015-12-12 00:36:12,303 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2232
2015-12-12 00:36:12,303 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000002095, cpktTxId=0000000000000002095)
2015-12-12 00:36:19,163 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:36:19,163 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:36:49,163 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:36:49,163 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:37:19,163 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:37:19,163 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:37:49,163 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:37:49,163 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:38:19,163 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:38:19,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:38:49,163 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:38:49,163 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:38:51,046 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 13 
2015-12-12 00:38:51,160 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0018/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742158_1334{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:38:51,317 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742158_1334{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:38:51,322 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0018/job.jar is closed by DFSClient_NONMAPREDUCE_832554930_1
2015-12-12 00:38:51,334 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0018/job.jar
2015-12-12 00:38:51,405 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0018/job.split
2015-12-12 00:38:51,419 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0018/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742159_1335{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:38:51,431 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742159_1335{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:38:51,435 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0018/job.split is closed by DFSClient_NONMAPREDUCE_832554930_1
2015-12-12 00:38:51,451 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0018/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742160_1336{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:38:51,457 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742160_1336{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:38:51,460 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0018/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_832554930_1
2015-12-12 00:38:51,706 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0018/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742161_1337{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:38:51,723 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742161_1337{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:38:51,725 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0018/job.xml is closed by DFSClient_NONMAPREDUCE_832554930_1
2015-12-12 00:38:58,020 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0018/job_1449846414359_0018_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742162_1338{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:38:58,185 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742162_1338{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:38:58,199 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0018/job_1449846414359_0018_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1720607980_1
2015-12-12 00:39:11,623 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0018/job_1449846414359_0018_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742163_1339{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:39:12,033 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0018/job_1449846414359_0018_1.jhist for DFSClient_NONMAPREDUCE_-1720607980_1
2015-12-12 00:39:19,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30006 milliseconds
2015-12-12 00:39:19,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 11 millisecond(s).
2015-12-12 00:39:21,926 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_a/_temporary/1/_temporary/attempt_1449846414359_0018_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742164_1340{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:39:22,179 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742164_1340{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:39:22,186 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_a/_temporary/1/_temporary/attempt_1449846414359_0018_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449846414359_0018_r_000000_0_165233177_1
2015-12-12 00:39:22,523 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0018/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1720607980_1
2015-12-12 00:39:22,601 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_a/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1720607980_1
2015-12-12 00:39:22,611 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0018/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1720607980_1
2015-12-12 00:39:22,642 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742163_1339{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12221
2015-12-12 00:39:22,646 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0018/job_1449846414359_0018_1.jhist is closed by DFSClient_NONMAPREDUCE_-1720607980_1
2015-12-12 00:39:22,661 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0018.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742165_1341{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:39:22,677 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742165_1341{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:39:22,681 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0018.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1720607980_1
2015-12-12 00:39:22,731 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0018-1449898731963-parallels-chess+taskA-1449898762613-3-1-SUCCEEDED-default-1449898737610.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742166_1342{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:39:22,748 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742166_1342{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:39:22,751 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0018-1449898731963-parallels-chess+taskA-1449898762613-3-1-SUCCEEDED-default-1449898737610.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1720607980_1
2015-12-12 00:39:22,776 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0018_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742167_1343{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:39:22,893 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742167_1343{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:39:22,899 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0018_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1720607980_1
2015-12-12 00:39:23,970 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742158_1334 127.0.0.1:50010 
2015-12-12 00:39:23,970 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742159_1335 127.0.0.1:50010 
2015-12-12 00:39:23,970 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742160_1336 127.0.0.1:50010 
2015-12-12 00:39:23,971 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742161_1337 127.0.0.1:50010 
2015-12-12 00:39:23,971 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742163_1339 127.0.0.1:50010 
2015-12-12 00:39:23,971 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742162_1338 127.0.0.1:50010 
2015-12-12 00:39:26,235 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742160_1336, blk_1073742161_1337, blk_1073742162_1338, blk_1073742163_1339, blk_1073742158_1334, blk_1073742159_1335]
2015-12-12 00:39:49,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30005 milliseconds
2015-12-12 00:39:49,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:40:19,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:40:19,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:40:49,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:40:49,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:41:19,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 00:41:19,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:41:40,867 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 82 Total time for transactions(ms): 15 Number of transactions batched in Syncs: 0 Number of syncs: 58 SyncTimes(ms): 537 
2015-12-12 00:41:40,868 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742164_1340 127.0.0.1:50010 
2015-12-12 00:41:41,277 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742164_1340]
2015-12-12 00:41:47,290 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0019/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742168_1344{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:41:47,403 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742168_1344{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:41:47,407 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0019/job.jar is closed by DFSClient_NONMAPREDUCE_-1472110699_1
2015-12-12 00:41:47,412 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0019/job.jar
2015-12-12 00:41:47,488 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0019/job.split
2015-12-12 00:41:47,507 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0019/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742169_1345{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:41:47,514 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742169_1345{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:41:47,521 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0019/job.split is closed by DFSClient_NONMAPREDUCE_-1472110699_1
2015-12-12 00:41:47,533 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0019/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742170_1346{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:41:47,542 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742170_1346{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:41:47,544 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0019/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1472110699_1
2015-12-12 00:41:47,788 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0019/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742171_1347{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:41:47,804 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742171_1347{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:41:47,807 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0019/job.xml is closed by DFSClient_NONMAPREDUCE_-1472110699_1
2015-12-12 00:41:49,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:41:49,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:41:53,710 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0019/job_1449846414359_0019_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742172_1348{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:41:53,816 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742172_1348{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:41:53,822 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0019/job_1449846414359_0019_1_conf.xml is closed by DFSClient_NONMAPREDUCE_567194823_1
2015-12-12 00:42:10,382 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4784ms
No GCs detected
2015-12-12 00:42:11,842 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0019/job_1449846414359_0019_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742173_1349{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:42:11,989 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0019/job_1449846414359_0019_1.jhist for DFSClient_NONMAPREDUCE_567194823_1
2015-12-12 00:42:19,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30005 milliseconds
2015-12-12 00:42:19,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 00:42:44,693 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 124 Total time for transactions(ms): 19 Number of transactions batched in Syncs: 0 Number of syncs: 86 SyncTimes(ms): 657 
2015-12-12 00:42:44,853 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742173_1349{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13445
2015-12-12 00:42:44,858 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0019/job_1449846414359_0019_1.jhist is closed by DFSClient_NONMAPREDUCE_567194823_1
2015-12-12 00:42:44,908 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0019.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742174_1350{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:42:44,927 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742174_1350{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:42:44,931 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0019.summary_tmp is closed by DFSClient_NONMAPREDUCE_567194823_1
2015-12-12 00:42:45,075 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0019-1449898908033-parallels-chess+taskA-1449898964543-0-0-FAILED-default-1449898913331.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742175_1351{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:42:45,089 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742175_1351{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:42:45,104 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0019-1449898908033-parallels-chess+taskA-1449898964543-0-0-FAILED-default-1449898913331.jhist_tmp is closed by DFSClient_NONMAPREDUCE_567194823_1
2015-12-12 00:42:45,127 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0019_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742176_1352{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:42:45,135 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742176_1352{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:42:45,138 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449846414359_0019_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_567194823_1
2015-12-12 00:42:46,204 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742168_1344 127.0.0.1:50010 
2015-12-12 00:42:46,204 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742169_1345 127.0.0.1:50010 
2015-12-12 00:42:46,204 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742170_1346 127.0.0.1:50010 
2015-12-12 00:42:46,204 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742171_1347 127.0.0.1:50010 
2015-12-12 00:42:46,204 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742173_1349 127.0.0.1:50010 
2015-12-12 00:42:46,204 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742172_1348 127.0.0.1:50010 
2015-12-12 00:42:46,464 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742168_1344, blk_1073742169_1345, blk_1073742170_1346, blk_1073742171_1347, blk_1073742172_1348, blk_1073742173_1349]
2015-12-12 00:42:49,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30006 milliseconds
2015-12-12 00:42:49,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:43:19,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 00:43:19,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:43:49,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:43:49,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:44:19,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:44:19,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:44:49,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:44:49,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:45:19,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:45:19,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:45:20,099 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 148 Total time for transactions(ms): 22 Number of transactions batched in Syncs: 0 Number of syncs: 104 SyncTimes(ms): 773 
2015-12-12 00:45:27,280 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0020/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742177_1353{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:45:27,430 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742177_1353{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:45:27,439 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0020/job.jar is closed by DFSClient_NONMAPREDUCE_-1570665407_1
2015-12-12 00:45:27,446 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0020/job.jar
2015-12-12 00:45:27,520 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0020/job.split
2015-12-12 00:45:27,536 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0020/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742178_1354{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:45:27,550 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742178_1354{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:45:27,552 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0020/job.split is closed by DFSClient_NONMAPREDUCE_-1570665407_1
2015-12-12 00:45:27,568 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0020/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742179_1355{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:45:27,576 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742179_1355{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:45:27,580 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0020/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1570665407_1
2015-12-12 00:45:27,815 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0020/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742180_1356{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:45:27,829 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742180_1356{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:45:27,833 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0020/job.xml is closed by DFSClient_NONMAPREDUCE_-1570665407_1
2015-12-12 00:45:34,063 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0020/job_1449846414359_0020_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742181_1357{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:45:34,195 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742181_1357{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:45:34,200 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0020/job_1449846414359_0020_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1372391359_1
2015-12-12 00:45:47,993 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0020/job_1449846414359_0020_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742182_1358{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:45:48,111 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0020/job_1449846414359_0020_1.jhist for DFSClient_NONMAPREDUCE_-1372391359_1
2015-12-12 00:45:49,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 00:45:49,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:48:26,399 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-12 00:48:26,408 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-12 00:48:26,410 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-12 00:48:26,766 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-12 00:48:26,888 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-12 00:48:26,888 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-12 00:48:26,889 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-12 00:48:26,890 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-12 00:48:27,265 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-12 00:48:27,328 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-12 00:48:27,334 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-12 00:48:27,347 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-12 00:48:27,349 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-12 00:48:27,349 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-12 00:48:27,350 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-12 00:48:27,405 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-12 00:48:27,409 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-12 00:48:27,454 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-12 00:48:27,454 INFO org.mortbay.log: jetty-6.1.26
2015-12-12 00:48:27,779 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-12 00:48:27,862 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-12 00:48:27,862 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-12 00:48:27,907 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-12 00:48:27,928 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-12 00:48:27,985 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-12 00:48:27,985 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-12 00:48:27,989 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-12 00:48:27,991 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 12 00:48:27
2015-12-12 00:48:27,994 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-12 00:48:27,994 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-12 00:48:27,997 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-12 00:48:27,997 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-12 00:48:28,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-12 00:48:28,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-12 00:48:28,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-12 00:48:28,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-12 00:48:28,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-12 00:48:28,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-12 00:48:28,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-12 00:48:28,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-12 00:48:28,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-12 00:48:28,043 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-12 00:48:28,043 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-12 00:48:28,043 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-12 00:48:28,043 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-12 00:48:28,046 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-12 00:48:28,291 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-12 00:48:28,291 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-12 00:48:28,291 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-12 00:48:28,291 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-12 00:48:28,294 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-12 00:48:28,306 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-12 00:48:28,306 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-12 00:48:28,306 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-12 00:48:28,306 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-12 00:48:28,307 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-12 00:48:28,307 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-12 00:48:28,307 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-12 00:48:28,308 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-12 00:48:28,309 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-12 00:48:28,311 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-12 00:48:28,311 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-12 00:48:28,311 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-12 00:48:28,312 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-12 00:48:28,318 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-12 00:48:28,318 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-12 00:48:28,319 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-12 00:48:28,329 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/name/in_use.lock acquired by nodename 23971@ubuntu
2015-12-12 00:48:28,445 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/parallels/hadoop/tmp/dfs/name/current
2015-12-12 00:48:28,598 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000002689 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000002689-0000000000000002877
2015-12-12 00:48:28,642 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 121 INodes.
2015-12-12 00:48:28,698 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-12 00:48:28,698 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 2688 from /home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000002688
2015-12-12 00:48:28,698 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@53ccbdb5 expecting start txid #2689
2015-12-12 00:48:28,698 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000002689-0000000000000002877
2015-12-12 00:48:28,700 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000002689-0000000000000002877' to transaction ID 2689
2015-12-12 00:48:28,744 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000002689-0000000000000002877 of size 1048576 edits # 189 loaded in 0 seconds
2015-12-12 00:48:28,745 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-12-12 00:48:28,748 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 2878
2015-12-12 00:48:28,787 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-12 00:48:28,787 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 468 msecs
2015-12-12 00:48:29,100 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-12 00:48:29,108 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-12 00:48:29,119 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-12 00:48:29,153 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-12 00:48:29,164 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 1
2015-12-12 00:48:29,165 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 1
2015-12-12 00:48:29,165 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 116 blocks to reach the threshold 0.9990 of total blocks 116.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2015-12-12 00:48:29,192 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-12 00:48:29,193 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-12 00:48:29,195 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-12 00:48:29,195 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-12 00:48:29,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-12 00:48:29,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 68721983 milliseconds
2015-12-12 00:48:29,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 6 millisecond(s).
2015-12-12 00:48:32,926 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0) storage 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-12 00:48:32,928 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-12 00:48:32,929 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-12 00:48:33,020 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-12 00:48:33,020 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-75e73aff-3510-410a-91be-881360516f94 for DN 127.0.0.1:50010
2015-12-12 00:48:33,061 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 115 has reached the threshold 0.9990 of total blocks 116. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2015-12-12 00:48:33,061 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-12 00:48:33,062 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-75e73aff-3510-410a-91be-881360516f94,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-12-12 00:48:33,062 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 117, hasStaleStorages: false, processing time: 8 msecs
2015-12-12 00:48:33,070 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 117
2015-12-12 00:48:33,070 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-12 00:48:33,070 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 4
2015-12-12 00:48:33,071 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-12 00:48:33,071 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 1
2015-12-12 00:48:33,071 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 10 msec
2015-12-12 00:48:53,070 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 116 has reached the threshold 0.9990 of total blocks 116. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2015-12-12 00:48:59,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:48:59,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:49:03,074 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 35 secs
2015-12-12 00:49:03,074 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2015-12-12 00:49:03,074 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2015-12-12 00:49:03,074 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 4 blocks
2015-12-12 00:49:24,314 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449899324019_0001/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742183_1359{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:49:24,583 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742183_1359{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-12 00:49:24,583 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2015-12-12 00:49:24,600 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742183_1359{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 5835
2015-12-12 00:49:24,997 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449899324019_0001/job.jar is closed by DFSClient_NONMAPREDUCE_1006313544_1
2015-12-12 00:49:25,005 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449899324019_0001/job.jar
2015-12-12 00:49:25,101 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449899324019_0001/job.split
2015-12-12 00:49:25,118 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449899324019_0001/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742184_1360{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:49:25,135 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742184_1360{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:49:25,138 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449899324019_0001/job.split is closed by DFSClient_NONMAPREDUCE_1006313544_1
2015-12-12 00:49:25,149 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449899324019_0001/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742185_1361{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:49:25,157 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742185_1361{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:49:25,159 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449899324019_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1006313544_1
2015-12-12 00:49:25,420 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449899324019_0001/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742186_1362{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:49:25,437 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742186_1362{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:49:25,442 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449899324019_0001/job.xml is closed by DFSClient_NONMAPREDUCE_1006313544_1
2015-12-12 00:49:29,206 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:49:29,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 00:49:35,093 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 31 Total time for transactions(ms): 8 Number of transactions batched in Syncs: 0 Number of syncs: 24 SyncTimes(ms): 41 
2015-12-12 00:49:35,538 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449899324019_0001/job_1449899324019_0001_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742187_1363{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:49:35,697 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742187_1363{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:49:35,726 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449899324019_0001/job_1449899324019_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_125393537_1
2015-12-12 00:49:39,011 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-12 00:49:39,014 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-12 00:49:39,014 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 2878
2015-12-12 00:49:39,156 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 40 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 0 Number of syncs: 30 SyncTimes(ms): 261 
2015-12-12 00:49:39,164 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000002878 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000002878-0000000000000002917
2015-12-12 00:49:39,170 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 2918
2015-12-12 00:49:44,587 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.02s at 714.29 KB/s
2015-12-12 00:49:44,589 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000002917 size 15928 bytes.
2015-12-12 00:49:44,608 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2688
2015-12-12 00:49:44,608 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000002232, cpktTxId=0000000000000002232)
2015-12-12 00:49:53,371 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449899324019_0001/job_1449899324019_0001_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742188_1364{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:49:53,631 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449899324019_0001/job_1449899324019_0001_1.jhist for DFSClient_NONMAPREDUCE_125393537_1
2015-12-12 00:49:59,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:49:59,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:50:03,453 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_a/_temporary/1/_temporary/attempt_1449899324019_0001_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742189_1365{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:50:03,668 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742189_1365{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:50:03,681 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_a/_temporary/1/_temporary/attempt_1449899324019_0001_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449899324019_0001_r_000000_0_-1335832294_1
2015-12-12 00:50:04,101 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449899324019_0001/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_125393537_1
2015-12-12 00:50:04,151 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_a/_SUCCESS is closed by DFSClient_NONMAPREDUCE_125393537_1
2015-12-12 00:50:04,160 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449899324019_0001/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_125393537_1
2015-12-12 00:50:04,205 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742188_1364{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14349
2015-12-12 00:50:04,208 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449899324019_0001/job_1449899324019_0001_1.jhist is closed by DFSClient_NONMAPREDUCE_125393537_1
2015-12-12 00:50:04,217 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449899324019_0001.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742190_1366{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:50:04,224 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742190_1366{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:50:04,229 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449899324019_0001.summary_tmp is closed by DFSClient_NONMAPREDUCE_125393537_1
2015-12-12 00:50:04,290 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449899324019_0001-1449899365789-parallels-chess+taskA-1449899404163-3-1-SUCCEEDED-default-1449899375079.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742191_1367{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:50:04,352 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742191_1367{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:50:04,353 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449899324019_0001-1449899365789-parallels-chess+taskA-1449899404163-3-1-SUCCEEDED-default-1449899375079.jhist_tmp is closed by DFSClient_NONMAPREDUCE_125393537_1
2015-12-12 00:50:04,376 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449899324019_0001_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742192_1368{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 00:50:04,384 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742192_1368{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 00:50:04,394 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449899324019_0001_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_125393537_1
2015-12-12 00:50:05,590 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742183_1359 127.0.0.1:50010 
2015-12-12 00:50:05,591 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742184_1360 127.0.0.1:50010 
2015-12-12 00:50:05,592 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742185_1361 127.0.0.1:50010 
2015-12-12 00:50:05,592 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742186_1362 127.0.0.1:50010 
2015-12-12 00:50:05,592 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742188_1364 127.0.0.1:50010 
2015-12-12 00:50:05,592 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742187_1363 127.0.0.1:50010 
2015-12-12 00:50:08,438 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742183_1359, blk_1073742184_1360, blk_1073742185_1361, blk_1073742186_1362, blk_1073742187_1363, blk_1073742188_1364]
2015-12-12 00:50:29,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:50:29,209 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:50:59,209 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:50:59,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:51:29,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:51:29,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:51:59,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:51:59,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:52:29,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:52:29,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:52:59,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:52:59,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:53:29,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:53:29,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 00:53:59,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 00:53:59,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:54:29,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:54:29,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:54:59,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:54:59,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:55:29,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 00:55:29,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 00:55:32,375 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-12 00:55:32,381 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-12 11:55:43,265 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-12 11:55:43,282 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-12 11:55:43,285 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-12 11:55:43,653 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-12 11:55:43,884 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-12 11:55:43,884 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-12 11:55:43,886 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-12 11:55:43,886 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-12 11:55:44,300 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-12 11:55:44,374 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-12 11:55:44,380 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-12 11:55:44,392 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-12 11:55:44,395 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-12 11:55:44,396 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-12 11:55:44,396 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-12 11:55:44,448 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-12 11:55:44,451 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-12 11:55:44,487 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-12 11:55:44,487 INFO org.mortbay.log: jetty-6.1.26
2015-12-12 11:55:44,839 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-12 11:55:44,910 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-12 11:55:44,910 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-12 11:55:44,958 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-12 11:55:44,978 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-12 11:55:45,036 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-12 11:55:45,036 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-12 11:55:45,040 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-12 11:55:45,051 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 12 11:55:45
2015-12-12 11:55:45,054 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-12 11:55:45,054 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-12 11:55:45,057 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-12 11:55:45,058 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-12 11:55:45,116 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-12 11:55:45,117 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-12 11:55:45,117 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-12 11:55:45,117 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-12 11:55:45,117 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-12 11:55:45,117 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-12 11:55:45,117 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-12 11:55:45,117 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-12 11:55:45,117 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-12 11:55:45,122 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-12 11:55:45,122 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-12 11:55:45,122 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-12 11:55:45,122 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-12 11:55:45,125 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-12 11:55:45,369 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-12 11:55:45,369 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-12 11:55:45,370 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-12 11:55:45,370 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-12 11:55:45,372 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-12 11:55:45,390 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-12 11:55:45,390 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-12 11:55:45,390 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-12 11:55:45,390 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-12 11:55:45,391 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-12 11:55:45,391 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-12 11:55:45,391 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-12 11:55:45,393 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-12 11:55:45,393 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-12 11:55:45,395 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-12 11:55:45,395 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-12 11:55:45,395 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-12 11:55:45,395 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-12 11:55:45,400 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-12 11:55:45,400 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-12 11:55:45,400 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-12 11:55:45,418 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/name/in_use.lock acquired by nodename 27821@ubuntu
2015-12-12 11:55:45,567 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/parallels/hadoop/tmp/dfs/name/current
2015-12-12 11:55:45,707 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000002918 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000002918-0000000000000002961
2015-12-12 11:55:45,754 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 144 INodes.
2015-12-12 11:55:45,805 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-12 11:55:45,805 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 2917 from /home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000002917
2015-12-12 11:55:45,805 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@52513e3b expecting start txid #2918
2015-12-12 11:55:45,806 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000002918-0000000000000002961
2015-12-12 11:55:45,807 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000002918-0000000000000002961' to transaction ID 2918
2015-12-12 11:55:45,832 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000002918-0000000000000002961 of size 1048576 edits # 44 loaded in 0 seconds
2015-12-12 11:55:45,834 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2015-12-12 11:55:45,834 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2015-12-12 11:55:45,911 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2917
2015-12-12 11:55:45,911 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000002688, cpktTxId=0000000000000002688)
2015-12-12 11:55:45,927 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 2962
2015-12-12 11:55:45,969 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-12 11:55:45,969 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 569 msecs
2015-12-12 11:55:46,303 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-12 11:55:46,310 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-12 11:55:46,326 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-12 11:55:46,365 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-12 11:55:46,376 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 1
2015-12-12 11:55:46,376 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 1
2015-12-12 11:55:46,377 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 120 blocks to reach the threshold 0.9990 of total blocks 120.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2015-12-12 11:55:46,408 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-12 11:55:46,409 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-12 11:55:46,418 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-12 11:55:46,418 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-12 11:55:46,423 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-12 11:55:46,423 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 77312843 milliseconds
2015-12-12 11:55:46,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 11:55:50,501 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0) storage 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-12 11:55:50,509 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-12 11:55:50,510 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-12 11:55:50,679 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-12 11:55:50,679 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-75e73aff-3510-410a-91be-881360516f94 for DN 127.0.0.1:50010
2015-12-12 11:55:50,741 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 119 has reached the threshold 0.9990 of total blocks 120. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2015-12-12 11:55:50,742 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-12 11:55:50,743 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-75e73aff-3510-410a-91be-881360516f94,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-12-12 11:55:50,743 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 121, hasStaleStorages: false, processing time: 13 msecs
2015-12-12 11:55:50,746 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 121
2015-12-12 11:55:50,746 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-12 11:55:50,746 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 4
2015-12-12 11:55:50,746 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-12 11:55:50,746 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 1
2015-12-12 11:55:50,746 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 3 msec
2015-12-12 11:56:10,748 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 120 has reached the threshold 0.9990 of total blocks 120. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2015-12-12 11:56:16,424 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 11:56:16,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 11:56:20,753 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 35 secs
2015-12-12 11:56:20,754 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2015-12-12 11:56:20,756 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2015-12-12 11:56:20,757 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 4 blocks
2015-12-12 11:56:46,424 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 11:56:46,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 11:56:58,493 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-12 11:56:58,493 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-12 11:56:58,493 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 2962
2015-12-12 11:56:58,494 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 7 
2015-12-12 11:56:58,498 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 10 
2015-12-12 11:56:58,500 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000002962 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000002962-0000000000000002963
2015-12-12 11:56:58,501 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 2964
2015-12-12 11:56:59,925 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 3750.00 KB/s
2015-12-12 11:56:59,925 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000002963 size 15604 bytes.
2015-12-12 11:56:59,929 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2961
2015-12-12 11:56:59,930 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000002917, cpktTxId=0000000000000002917)
2015-12-12 11:57:16,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 11:57:16,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 11:57:46,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 11:57:46,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 11:58:16,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 11:58:16,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 11:58:46,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 11:58:46,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 11:59:16,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 11:59:16,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 11:59:46,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 11:59:46,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:00:16,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:00:16,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:00:46,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:00:46,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:01:16,428 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:01:16,429 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:01:46,429 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:01:46,430 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:02:16,429 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:02:16,429 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:02:46,431 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 12:02:46,432 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:03:04,045 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 12 
2015-12-12 12:03:04,215 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0001/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742193_1369{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:03:04,799 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742193_1369{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-12 12:03:04,799 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2015-12-12 12:03:04,854 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742193_1369{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 5395
2015-12-12 12:03:05,219 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0001/job.jar is closed by DFSClient_NONMAPREDUCE_1988214434_1
2015-12-12 12:03:05,226 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0001/job.jar
2015-12-12 12:03:05,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0001/job.split
2015-12-12 12:03:05,347 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0001/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742194_1370{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:03:05,369 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742194_1370{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:03:05,370 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0001/job.split is closed by DFSClient_NONMAPREDUCE_1988214434_1
2015-12-12 12:03:05,381 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0001/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742195_1371{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:03:05,388 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742195_1371{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:03:05,391 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1988214434_1
2015-12-12 12:03:05,695 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0001/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742196_1372{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:03:05,716 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742196_1372{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:03:05,719 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0001/job.xml is closed by DFSClient_NONMAPREDUCE_1988214434_1
2015-12-12 12:03:15,587 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0001/job_1449939365346_0001_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742197_1373{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:03:15,783 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742197_1373{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:03:15,789 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0001/job_1449939365346_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-819830546_1
2015-12-12 12:03:16,432 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:03:16,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 12:03:31,924 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0001/job_1449939365346_0001_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742198_1374{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:03:32,014 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0001/job_1449939365346_0001_1.jhist for DFSClient_NONMAPREDUCE_-819830546_1
2015-12-12 12:03:46,432 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:03:46,438 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 6 millisecond(s).
2015-12-12 12:04:11,233 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 45 Total time for transactions(ms): 17 Number of transactions batched in Syncs: 0 Number of syncs: 30 SyncTimes(ms): 131 
2015-12-12 12:04:11,284 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742198_1374{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13149
2015-12-12 12:04:11,292 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0001/job_1449939365346_0001_1.jhist is closed by DFSClient_NONMAPREDUCE_-819830546_1
2015-12-12 12:04:11,315 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0001.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742199_1375{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:04:11,323 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742199_1375{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:04:11,326 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0001.summary_tmp is closed by DFSClient_NONMAPREDUCE_-819830546_1
2015-12-12 12:04:11,378 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0001-1449939786000-parallels-chess+taskA-1449939851175-0-0-FAILED-default-1449939795099.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742200_1376{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:04:11,387 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742200_1376{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:04:11,392 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0001-1449939786000-parallels-chess+taskA-1449939851175-0-0-FAILED-default-1449939795099.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-819830546_1
2015-12-12 12:04:11,419 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0001_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742201_1377{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:04:11,429 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742201_1377{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:04:11,431 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0001_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-819830546_1
2015-12-12 12:04:12,554 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742193_1369 127.0.0.1:50010 
2015-12-12 12:04:12,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742194_1370 127.0.0.1:50010 
2015-12-12 12:04:12,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742195_1371 127.0.0.1:50010 
2015-12-12 12:04:12,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742196_1372 127.0.0.1:50010 
2015-12-12 12:04:12,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742198_1374 127.0.0.1:50010 
2015-12-12 12:04:12,555 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742197_1373 127.0.0.1:50010 
2015-12-12 12:04:13,574 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742193_1369, blk_1073742194_1370, blk_1073742195_1371, blk_1073742196_1372, blk_1073742197_1373, blk_1073742198_1374]
2015-12-12 12:04:16,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:04:16,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:04:46,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:04:46,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:05:16,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:05:16,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:05:46,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:05:46,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:06:16,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:06:16,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:06:46,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:06:46,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:07:16,435 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:07:16,435 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:07:46,435 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:07:46,435 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:08:16,435 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:08:16,436 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:08:46,435 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:08:46,438 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 12:09:16,436 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:09:16,436 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:09:46,437 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:09:46,438 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:10:16,437 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:10:16,437 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:10:46,438 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:10:46,438 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:11:16,439 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:11:16,439 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:11:46,440 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:11:46,440 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:11:46,514 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 69 Total time for transactions(ms): 23 Number of transactions batched in Syncs: 0 Number of syncs: 48 SyncTimes(ms): 152 
2015-12-12 12:12:16,442 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-12 12:12:16,442 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:12:28,112 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0002/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742202_1378{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:12:28,284 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742202_1378{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:12:28,288 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0002/job.jar is closed by DFSClient_NONMAPREDUCE_82792457_1
2015-12-12 12:12:28,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0002/job.jar
2015-12-12 12:12:28,366 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0002/job.split
2015-12-12 12:12:28,381 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0002/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742203_1379{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:12:28,394 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742203_1379{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:12:28,396 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0002/job.split is closed by DFSClient_NONMAPREDUCE_82792457_1
2015-12-12 12:12:28,413 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0002/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742204_1380{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:12:28,421 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742204_1380{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:12:28,424 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0002/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_82792457_1
2015-12-12 12:12:28,729 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0002/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742205_1381{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:12:28,759 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742205_1381{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:12:28,762 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0002/job.xml is closed by DFSClient_NONMAPREDUCE_82792457_1
2015-12-12 12:12:37,055 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0002/job_1449939365346_0002_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742206_1382{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:12:37,180 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742206_1382{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:12:37,189 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0002/job_1449939365346_0002_1_conf.xml is closed by DFSClient_NONMAPREDUCE_2030259423_1
2015-12-12 12:12:46,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30004 milliseconds
2015-12-12 12:12:46,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-12 12:12:55,491 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0002/job_1449939365346_0002_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742207_1383{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:12:55,493 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 109 Total time for transactions(ms): 24 Number of transactions batched in Syncs: 0 Number of syncs: 74 SyncTimes(ms): 234 
2015-12-12 12:12:55,540 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0002/job_1449939365346_0002_1.jhist for DFSClient_NONMAPREDUCE_2030259423_1
2015-12-12 12:13:16,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:13:16,462 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 15 millisecond(s).
2015-12-12 12:13:33,302 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742207_1383{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13149
2015-12-12 12:13:33,306 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0002/job_1449939365346_0002_1.jhist is closed by DFSClient_NONMAPREDUCE_2030259423_1
2015-12-12 12:13:33,319 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0002.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742208_1384{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:13:33,331 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742208_1384{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:13:33,334 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0002.summary_tmp is closed by DFSClient_NONMAPREDUCE_2030259423_1
2015-12-12 12:13:33,383 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0002-1449940349024-parallels-chess+taskA-1449940413212-0-0-FAILED-default-1449940356532.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742209_1385{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:13:33,391 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742209_1385{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:13:33,394 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0002-1449940349024-parallels-chess+taskA-1449940413212-0-0-FAILED-default-1449940356532.jhist_tmp is closed by DFSClient_NONMAPREDUCE_2030259423_1
2015-12-12 12:13:33,422 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0002_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742210_1386{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:13:33,431 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742210_1386{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:13:33,434 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0002_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_2030259423_1
2015-12-12 12:13:34,512 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742202_1378 127.0.0.1:50010 
2015-12-12 12:13:34,513 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742203_1379 127.0.0.1:50010 
2015-12-12 12:13:34,513 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742204_1380 127.0.0.1:50010 
2015-12-12 12:13:34,513 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742205_1381 127.0.0.1:50010 
2015-12-12 12:13:34,513 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742207_1383 127.0.0.1:50010 
2015-12-12 12:13:34,513 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742206_1382 127.0.0.1:50010 
2015-12-12 12:13:34,745 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742202_1378, blk_1073742203_1379, blk_1073742204_1380, blk_1073742205_1381, blk_1073742206_1382, blk_1073742207_1383]
2015-12-12 12:13:46,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:13:46,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:14:16,448 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:14:16,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:14:46,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:14:46,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:15:16,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:15:16,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:15:46,451 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:15:46,451 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:16:16,451 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:16:16,451 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:16:46,452 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:16:46,452 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:17:16,453 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:17:16,453 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:17:46,453 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:17:46,453 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:18:16,453 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:18:16,454 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:18:46,454 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:18:46,454 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:19:16,455 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:19:16,455 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:19:46,456 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:19:46,456 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:19:57,867 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 135 Total time for transactions(ms): 33 Number of transactions batched in Syncs: 0 Number of syncs: 94 SyncTimes(ms): 274 
2015-12-12 12:20:05,931 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0003/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742211_1387{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:20:06,082 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742211_1387{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:20:06,087 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0003/job.jar is closed by DFSClient_NONMAPREDUCE_695849014_1
2015-12-12 12:20:06,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0003/job.jar
2015-12-12 12:20:06,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0003/job.split
2015-12-12 12:20:06,205 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0003/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742212_1388{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:20:06,218 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742212_1388{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:20:06,221 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0003/job.split is closed by DFSClient_NONMAPREDUCE_695849014_1
2015-12-12 12:20:06,231 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0003/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742213_1389{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:20:06,238 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742213_1389{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:20:06,242 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0003/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_695849014_1
2015-12-12 12:20:06,510 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0003/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742214_1390{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:20:06,529 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742214_1390{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:20:06,538 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0003/job.xml is closed by DFSClient_NONMAPREDUCE_695849014_1
2015-12-12 12:20:13,826 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0003/job_1449939365346_0003_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742215_1391{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:20:13,966 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742215_1391{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:20:13,971 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0003/job_1449939365346_0003_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-731835732_1
2015-12-12 12:20:16,462 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-12 12:20:16,463 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 4 millisecond(s).
2015-12-12 12:20:32,510 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0003/job_1449939365346_0003_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742216_1392{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:20:32,669 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0003/job_1449939365346_0003_1.jhist for DFSClient_NONMAPREDUCE_-731835732_1
2015-12-12 12:20:46,554 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30062 milliseconds
2015-12-12 12:20:46,612 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 90 millisecond(s).
2015-12-12 12:21:16,532 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30007 milliseconds
2015-12-12 12:21:16,562 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 34 millisecond(s).
2015-12-12 12:21:27,268 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 178 Total time for transactions(ms): 50 Number of transactions batched in Syncs: 0 Number of syncs: 122 SyncTimes(ms): 502 
2015-12-12 12:21:27,768 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742216_1392{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13149
2015-12-12 12:21:27,810 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0003/job_1449939365346_0003_1.jhist is closed by DFSClient_NONMAPREDUCE_-731835732_1
2015-12-12 12:21:27,868 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0003.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742217_1393{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:21:27,905 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742217_1393{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:21:27,907 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0003.summary_tmp is closed by DFSClient_NONMAPREDUCE_-731835732_1
2015-12-12 12:21:28,036 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0003-1449940806822-parallels-chess+taskB-1449940887140-0-0-FAILED-default-1449940813348.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742218_1394{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:21:28,048 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742218_1394{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:21:28,052 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0003-1449940806822-parallels-chess+taskB-1449940887140-0-0-FAILED-default-1449940813348.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-731835732_1
2015-12-12 12:21:28,088 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0003_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742219_1395{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:21:28,111 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742219_1395{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:21:28,113 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0003_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-731835732_1
2015-12-12 12:21:29,221 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742211_1387 127.0.0.1:50010 
2015-12-12 12:21:29,222 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742212_1388 127.0.0.1:50010 
2015-12-12 12:21:29,222 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742213_1389 127.0.0.1:50010 
2015-12-12 12:21:29,222 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742214_1390 127.0.0.1:50010 
2015-12-12 12:21:29,222 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742216_1392 127.0.0.1:50010 
2015-12-12 12:21:29,223 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742215_1391 127.0.0.1:50010 
2015-12-12 12:21:31,936 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742211_1387, blk_1073742212_1388, blk_1073742213_1389, blk_1073742214_1390, blk_1073742215_1391, blk_1073742216_1392]
2015-12-12 12:21:46,528 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:21:46,534 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 5 millisecond(s).
2015-12-12 12:22:16,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 12:22:16,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:22:29,289 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-12 12:22:46,531 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:22:46,531 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:22:49,610 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 202 Total time for transactions(ms): 54 Number of transactions batched in Syncs: 0 Number of syncs: 140 SyncTimes(ms): 917 
2015-12-12 12:23:00,474 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0004/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742220_1396{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:23:00,639 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742220_1396{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:23:00,648 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0004/job.jar is closed by DFSClient_NONMAPREDUCE_169392428_1
2015-12-12 12:23:00,655 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0004/job.jar
2015-12-12 12:23:00,729 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0004/job.split
2015-12-12 12:23:00,744 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0004/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742221_1397{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:23:00,759 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742221_1397{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:23:00,763 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0004/job.split is closed by DFSClient_NONMAPREDUCE_169392428_1
2015-12-12 12:23:00,773 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0004/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742222_1398{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:23:00,784 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742222_1398{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:23:00,787 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0004/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_169392428_1
2015-12-12 12:23:01,038 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0004/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742223_1399{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:23:01,063 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742223_1399{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:23:01,067 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0004/job.xml is closed by DFSClient_NONMAPREDUCE_169392428_1
2015-12-12 12:23:07,533 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0004/job_1449939365346_0004_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742224_1400{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:23:07,768 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742224_1400{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:23:07,776 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0004/job_1449939365346_0004_1_conf.xml is closed by DFSClient_NONMAPREDUCE_630512287_1
2015-12-12 12:23:16,531 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:23:16,533 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-12 12:23:23,160 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0004/job_1449939365346_0004_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742225_1401{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:23:23,538 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0004/job_1449939365346_0004_1.jhist for DFSClient_NONMAPREDUCE_630512287_1
2015-12-12 12:23:46,536 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30005 milliseconds
2015-12-12 12:23:46,594 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 59 millisecond(s).
2015-12-12 12:23:59,108 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 244 Total time for transactions(ms): 70 Number of transactions batched in Syncs: 0 Number of syncs: 168 SyncTimes(ms): 1138 
2015-12-12 12:23:59,473 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742225_1401{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13053
2015-12-12 12:23:59,514 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0004/job_1449939365346_0004_1.jhist is closed by DFSClient_NONMAPREDUCE_630512287_1
2015-12-12 12:23:59,584 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0004.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742226_1402{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:23:59,626 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742226_1402{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:23:59,633 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0004.summary_tmp is closed by DFSClient_NONMAPREDUCE_630512287_1
2015-12-12 12:23:59,745 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0004-1449940981338-parallels-chess+taskB-1449941039044-0-0-FAILED-default-1449940987038.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742227_1403{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:23:59,761 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742227_1403{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:23:59,765 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0004-1449940981338-parallels-chess+taskB-1449941039044-0-0-FAILED-default-1449940987038.jhist_tmp is closed by DFSClient_NONMAPREDUCE_630512287_1
2015-12-12 12:23:59,794 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0004_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742228_1404{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:23:59,809 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742228_1404{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:23:59,812 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0004_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_630512287_1
2015-12-12 12:24:00,967 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742220_1396 127.0.0.1:50010 
2015-12-12 12:24:00,968 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742221_1397 127.0.0.1:50010 
2015-12-12 12:24:00,968 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742222_1398 127.0.0.1:50010 
2015-12-12 12:24:00,968 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742223_1399 127.0.0.1:50010 
2015-12-12 12:24:00,969 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742225_1401 127.0.0.1:50010 
2015-12-12 12:24:00,969 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742224_1400 127.0.0.1:50010 
2015-12-12 12:24:02,113 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742224_1400, blk_1073742225_1401, blk_1073742220_1396, blk_1073742221_1397, blk_1073742222_1398, blk_1073742223_1399]
2015-12-12 12:24:16,537 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 12:24:16,539 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:24:46,538 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:24:46,539 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:25:16,539 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:25:16,540 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:25:46,540 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:25:46,541 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:25:50,544 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 268 Total time for transactions(ms): 76 Number of transactions batched in Syncs: 0 Number of syncs: 186 SyncTimes(ms): 1346 
2015-12-12 12:25:57,724 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0005/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742229_1405{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:25:57,860 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742229_1405{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:25:57,864 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0005/job.jar is closed by DFSClient_NONMAPREDUCE_1764368169_1
2015-12-12 12:25:57,870 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0005/job.jar
2015-12-12 12:25:57,956 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0005/job.split
2015-12-12 12:25:57,970 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0005/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742230_1406{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:25:57,982 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742230_1406{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:25:57,984 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0005/job.split is closed by DFSClient_NONMAPREDUCE_1764368169_1
2015-12-12 12:25:58,001 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0005/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742231_1407{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:25:58,008 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742231_1407{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:25:58,011 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0005/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1764368169_1
2015-12-12 12:25:58,291 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0005/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742232_1408{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:25:58,304 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742232_1408{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:25:58,306 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0005/job.xml is closed by DFSClient_NONMAPREDUCE_1764368169_1
2015-12-12 12:26:04,392 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0005/job_1449939365346_0005_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742233_1409{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:26:04,567 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742233_1409{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:26:04,579 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0005/job_1449939365346_0005_1_conf.xml is closed by DFSClient_NONMAPREDUCE_102225082_1
2015-12-12 12:26:16,629 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30089 milliseconds
2015-12-12 12:26:16,637 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 7 millisecond(s).
2015-12-12 12:26:18,190 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0005/job_1449939365346_0005_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742234_1410{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:26:18,508 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0005/job_1449939365346_0005_1.jhist for DFSClient_NONMAPREDUCE_102225082_1
2015-12-12 12:26:46,632 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 12:26:46,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 11 millisecond(s).
2015-12-12 12:26:49,665 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742234_1410{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13053
2015-12-12 12:26:49,674 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0005/job_1449939365346_0005_1.jhist is closed by DFSClient_NONMAPREDUCE_102225082_1
2015-12-12 12:26:49,697 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0005.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742235_1411{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:26:49,710 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742235_1411{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:26:49,716 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0005.summary_tmp is closed by DFSClient_NONMAPREDUCE_102225082_1
2015-12-12 12:26:49,792 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0005-1449941158558-parallels-chess+taskB-1449941209480-0-0-FAILED-default-1449941164004.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742236_1412{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:26:49,808 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742236_1412{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:26:49,813 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0005-1449941158558-parallels-chess+taskB-1449941209480-0-0-FAILED-default-1449941164004.jhist_tmp is closed by DFSClient_NONMAPREDUCE_102225082_1
2015-12-12 12:26:49,856 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0005_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742237_1413{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 12:26:49,871 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742237_1413{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 12:26:49,879 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0005_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_102225082_1
2015-12-12 12:26:50,937 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 333 Total time for transactions(ms): 82 Number of transactions batched in Syncs: 0 Number of syncs: 231 SyncTimes(ms): 1580 
2015-12-12 12:26:50,938 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742229_1405 127.0.0.1:50010 
2015-12-12 12:26:50,938 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742230_1406 127.0.0.1:50010 
2015-12-12 12:26:50,938 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742231_1407 127.0.0.1:50010 
2015-12-12 12:26:50,939 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742232_1408 127.0.0.1:50010 
2015-12-12 12:26:50,940 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742234_1410 127.0.0.1:50010 
2015-12-12 12:26:50,940 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742233_1409 127.0.0.1:50010 
2015-12-12 12:26:53,163 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742229_1405, blk_1073742230_1406, blk_1073742231_1407, blk_1073742232_1408, blk_1073742233_1409, blk_1073742234_1410]
2015-12-12 12:27:16,632 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:27:16,633 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:27:46,633 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:27:46,633 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:28:16,634 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:28:16,635 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:28:46,634 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:28:46,635 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:29:16,636 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 12:29:16,637 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:29:46,636 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:29:46,637 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:30:16,637 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:30:16,638 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:30:46,637 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:30:46,638 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:31:16,638 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:31:16,639 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:31:46,638 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:31:46,639 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:32:16,638 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:32:16,639 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:32:46,639 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:32:46,640 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:33:16,640 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:33:16,640 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:33:46,641 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:33:46,641 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:34:16,641 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:34:16,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:34:46,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:34:46,644 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 12:35:16,643 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:35:16,643 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:35:46,643 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:35:46,643 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:36:16,644 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:36:16,644 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:36:46,644 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:36:46,647 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 12:37:16,645 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:37:16,645 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:37:46,646 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:37:46,646 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:38:16,648 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 12:38:16,648 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:38:46,649 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:38:46,650 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:39:16,650 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:39:16,650 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:39:46,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:39:46,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:40:16,650 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:40:16,652 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 12:40:46,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:40:46,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:41:16,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:41:16,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:41:46,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:41:46,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 12:42:16,652 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:42:16,652 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:42:46,652 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 12:42:46,653 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:43:16,652 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:43:16,653 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 12:43:46,652 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 12:43:46,652 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 14:43:49,800 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: [Lease.  Holder: DFSClient_NONMAPREDUCE_-1372391359_1, pendingcreates: 1] has expired hard limit
2015-12-12 14:43:49,802 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering [Lease.  Holder: DFSClient_NONMAPREDUCE_-1372391359_1, pendingcreates: 1], src=/tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0020/job_1449846414359_0020_1.jhist
2015-12-12 14:43:49,833 INFO BlockStateChange: BLOCK* blk_1073742182_1358{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|FINALIZED]]} recovery started, primary=ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|FINALIZED]
2015-12-12 14:43:49,833 WARN org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.internalReleaseLease: File /tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0020/job_1449846414359_0020_1.jhist has not been closed. Lease recovery is in progress. RecoveryId = 1414 for block blk_1073742182_1358{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|FINALIZED]]}
2015-12-12 14:43:49,834 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 335 Total time for transactions(ms): 82 Number of transactions batched in Syncs: 0 Number of syncs: 232 SyncTimes(ms): 1580 
2015-12-12 14:43:50,691 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 136, hasStaleStorages: false, processing time: 10 msecs
2015-12-12 14:43:51,312 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(lastblock=BP-1841603009-127.0.1.1-1449456040486:blk_1073742182_1358, newgenerationstamp=1414, newlength=58574, newtargets=[127.0.0.1:50010], closeFile=true, deleteBlock=false)
2015-12-12 14:43:51,324 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(newblock=BP-1841603009-127.0.1.1-1449456040486:blk_1073742182_1358, file=/tmp/hadoop-yarn/staging/parallels/.staging/job_1449846414359_0020/job_1449846414359_0020_1.jhist, newgenerationstamp=1414, newlength=58574, newtargets=[127.0.0.1:50010]) successful
2015-12-12 14:44:07,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-12 14:44:07,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 14:44:37,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 14:44:37,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 14:45:07,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 14:45:07,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 14:45:37,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 14:45:37,296 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:38:56,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:38:56,130 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:39:26,130 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:39:26,130 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:39:56,130 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:39:56,130 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:40:26,131 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:40:26,131 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:40:56,131 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:40:56,132 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:41:26,132 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:41:26,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:41:56,133 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:41:56,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:42:26,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:42:26,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:42:56,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:42:56,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:43:26,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:43:26,136 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:43:56,135 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:43:56,136 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:44:26,137 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:44:26,137 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:44:56,137 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:44:56,137 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:45:26,138 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:45:26,138 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:45:56,139 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:45:56,139 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:46:26,139 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:46:26,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:46:56,141 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:46:56,141 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:47:26,141 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:47:26,142 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:47:56,141 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:47:56,141 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:48:26,141 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:48:26,141 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:48:56,141 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:48:56,142 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:49:26,142 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:49:26,143 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:49:41,358 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-12 15:49:41,359 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-12 15:49:41,359 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 2964
2015-12-12 15:49:41,359 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 337 Total time for transactions(ms): 82 Number of transactions batched in Syncs: 0 Number of syncs: 234 SyncTimes(ms): 1581 
2015-12-12 15:49:41,359 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 337 Total time for transactions(ms): 82 Number of transactions batched in Syncs: 0 Number of syncs: 235 SyncTimes(ms): 1581 
2015-12-12 15:49:41,360 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000002964 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000002964-0000000000000003300
2015-12-12 15:49:41,362 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3301
2015-12-12 15:49:41,792 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 5333.33 KB/s
2015-12-12 15:49:41,792 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000003300 size 17293 bytes.
2015-12-12 15:49:41,795 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2963
2015-12-12 15:49:41,795 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000002961, cpktTxId=0000000000000002961)
2015-12-12 15:49:56,142 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:49:56,143 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:50:26,143 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:50:26,143 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:50:56,144 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:50:56,146 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-12 15:51:26,145 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:51:26,145 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:51:56,145 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:51:56,146 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:52:26,145 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:52:26,146 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:52:56,146 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:52:56,147 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:53:26,146 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:53:26,146 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:53:36,547 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 20 
2015-12-12 15:53:47,346 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0006/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742238_1415{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 15:53:47,497 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742238_1415{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 15:53:47,500 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0006/job.jar is closed by DFSClient_NONMAPREDUCE_1859945875_1
2015-12-12 15:53:47,505 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0006/job.jar
2015-12-12 15:53:47,590 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0006/job.split
2015-12-12 15:53:47,605 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0006/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742239_1416{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 15:53:47,612 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742239_1416{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 15:53:47,618 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0006/job.split is closed by DFSClient_NONMAPREDUCE_1859945875_1
2015-12-12 15:53:47,631 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0006/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742240_1417{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 15:53:47,644 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742240_1417{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 15:53:47,647 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0006/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1859945875_1
2015-12-12 15:53:47,893 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0006/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742241_1418{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 15:53:47,916 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742241_1418{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 15:53:47,919 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0006/job.xml is closed by DFSClient_NONMAPREDUCE_1859945875_1
2015-12-12 15:53:54,885 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0006/job_1449939365346_0006_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742242_1419{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 15:53:55,038 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742242_1419{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 15:53:55,044 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0006/job_1449939365346_0006_1_conf.xml is closed by DFSClient_NONMAPREDUCE_517446540_1
2015-12-12 15:53:56,147 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:53:56,148 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:54:09,016 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0006/job_1449939365346_0006_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742243_1420{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 15:54:09,274 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0006/job_1449939365346_0006_1.jhist for DFSClient_NONMAPREDUCE_517446540_1
2015-12-12 15:54:26,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30014 milliseconds
2015-12-12 15:54:26,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 38 millisecond(s).
2015-12-12 15:54:41,270 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 47 Total time for transactions(ms): 11 Number of transactions batched in Syncs: 0 Number of syncs: 30 SyncTimes(ms): 225 
2015-12-12 15:54:41,479 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742243_1420{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13053
2015-12-12 15:54:41,507 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0006/job_1449939365346_0006_1.jhist is closed by DFSClient_NONMAPREDUCE_517446540_1
2015-12-12 15:54:41,540 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0006.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742244_1421{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 15:54:41,570 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742244_1421{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 15:54:41,573 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0006.summary_tmp is closed by DFSClient_NONMAPREDUCE_517446540_1
2015-12-12 15:54:41,675 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0006-1449953628180-parallels-chess+taskB-1449953681022-0-0-FAILED-default-1449953634394.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742245_1422{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 15:54:41,692 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742245_1422{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 15:54:41,695 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0006-1449953628180-parallels-chess+taskB-1449953681022-0-0-FAILED-default-1449953634394.jhist_tmp is closed by DFSClient_NONMAPREDUCE_517446540_1
2015-12-12 15:54:41,732 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0006_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742246_1423{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 15:54:41,746 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742246_1423{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 15:54:41,749 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0006_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_517446540_1
2015-12-12 15:54:42,866 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742238_1415 127.0.0.1:50010 
2015-12-12 15:54:42,867 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742239_1416 127.0.0.1:50010 
2015-12-12 15:54:42,867 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742240_1417 127.0.0.1:50010 
2015-12-12 15:54:42,868 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742241_1418 127.0.0.1:50010 
2015-12-12 15:54:42,868 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742243_1420 127.0.0.1:50010 
2015-12-12 15:54:42,868 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742242_1419 127.0.0.1:50010 
2015-12-12 15:54:45,380 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742240_1417, blk_1073742241_1418, blk_1073742242_1419, blk_1073742243_1420, blk_1073742238_1415, blk_1073742239_1416]
2015-12-12 15:54:56,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:54:56,162 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:55:26,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:55:26,162 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:55:56,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:55:56,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 15:56:26,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:56:26,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:56:56,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:56:56,167 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 15:57:26,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:57:26,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:57:56,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:57:56,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:58:26,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:58:26,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:58:56,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:58:56,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 15:59:26,167 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 15:59:26,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 15:59:33,788 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 71 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 0 Number of syncs: 48 SyncTimes(ms): 254 
2015-12-12 15:59:40,054 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0007/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742247_1424{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 15:59:40,190 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742247_1424{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 15:59:40,366 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0007/job.jar is closed by DFSClient_NONMAPREDUCE_-1412005870_1
2015-12-12 15:59:40,375 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0007/job.jar
2015-12-12 15:59:40,438 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0007/job.split
2015-12-12 15:59:40,456 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0007/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742248_1425{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 15:59:40,467 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742248_1425{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 15:59:40,469 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0007/job.split is closed by DFSClient_NONMAPREDUCE_-1412005870_1
2015-12-12 15:59:40,484 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0007/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742249_1426{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 15:59:40,492 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742249_1426{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 15:59:40,495 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0007/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1412005870_1
2015-12-12 15:59:40,741 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0007/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742250_1427{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 15:59:40,764 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742250_1427{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 15:59:40,768 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0007/job.xml is closed by DFSClient_NONMAPREDUCE_-1412005870_1
2015-12-12 15:59:46,526 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0007/job_1449939365346_0007_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742251_1428{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 15:59:46,669 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742251_1428{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 15:59:46,675 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0007/job_1449939365346_0007_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-402241059_1
2015-12-12 15:59:56,167 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 15:59:56,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:00:02,958 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0007/job_1449939365346_0007_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742252_1429{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 16:00:03,090 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0007/job_1449939365346_0007_1.jhist for DFSClient_NONMAPREDUCE_-402241059_1
2015-12-12 16:00:12,993 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_b/_temporary/1/_temporary/attempt_1449939365346_0007_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449939365346_0007_r_000000_0_-153575875_1
2015-12-12 16:00:13,181 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0007/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-402241059_1
2015-12-12 16:00:13,233 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_b/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-402241059_1
2015-12-12 16:00:13,243 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0007/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-402241059_1
2015-12-12 16:00:13,288 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742252_1429{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14326
2015-12-12 16:00:13,292 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0007/job_1449939365346_0007_1.jhist is closed by DFSClient_NONMAPREDUCE_-402241059_1
2015-12-12 16:00:13,303 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0007.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742253_1430{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 16:00:13,311 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742253_1430{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 16:00:13,315 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0007.summary_tmp is closed by DFSClient_NONMAPREDUCE_-402241059_1
2015-12-12 16:00:13,369 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0007-1449953981011-parallels-chess+taskB-1449954013255-3-1-SUCCEEDED-default-1449953986242.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742254_1431{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 16:00:13,380 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742254_1431{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 16:00:13,382 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0007-1449953981011-parallels-chess+taskB-1449954013255-3-1-SUCCEEDED-default-1449953986242.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-402241059_1
2015-12-12 16:00:13,405 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0007_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742255_1432{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 16:00:13,415 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742255_1432{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 16:00:13,421 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0007_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-402241059_1
2015-12-12 16:00:14,485 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742247_1424 127.0.0.1:50010 
2015-12-12 16:00:14,485 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742248_1425 127.0.0.1:50010 
2015-12-12 16:00:14,485 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742249_1426 127.0.0.1:50010 
2015-12-12 16:00:14,485 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742250_1427 127.0.0.1:50010 
2015-12-12 16:00:14,486 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742252_1429 127.0.0.1:50010 
2015-12-12 16:00:14,486 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742251_1428 127.0.0.1:50010 
2015-12-12 16:00:15,654 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742247_1424, blk_1073742248_1425, blk_1073742249_1426, blk_1073742250_1427, blk_1073742251_1428, blk_1073742252_1429]
2015-12-12 16:00:26,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-12 16:00:26,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 9 millisecond(s).
2015-12-12 16:00:56,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:00:56,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:01:26,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:01:26,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:01:56,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:01:56,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:02:26,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:02:26,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:02:56,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:02:56,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:03:26,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:03:26,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:03:56,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:03:56,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:04:26,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:04:26,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:04:56,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:04:56,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:05:26,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:05:26,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:05:56,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:05:56,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:06:26,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:06:26,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:06:56,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:06:56,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 16:07:26,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:07:26,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:07:56,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:07:56,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:08:26,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 16:08:26,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:08:56,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:08:56,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:09:26,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:09:26,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:09:56,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:09:56,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:10:26,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:10:26,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:10:56,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:10:56,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:11:26,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:11:26,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:11:56,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:11:56,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:12:26,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:12:26,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:12:56,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:12:56,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:13:26,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:13:26,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:13:56,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:13:56,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:14:26,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:14:26,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 16:14:56,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:14:56,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:15:26,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:15:26,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:15:56,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:15:56,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 16:16:26,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-12 16:16:26,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:16:56,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:16:56,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 16:17:26,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:17:26,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:17:56,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:17:56,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:18:26,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 16:18:26,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:18:56,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:18:56,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:19:26,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:19:26,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:19:56,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:19:56,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:20:26,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:20:26,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:20:56,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:20:56,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:21:26,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 16:21:26,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:21:56,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:21:56,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:22:26,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:22:26,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:22:56,203 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:22:56,203 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:23:26,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:23:26,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:23:56,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:23:56,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:24:26,206 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:24:26,206 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:24:56,206 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:24:56,206 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:25:26,207 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:25:26,207 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:25:56,207 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:25:56,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:26:26,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:26:26,209 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:26:56,209 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:26:56,209 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:27:26,209 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:27:26,209 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:27:56,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:27:56,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:28:26,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:28:26,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:28:56,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:28:56,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:29:26,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:29:26,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:29:56,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:29:56,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:30:26,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:30:26,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:30:56,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:30:56,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:31:26,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:31:26,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:31:56,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:31:56,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:32:26,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:32:26,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:32:56,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:32:56,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:33:26,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:33:26,216 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:33:56,216 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:33:56,216 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:34:26,216 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:34:26,216 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:34:56,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:34:56,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:35:26,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:35:26,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:35:56,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:35:56,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:36:26,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:36:26,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:36:56,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:36:56,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:37:26,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:37:26,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:37:56,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:37:56,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:38:26,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:38:26,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:38:48,916 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 149 Total time for transactions(ms): 26 Number of transactions batched in Syncs: 0 Number of syncs: 104 SyncTimes(ms): 290 
2015-12-12 16:38:56,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:38:56,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:38:56,420 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0008/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742256_1433{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 16:38:56,573 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742256_1433{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 16:38:56,577 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0008/job.jar is closed by DFSClient_NONMAPREDUCE_899392274_1
2015-12-12 16:38:56,584 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0008/job.jar
2015-12-12 16:38:56,665 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0008/job.split
2015-12-12 16:38:56,679 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0008/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742257_1434{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 16:38:56,686 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742257_1434{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 16:38:56,689 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0008/job.split is closed by DFSClient_NONMAPREDUCE_899392274_1
2015-12-12 16:38:56,704 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0008/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742258_1435{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 16:38:56,715 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742258_1435{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 16:38:56,717 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0008/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_899392274_1
2015-12-12 16:38:56,966 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0008/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742259_1436{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 16:38:56,982 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742259_1436{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 16:38:56,985 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0008/job.xml is closed by DFSClient_NONMAPREDUCE_899392274_1
2015-12-12 16:39:03,630 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0008/job_1449939365346_0008_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742260_1437{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 16:39:03,760 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742260_1437{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 16:39:03,768 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0008/job_1449939365346_0008_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1937711158_1
2015-12-12 16:39:19,825 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0008/job_1449939365346_0008_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742261_1438{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 16:39:19,904 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0008/job_1449939365346_0008_1.jhist for DFSClient_NONMAPREDUCE_1937711158_1
2015-12-12 16:39:26,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-12 16:39:26,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 14 millisecond(s).
2015-12-12 16:39:27,114 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_b/_temporary/1/_temporary/attempt_1449939365346_0008_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449939365346_0008_r_000000_0_1467695158_1
2015-12-12 16:39:27,302 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0008/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_1937711158_1
2015-12-12 16:39:27,364 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_b/_SUCCESS is closed by DFSClient_NONMAPREDUCE_1937711158_1
2015-12-12 16:39:27,372 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0008/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_1937711158_1
2015-12-12 16:39:27,407 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742261_1438{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14328
2015-12-12 16:39:27,434 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0008/job_1449939365346_0008_1.jhist is closed by DFSClient_NONMAPREDUCE_1937711158_1
2015-12-12 16:39:27,450 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0008.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742262_1439{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 16:39:27,459 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742262_1439{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 16:39:27,463 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0008.summary_tmp is closed by DFSClient_NONMAPREDUCE_1937711158_1
2015-12-12 16:39:27,514 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0008-1449956337266-parallels-chess+taskB-1449956367374-3-1-SUCCEEDED-default-1449956343203.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742263_1440{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 16:39:27,526 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742263_1440{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 16:39:27,533 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0008-1449956337266-parallels-chess+taskB-1449956367374-3-1-SUCCEEDED-default-1449956343203.jhist_tmp is closed by DFSClient_NONMAPREDUCE_1937711158_1
2015-12-12 16:39:27,580 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0008_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742264_1441{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 16:39:27,600 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742264_1441{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 16:39:27,603 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0008_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_1937711158_1
2015-12-12 16:39:28,677 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742256_1433 127.0.0.1:50010 
2015-12-12 16:39:28,677 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742257_1434 127.0.0.1:50010 
2015-12-12 16:39:28,677 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742258_1435 127.0.0.1:50010 
2015-12-12 16:39:28,677 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742259_1436 127.0.0.1:50010 
2015-12-12 16:39:28,677 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742261_1438 127.0.0.1:50010 
2015-12-12 16:39:28,677 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742260_1437 127.0.0.1:50010 
2015-12-12 16:39:31,522 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742256_1433, blk_1073742257_1434, blk_1073742258_1435, blk_1073742259_1436, blk_1073742260_1437, blk_1073742261_1438]
2015-12-12 16:39:56,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:39:56,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:40:26,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:40:26,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:40:53,792 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 227 Total time for transactions(ms): 35 Number of transactions batched in Syncs: 0 Number of syncs: 160 SyncTimes(ms): 330 
2015-12-12 16:40:56,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:40:56,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:41:26,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:41:26,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:41:56,226 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:41:56,227 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:42:26,227 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:42:26,227 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:42:56,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:42:56,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:43:26,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:43:26,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 16:43:56,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:43:56,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:44:26,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:44:26,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:44:56,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:44:56,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:45:26,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:45:26,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:45:56,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:45:56,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:46:26,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:46:26,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:46:56,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:46:56,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:47:26,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:47:26,235 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:47:56,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:47:56,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:48:26,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:48:26,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:48:56,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:48:56,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:49:26,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:49:26,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:49:43,205 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-12 16:49:43,206 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-12 16:49:43,206 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 3301
2015-12-12 16:49:43,206 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 228 Total time for transactions(ms): 35 Number of transactions batched in Syncs: 0 Number of syncs: 161 SyncTimes(ms): 330 
2015-12-12 16:49:43,210 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 228 Total time for transactions(ms): 35 Number of transactions batched in Syncs: 0 Number of syncs: 162 SyncTimes(ms): 334 
2015-12-12 16:49:43,217 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000003301 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000003301-0000000000000003528
2015-12-12 16:49:43,221 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3529
2015-12-12 16:49:43,588 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 2428.57 KB/s
2015-12-12 16:49:43,589 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000003528 size 18304 bytes.
2015-12-12 16:49:43,602 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 3300
2015-12-12 16:49:43,602 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000002963, cpktTxId=0000000000000002963)
2015-12-12 16:49:56,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:49:56,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:50:26,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:50:26,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:50:56,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:50:56,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:51:26,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:51:26,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 16:51:56,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:51:56,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 16:52:26,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:52:26,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:52:56,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:52:56,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 16:53:26,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:53:26,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:53:56,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:53:56,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:54:26,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:54:26,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:54:56,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:54:56,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:55:26,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:55:26,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:55:56,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:55:56,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:56:26,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:56:26,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:56:56,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:56:56,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:57:26,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:57:26,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 16:57:56,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:57:56,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 16:58:26,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:58:26,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 16:58:56,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:58:56,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-12 16:59:26,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 16:59:26,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 16:59:56,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 16:59:56,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:00:26,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:00:26,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:00:56,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:00:56,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:01:26,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:01:26,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:01:56,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:01:56,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:02:26,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:02:26,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 17:02:56,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:02:56,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:03:19,386 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 17 
2015-12-12 17:03:19,533 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0009/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742265_1442{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:03:19,711 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742265_1442{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 17:03:19,722 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0009/job.jar is closed by DFSClient_NONMAPREDUCE_259439563_1
2015-12-12 17:03:19,729 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0009/job.jar
2015-12-12 17:03:19,810 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0009/job.split
2015-12-12 17:03:19,823 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0009/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742266_1443{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:03:19,837 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742266_1443{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 17:03:19,839 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0009/job.split is closed by DFSClient_NONMAPREDUCE_259439563_1
2015-12-12 17:03:19,854 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0009/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742267_1444{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:03:19,861 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742267_1444{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 17:03:19,864 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0009/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_259439563_1
2015-12-12 17:03:20,130 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0009/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742268_1445{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:03:20,150 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742268_1445{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 17:03:20,153 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0009/job.xml is closed by DFSClient_NONMAPREDUCE_259439563_1
2015-12-12 17:03:26,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-12 17:03:26,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:03:27,755 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0009/job_1449939365346_0009_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742269_1446{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:03:27,918 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742269_1446{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 17:03:27,924 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0009/job_1449939365346_0009_1_conf.xml is closed by DFSClient_NONMAPREDUCE_308101612_1
2015-12-12 17:03:43,764 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0009/job_1449939365346_0009_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742270_1447{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:03:44,062 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0009/job_1449939365346_0009_1.jhist for DFSClient_NONMAPREDUCE_308101612_1
2015-12-12 17:03:50,650 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_b/_temporary/1/_temporary/attempt_1449939365346_0009_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449939365346_0009_r_000000_0_-1874510134_1
2015-12-12 17:03:50,860 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0009/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_308101612_1
2015-12-12 17:03:50,931 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_b/_SUCCESS is closed by DFSClient_NONMAPREDUCE_308101612_1
2015-12-12 17:03:50,938 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0009/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_308101612_1
2015-12-12 17:03:50,974 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742270_1447{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14245
2015-12-12 17:03:50,979 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0009/job_1449939365346_0009_1.jhist is closed by DFSClient_NONMAPREDUCE_308101612_1
2015-12-12 17:03:50,991 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0009.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742271_1448{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:03:51,007 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742271_1448{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 17:03:51,010 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0009.summary_tmp is closed by DFSClient_NONMAPREDUCE_308101612_1
2015-12-12 17:03:51,065 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0009-1449957800453-parallels-chess+taskB-1449957830940-3-1-SUCCEEDED-default-1449957807292.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742272_1449{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:03:51,074 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742272_1449{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 17:03:51,077 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0009-1449957800453-parallels-chess+taskB-1449957830940-3-1-SUCCEEDED-default-1449957807292.jhist_tmp is closed by DFSClient_NONMAPREDUCE_308101612_1
2015-12-12 17:03:51,119 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0009_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742273_1450{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:03:51,134 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742273_1450{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 17:03:51,137 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0009_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_308101612_1
2015-12-12 17:03:52,196 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742265_1442 127.0.0.1:50010 
2015-12-12 17:03:52,196 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742266_1443 127.0.0.1:50010 
2015-12-12 17:03:52,197 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742267_1444 127.0.0.1:50010 
2015-12-12 17:03:52,197 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742268_1445 127.0.0.1:50010 
2015-12-12 17:03:52,197 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742270_1447 127.0.0.1:50010 
2015-12-12 17:03:52,197 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742269_1446 127.0.0.1:50010 
2015-12-12 17:03:52,988 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742265_1442, blk_1073742266_1443, blk_1073742267_1444, blk_1073742268_1445, blk_1073742269_1446, blk_1073742270_1447]
2015-12-12 17:03:56,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30005 milliseconds
2015-12-12 17:03:56,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 7 millisecond(s).
2015-12-12 17:04:26,257 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:04:26,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:04:56,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:04:56,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:05:26,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:05:26,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:05:56,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:05:56,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:06:26,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:06:26,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:06:56,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:06:56,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:07:26,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:07:26,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:07:56,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:07:56,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:08:26,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:08:26,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:08:56,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:08:56,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:09:26,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:09:26,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:09:56,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:09:56,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 17:10:26,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:10:26,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:10:56,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:10:56,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:11:26,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:11:26,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:11:39,935 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 82 Total time for transactions(ms): 17 Number of transactions batched in Syncs: 0 Number of syncs: 57 SyncTimes(ms): 202 
2015-12-12 17:11:45,191 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0010/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742274_1451{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:11:45,367 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742274_1451{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 17:11:45,374 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0010/job.jar is closed by DFSClient_NONMAPREDUCE_-2022732746_1
2015-12-12 17:11:45,380 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0010/job.jar
2015-12-12 17:11:45,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0010/job.split
2015-12-12 17:11:45,483 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0010/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742275_1452{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:11:45,496 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742275_1452{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 17:11:45,498 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0010/job.split is closed by DFSClient_NONMAPREDUCE_-2022732746_1
2015-12-12 17:11:45,510 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0010/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742276_1453{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:11:45,517 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742276_1453{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 17:11:45,519 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0010/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-2022732746_1
2015-12-12 17:11:45,740 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0010/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742277_1454{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:11:45,759 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742277_1454{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 17:11:45,764 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0010/job.xml is closed by DFSClient_NONMAPREDUCE_-2022732746_1
2015-12-12 17:11:52,488 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0010/job_1449939365346_0010_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742278_1455{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:11:52,616 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742278_1455{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 17:11:52,621 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0010/job_1449939365346_0010_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1955253105_1
2015-12-12 17:11:56,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:11:56,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-12 17:12:08,487 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0010/job_1449939365346_0010_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742279_1456{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:12:08,815 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0010/job_1449939365346_0010_1.jhist for DFSClient_NONMAPREDUCE_1955253105_1
2015-12-12 17:12:15,054 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_b/_temporary/1/_temporary/attempt_1449939365346_0010_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449939365346_0010_r_000000_0_-1463519953_1
2015-12-12 17:12:15,266 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0010/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_1955253105_1
2015-12-12 17:12:15,345 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_b/_SUCCESS is closed by DFSClient_NONMAPREDUCE_1955253105_1
2015-12-12 17:12:15,353 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0010/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_1955253105_1
2015-12-12 17:12:15,388 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742279_1456{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14328
2015-12-12 17:12:15,393 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0010/job_1449939365346_0010_1.jhist is closed by DFSClient_NONMAPREDUCE_1955253105_1
2015-12-12 17:12:15,409 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0010.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742280_1457{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:12:15,418 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742280_1457{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 17:12:15,421 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0010.summary_tmp is closed by DFSClient_NONMAPREDUCE_1955253105_1
2015-12-12 17:12:15,480 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0010-1449958306028-parallels-chess+taskB-1449958335354-3-1-SUCCEEDED-default-1449958312057.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742281_1458{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:12:15,494 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742281_1458{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 17:12:15,497 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0010-1449958306028-parallels-chess+taskB-1449958335354-3-1-SUCCEEDED-default-1449958312057.jhist_tmp is closed by DFSClient_NONMAPREDUCE_1955253105_1
2015-12-12 17:12:15,533 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0010_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742282_1459{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 17:12:15,545 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742282_1459{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 17:12:15,549 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0010_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_1955253105_1
2015-12-12 17:12:16,609 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742274_1451 127.0.0.1:50010 
2015-12-12 17:12:16,611 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742275_1452 127.0.0.1:50010 
2015-12-12 17:12:16,611 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742276_1453 127.0.0.1:50010 
2015-12-12 17:12:16,611 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742277_1454 127.0.0.1:50010 
2015-12-12 17:12:16,611 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742279_1456 127.0.0.1:50010 
2015-12-12 17:12:16,611 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742278_1455 127.0.0.1:50010 
2015-12-12 17:12:17,120 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742274_1451, blk_1073742275_1452, blk_1073742276_1453, blk_1073742277_1454, blk_1073742278_1455, blk_1073742279_1456]
2015-12-12 17:12:26,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30009 milliseconds
2015-12-12 17:12:26,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 29 millisecond(s).
2015-12-12 17:12:56,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:12:56,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:13:26,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:13:26,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:13:56,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:13:56,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:14:26,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:14:26,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:14:56,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:14:56,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:15:26,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:15:26,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:15:56,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:15:56,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:16:26,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:16:26,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:16:56,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:16:56,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:17:26,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:17:26,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:17:56,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:17:56,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:18:26,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:18:26,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:18:56,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:18:56,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:19:26,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:19:26,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:19:56,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:19:56,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:20:26,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:20:26,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:20:56,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:20:56,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:21:26,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:21:26,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:21:56,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:21:56,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:22:26,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:22:26,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:22:56,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:22:56,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:23:26,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:23:26,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:23:56,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:23:56,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:39:57,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:39:57,428 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 17:40:27,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:40:27,428 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:40:57,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:40:57,428 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:41:27,427 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:41:27,428 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:41:57,428 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:41:57,429 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:42:27,429 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:42:27,430 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:42:57,430 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:42:57,430 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:43:27,430 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:43:27,430 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:43:57,430 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:43:57,431 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:44:27,431 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:44:27,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-12 17:44:57,432 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:44:57,432 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:45:27,432 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:45:27,432 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:45:57,432 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:45:57,432 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:46:27,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:46:27,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 17:46:57,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:46:57,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:47:27,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:47:27,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:47:57,438 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30005 milliseconds
2015-12-12 17:47:57,439 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:48:27,440 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:48:27,444 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 4 millisecond(s).
2015-12-12 17:48:57,440 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:48:57,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:49:27,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:49:27,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:49:57,442 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:49:57,442 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:50:27,441 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:50:27,442 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:50:57,442 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:50:57,442 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:51:27,443 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:51:27,443 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:51:57,443 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:51:57,443 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:52:27,442 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:52:27,443 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:52:57,443 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:52:57,444 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:53:27,444 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:53:27,444 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:53:57,444 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:53:57,444 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:54:27,444 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:54:27,444 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:54:57,444 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:54:57,444 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:55:27,445 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:55:27,445 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:55:57,444 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:55:57,445 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:56:27,446 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:56:27,446 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:56:57,445 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:56:57,446 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 17:57:27,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 17:57:27,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:57:57,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:57:57,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:58:27,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:58:27,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:58:57,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:58:57,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:59:27,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:59:27,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 17:59:57,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 17:59:57,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:00:27,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:00:27,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:00:57,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:00:57,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:01:27,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:01:27,448 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:01:57,448 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:01:57,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:02:27,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:02:27,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:02:57,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:02:57,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:03:27,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:03:27,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:03:57,451 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:03:57,451 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:04:27,451 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:04:27,451 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:04:57,452 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:04:57,452 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:05:16,115 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-12 18:05:16,115 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-12 18:05:16,115 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 3529
2015-12-12 18:05:16,115 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 160 Total time for transactions(ms): 29 Number of transactions batched in Syncs: 0 Number of syncs: 113 SyncTimes(ms): 396 
2015-12-12 18:05:16,120 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 160 Total time for transactions(ms): 29 Number of transactions batched in Syncs: 0 Number of syncs: 114 SyncTimes(ms): 401 
2015-12-12 18:05:16,120 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000003529 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000003529-0000000000000003688
2015-12-12 18:05:16,122 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3689
2015-12-12 18:05:16,307 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 6000.00 KB/s
2015-12-12 18:05:16,307 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000003688 size 19199 bytes.
2015-12-12 18:05:16,310 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 3528
2015-12-12 18:05:16,310 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000003300, cpktTxId=0000000000000003300)
2015-12-12 18:05:27,452 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:05:27,452 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:05:57,452 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:05:57,452 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:06:27,451 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:06:27,452 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:06:57,452 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:06:57,452 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:07:27,453 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:07:27,453 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:07:57,452 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:07:57,453 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:08:27,453 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:08:27,454 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:08:57,454 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:08:57,454 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:09:27,453 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:09:27,454 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:09:57,454 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:09:57,454 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:10:27,455 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:10:27,455 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:10:57,455 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:10:57,456 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:11:27,456 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:11:27,456 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:11:57,456 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:11:57,456 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:12:27,455 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:12:27,456 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:12:57,457 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 18:12:57,458 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:13:27,458 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:13:27,458 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:13:57,458 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:13:57,458 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:14:27,459 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:14:27,459 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:14:57,459 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:14:57,460 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:15:27,460 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:15:27,460 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:15:57,460 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:15:57,460 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:16:27,459 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:16:27,460 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:16:57,460 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:16:57,460 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:17:27,461 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:17:27,461 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:17:57,460 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:17:57,461 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:18:27,461 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:18:27,462 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:18:57,462 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:18:57,462 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:19:27,463 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:19:27,463 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:19:57,462 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:19:57,463 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:20:27,464 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:20:27,464 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:20:57,464 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:20:57,464 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:21:27,465 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:21:27,465 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:21:57,464 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:21:57,465 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:22:27,465 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:22:27,465 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:22:57,465 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:22:57,466 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:23:27,466 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:23:27,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:23:57,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:23:57,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:24:27,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:24:27,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:24:57,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:24:57,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:25:27,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:25:27,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:25:57,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:25:57,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:26:27,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:26:27,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:26:57,467 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:26:57,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:27:27,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:27:27,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:27:57,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:27:57,470 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 18:28:27,468 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:28:27,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:28:57,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:28:57,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:29:27,470 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:29:27,470 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:29:57,470 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:29:57,471 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:30:27,470 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:30:27,470 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:30:57,470 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:30:57,471 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:31:27,471 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:31:27,471 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:31:57,471 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:31:57,471 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:32:27,471 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:32:27,471 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:32:57,471 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:32:57,471 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:33:27,470 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:33:27,471 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:33:57,471 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:33:57,471 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:34:27,472 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:34:27,472 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:34:57,471 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:34:57,472 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:35:27,472 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:35:27,472 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:35:57,473 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:35:57,474 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:36:27,474 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:36:27,475 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:36:57,475 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:36:57,476 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:37:27,476 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:37:27,477 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:37:57,477 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:37:57,478 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:38:27,478 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:38:27,478 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:38:50,994 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 16 
2015-12-12 18:38:55,529 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0011/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742283_1460{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:38:55,701 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742283_1460{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 18:38:55,705 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0011/job.jar is closed by DFSClient_NONMAPREDUCE_1328244293_1
2015-12-12 18:38:55,711 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0011/job.jar
2015-12-12 18:38:55,784 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0011/job.split
2015-12-12 18:38:55,797 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0011/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742284_1461{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:38:55,809 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742284_1461{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 18:38:55,812 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0011/job.split is closed by DFSClient_NONMAPREDUCE_1328244293_1
2015-12-12 18:38:55,826 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0011/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742285_1462{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:38:55,833 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742285_1462{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 18:38:55,835 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0011/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1328244293_1
2015-12-12 18:38:56,074 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0011/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742286_1463{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:38:56,089 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742286_1463{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 18:38:56,091 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0011/job.xml is closed by DFSClient_NONMAPREDUCE_1328244293_1
2015-12-12 18:38:57,477 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:38:57,478 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:39:03,224 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0011/job_1449939365346_0011_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742287_1464{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:39:03,354 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742287_1464{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 18:39:03,360 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0011/job_1449939365346_0011_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-196434767_1
2015-12-12 18:39:19,318 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0011/job_1449939365346_0011_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742288_1465{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:39:19,375 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0011/job_1449939365346_0011_1.jhist for DFSClient_NONMAPREDUCE_-196434767_1
2015-12-12 18:39:27,479 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 18:39:27,485 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 6 millisecond(s).
2015-12-12 18:39:27,844 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_b/_temporary/1/_temporary/attempt_1449939365346_0011_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449939365346_0011_r_000000_0_1853165518_1
2015-12-12 18:39:28,040 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0011/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-196434767_1
2015-12-12 18:39:28,077 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_b/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-196434767_1
2015-12-12 18:39:28,083 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0011/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-196434767_1
2015-12-12 18:39:28,115 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742288_1465{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14328
2015-12-12 18:39:28,118 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0011/job_1449939365346_0011_1.jhist is closed by DFSClient_NONMAPREDUCE_-196434767_1
2015-12-12 18:39:28,127 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0011.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742289_1466{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:39:28,134 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742289_1466{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 18:39:28,138 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0011.summary_tmp is closed by DFSClient_NONMAPREDUCE_-196434767_1
2015-12-12 18:39:28,188 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0011-1449963536354-parallels-chess+taskB-1449963568084-3-1-SUCCEEDED-default-1449963542825.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742290_1467{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:39:28,199 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742290_1467{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 18:39:28,202 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0011-1449963536354-parallels-chess+taskB-1449963568084-3-1-SUCCEEDED-default-1449963542825.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-196434767_1
2015-12-12 18:39:28,303 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0011_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742291_1468{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:39:28,323 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742291_1468{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 18:39:28,325 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0011_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-196434767_1
2015-12-12 18:39:29,384 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742283_1460 127.0.0.1:50010 
2015-12-12 18:39:29,384 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742284_1461 127.0.0.1:50010 
2015-12-12 18:39:29,384 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742285_1462 127.0.0.1:50010 
2015-12-12 18:39:29,385 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742286_1463 127.0.0.1:50010 
2015-12-12 18:39:29,385 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742288_1465 127.0.0.1:50010 
2015-12-12 18:39:29,385 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742287_1464 127.0.0.1:50010 
2015-12-12 18:39:31,382 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742288_1465, blk_1073742283_1460, blk_1073742284_1461, blk_1073742285_1462, blk_1073742286_1463, blk_1073742287_1464]
2015-12-12 18:39:57,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:39:57,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:40:27,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:40:27,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:40:57,480 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:40:57,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:41:27,481 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:41:27,482 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:41:56,053 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 83 Total time for transactions(ms): 24 Number of transactions batched in Syncs: 0 Number of syncs: 58 SyncTimes(ms): 80 
2015-12-12 18:41:57,484 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-12 18:41:57,485 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:42:00,210 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0012/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742292_1469{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:42:00,374 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742292_1469{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 18:42:00,379 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0012/job.jar is closed by DFSClient_NONMAPREDUCE_144315395_1
2015-12-12 18:42:00,386 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0012/job.jar
2015-12-12 18:42:00,444 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0012/job.split
2015-12-12 18:42:00,460 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0012/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742293_1470{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:42:00,481 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742293_1470{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 18:42:00,489 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0012/job.split is closed by DFSClient_NONMAPREDUCE_144315395_1
2015-12-12 18:42:00,503 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0012/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742294_1471{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:42:00,511 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742294_1471{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 18:42:00,516 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0012/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_144315395_1
2015-12-12 18:42:00,706 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0012/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742295_1472{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:42:00,730 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742295_1472{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 18:42:00,732 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0012/job.xml is closed by DFSClient_NONMAPREDUCE_144315395_1
2015-12-12 18:42:05,959 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0012/job_1449939365346_0012_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742296_1473{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:42:06,063 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742296_1473{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 18:42:06,068 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0012/job_1449939365346_0012_1_conf.xml is closed by DFSClient_NONMAPREDUCE_811875631_1
2015-12-12 18:42:23,059 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0012/job_1449939365346_0012_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742297_1474{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:42:23,146 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0012/job_1449939365346_0012_1.jhist for DFSClient_NONMAPREDUCE_811875631_1
2015-12-12 18:42:27,485 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:42:27,486 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:42:32,777 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_b/_temporary/1/_temporary/attempt_1449939365346_0012_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449939365346_0012_r_000000_0_-1433180349_1
2015-12-12 18:42:32,948 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0012/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_811875631_1
2015-12-12 18:42:32,991 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_b/_SUCCESS is closed by DFSClient_NONMAPREDUCE_811875631_1
2015-12-12 18:42:33,000 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0012/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_811875631_1
2015-12-12 18:42:33,040 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742297_1474{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14329
2015-12-12 18:42:33,045 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0012/job_1449939365346_0012_1.jhist is closed by DFSClient_NONMAPREDUCE_811875631_1
2015-12-12 18:42:33,055 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0012.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742298_1475{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:42:33,066 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742298_1475{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 18:42:33,071 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0012.summary_tmp is closed by DFSClient_NONMAPREDUCE_811875631_1
2015-12-12 18:42:33,132 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0012-1449963720959-parallels-chess+taskB-1449963753003-3-1-SUCCEEDED-default-1449963725621.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742299_1476{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:42:33,138 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742299_1476{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 18:42:33,144 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0012-1449963720959-parallels-chess+taskB-1449963753003-3-1-SUCCEEDED-default-1449963725621.jhist_tmp is closed by DFSClient_NONMAPREDUCE_811875631_1
2015-12-12 18:42:33,174 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0012_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742300_1477{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 18:42:33,186 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742300_1477{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 18:42:33,194 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0012_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_811875631_1
2015-12-12 18:42:34,259 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742292_1469 127.0.0.1:50010 
2015-12-12 18:42:34,259 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742293_1470 127.0.0.1:50010 
2015-12-12 18:42:34,259 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742294_1471 127.0.0.1:50010 
2015-12-12 18:42:34,259 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742295_1472 127.0.0.1:50010 
2015-12-12 18:42:34,260 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742297_1474 127.0.0.1:50010 
2015-12-12 18:42:34,260 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742296_1473 127.0.0.1:50010 
2015-12-12 18:42:35,100 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742292_1469, blk_1073742293_1470, blk_1073742294_1471, blk_1073742295_1472, blk_1073742296_1473, blk_1073742297_1474]
2015-12-12 18:42:57,487 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:42:57,487 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:43:27,486 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:43:27,487 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:43:57,487 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:43:57,487 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:44:27,487 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:44:27,487 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:44:57,488 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:44:57,488 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:45:27,490 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 18:45:27,490 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:45:57,490 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:45:57,491 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:46:27,491 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:46:27,491 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:46:57,491 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:46:57,491 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:47:27,492 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:47:27,492 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:47:57,492 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:47:57,492 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:48:27,493 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:48:27,493 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:48:57,493 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:48:57,494 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:49:27,494 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:49:27,494 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:49:57,496 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:49:57,496 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:50:27,496 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:50:27,496 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:50:57,497 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 18:50:57,498 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:51:27,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:51:27,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:51:57,498 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:51:57,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:52:27,498 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:52:27,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:52:57,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:52:57,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:53:27,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:53:27,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:53:57,498 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:53:57,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:54:27,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:54:27,500 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:54:57,500 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:54:57,500 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:55:27,500 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:55:27,500 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:55:57,502 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 18:55:57,502 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:56:27,503 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:56:27,503 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:56:57,503 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:56:57,503 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:57:27,502 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:57:27,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:57:57,503 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:57:57,503 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:58:27,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:58:27,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:58:57,504 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 18:58:57,505 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 18:59:27,505 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:59:27,506 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 18:59:57,505 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 18:59:57,505 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 19:00:27,505 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 19:00:27,505 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 19:00:57,505 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 19:00:57,506 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 19:01:27,506 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 19:01:27,507 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 19:01:57,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 19:01:57,509 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 19:02:27,509 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 19:02:27,510 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 19:02:57,510 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 19:02:57,511 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 19:03:27,511 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 19:03:27,512 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 19:03:57,513 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 19:03:57,514 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 19:04:27,514 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 19:04:27,514 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 19:04:57,515 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 19:04:57,515 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 19:05:18,397 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-12 19:05:18,397 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-12 19:05:18,397 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 3689
2015-12-12 19:05:18,397 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 161 Total time for transactions(ms): 34 Number of transactions batched in Syncs: 0 Number of syncs: 114 SyncTimes(ms): 172 
2015-12-12 19:05:18,398 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 161 Total time for transactions(ms): 34 Number of transactions batched in Syncs: 0 Number of syncs: 115 SyncTimes(ms): 173 
2015-12-12 19:05:18,400 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000003689 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000003689-0000000000000003849
2015-12-12 19:05:18,400 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3850
2015-12-12 19:05:18,625 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 6333.33 KB/s
2015-12-12 19:05:18,625 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000003849 size 19915 bytes.
2015-12-12 19:05:18,630 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 3688
2015-12-12 19:05:18,630 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000003528, cpktTxId=0000000000000003528)
2015-12-12 19:05:27,514 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 19:05:27,515 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 19:05:57,516 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 19:05:57,516 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 19:06:27,516 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 19:06:27,517 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 19:06:57,517 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 19:06:57,518 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 19:07:27,518 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 19:07:27,518 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 19:07:57,519 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 19:07:57,519 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 19:08:27,519 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 19:08:27,520 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 19:08:57,520 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 19:08:57,520 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 20:12:22,432 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* removeDeadDatanode: lost heartbeat from 127.0.0.1:50010
2015-12-12 20:12:22,461 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/127.0.0.1:50010
2015-12-12 20:12:23,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 20:12:23,774 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 20:12:23,830 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0) storage 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-12 20:12:23,830 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/127.0.0.1:50010
2015-12-12 20:12:23,830 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-12 20:12:23,830 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-12 20:12:23,831 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: [DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010 failed.
2015-12-12 20:12:23,837 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-12 20:12:23,848 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 157, hasStaleStorages: false, processing time: 4 msecs
2015-12-12 20:12:53,774 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 20:12:53,775 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 20:13:23,774 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 20:13:23,775 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 20:13:53,775 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 20:13:53,775 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 20:14:23,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 20:14:23,777 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 20:14:53,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 20:14:53,777 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 20:15:23,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 20:15:23,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 20:15:53,777 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 20:15:53,777 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 20:16:23,777 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 20:16:23,778 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 20:16:53,778 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 20:16:53,778 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 20:17:23,779 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 20:17:23,780 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 20:17:53,779 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 20:17:53,780 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 20:18:23,779 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 20:18:23,780 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 20:18:53,780 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 20:18:53,780 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 20:19:23,781 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 20:19:23,781 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 20:19:53,782 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 20:19:53,783 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 20:20:23,781 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 20:20:23,782 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 20:20:53,781 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 20:20:53,782 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 20:21:23,782 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 20:21:23,782 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 20:21:53,783 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 20:21:53,784 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 20:22:23,783 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 20:22:23,784 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 20:22:53,783 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 20:22:53,784 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 20:23:23,786 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-12 20:23:23,787 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 21:59:39,376 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 21:59:39,378 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 22:00:09,378 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 22:00:09,378 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:00:39,379 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:00:39,379 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:01:09,379 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:01:09,380 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:01:39,380 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:01:39,381 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:02:09,381 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:02:09,381 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:02:39,382 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:02:39,382 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:03:09,382 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:03:09,383 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:03:39,382 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:03:39,383 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:04:09,383 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:04:09,383 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:04:39,384 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:04:39,384 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:05:09,384 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:05:09,385 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:05:39,384 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:05:39,385 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:06:09,385 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:06:09,385 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:06:39,386 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:06:39,387 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:07:09,386 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:07:09,387 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:07:39,386 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:07:39,387 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:08:09,386 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:08:09,386 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:08:39,386 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:08:39,387 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:09:09,387 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:09:09,387 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:09:39,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:09:39,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:10:09,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:10:09,389 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:10:39,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:10:39,389 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:11:09,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:11:09,389 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:11:39,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:11:39,389 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:12:09,388 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:12:09,389 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:12:39,389 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:12:39,389 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:13:09,390 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:13:09,391 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:13:39,391 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:13:39,391 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:14:09,391 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:14:09,391 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:14:39,392 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:14:39,392 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:15:09,392 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:15:09,393 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:15:39,393 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:15:39,393 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:16:09,393 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:16:09,394 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:16:39,393 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:16:39,394 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:17:09,394 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:17:09,394 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:17:39,394 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:17:39,395 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:18:09,395 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:18:09,395 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:18:39,395 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:18:39,395 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:19:09,396 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:19:09,397 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:19:39,396 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:19:39,397 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:20:09,396 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:20:09,396 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:20:39,397 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:20:39,398 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:21:09,398 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:21:09,400 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 22:21:39,399 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:21:39,399 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:22:09,400 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:22:09,400 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:22:39,401 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:22:39,401 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:23:09,401 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:23:09,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:23:39,401 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:23:39,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:24:09,401 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:24:09,401 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:24:39,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:24:39,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:25:09,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:25:09,403 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:25:39,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:25:39,403 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:26:09,403 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:26:09,403 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:26:39,403 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:26:39,403 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:27:09,403 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:27:09,404 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:27:39,403 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:27:39,404 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:28:09,404 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:28:09,405 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:28:39,405 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:28:39,406 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:29:09,406 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:29:09,406 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:29:39,407 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:29:39,407 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:30:09,407 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:30:09,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:30:39,407 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:30:39,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:31:09,407 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:31:09,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:31:39,407 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:31:39,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:32:09,407 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:32:09,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:32:39,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:32:39,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:33:09,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:33:09,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:33:39,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:33:39,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:34:09,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:34:09,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:34:39,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:34:39,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:35:09,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:35:09,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:35:39,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:35:39,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:36:09,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:36:09,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:36:39,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:36:39,412 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:37:09,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:37:09,412 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:37:39,412 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:37:39,412 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:38:09,412 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:38:09,412 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:38:39,413 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:38:39,413 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:39:09,414 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:39:09,414 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:39:39,413 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:39:39,414 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:40:09,414 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:40:09,414 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:40:39,414 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:40:39,415 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:41:09,415 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:41:09,415 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:41:39,415 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:41:39,416 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:42:09,416 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:42:09,416 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:42:17,306 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1765ms
No GCs detected
2015-12-12 22:51:30,158 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:51:30,158 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:52:00,159 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:52:00,159 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:52:30,158 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:52:30,159 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:52:51,935 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-12 22:52:51,935 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-12 22:52:51,935 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 3850
2015-12-12 22:52:51,936 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 13 
2015-12-12 22:52:51,938 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 15 
2015-12-12 22:52:51,939 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000003850 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000003850-0000000000000003851
2015-12-12 22:52:51,942 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3852
2015-12-12 22:52:52,019 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 4750.00 KB/s
2015-12-12 22:52:52,020 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000003851 size 19915 bytes.
2015-12-12 22:52:52,023 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 3849
2015-12-12 22:52:52,023 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000003688, cpktTxId=0000000000000003688)
2015-12-12 22:53:00,159 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:53:00,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:53:30,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:53:30,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:54:00,159 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:54:00,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:54:30,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:54:30,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:55:00,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:55:00,162 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 22:55:30,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:55:30,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:56:00,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:56:00,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:56:30,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:56:30,162 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:57:00,163 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 22:57:00,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:57:30,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:57:30,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:58:00,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:58:00,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:58:30,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:58:30,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 22:59:00,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 22:59:00,167 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 22:59:30,167 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 22:59:30,167 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:00:00,167 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:00:00,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:00:30,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:00:30,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:01:00,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:01:00,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:01:30,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:01:30,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:02:00,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:02:00,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:02:30,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:02:30,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:03:00,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:03:00,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:03:30,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:03:30,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:04:00,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:04:00,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:04:30,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:04:30,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:05:00,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:05:00,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:05:30,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:05:30,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:06:00,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:06:00,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:06:30,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:06:30,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:07:00,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:07:00,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:07:30,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:07:30,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:08:00,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:08:00,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:08:30,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:08:30,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:09:00,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:09:00,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:09:30,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:09:30,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:10:00,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:10:00,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:10:30,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:10:30,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:11:00,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:11:00,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:11:30,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:11:30,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:12:00,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:12:00,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:12:30,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:12:30,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:13:00,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:13:00,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:13:30,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:13:30,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:14:00,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:14:00,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:14:30,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:14:30,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:15:00,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:15:00,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:15:30,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:15:30,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:16:00,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:16:00,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:16:30,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:16:30,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:17:00,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:17:00,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:17:30,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:17:30,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:18:00,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:18:00,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:18:30,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:18:30,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:19:00,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:19:00,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 23:19:30,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:19:30,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:20:00,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:20:00,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:20:30,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:20:30,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:21:00,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:21:00,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:21:30,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:21:30,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:22:00,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:22:00,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 23:22:30,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:22:30,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:23:00,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:23:00,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:23:30,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:23:30,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:24:00,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:24:00,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:24:30,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:24:30,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:25:00,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:25:00,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 23:25:30,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 23:25:30,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:26:00,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:26:00,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:26:30,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:26:30,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:27:00,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:27:00,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:27:30,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:27:30,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:28:00,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:28:00,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:28:10,551 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 5 
2015-12-12 23:28:21,007 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0013/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742301_1478{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:28:21,154 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742301_1478{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 23:28:21,159 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0013/job.jar is closed by DFSClient_NONMAPREDUCE_-1110650358_1
2015-12-12 23:28:21,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0013/job.jar
2015-12-12 23:28:21,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0013/job.split
2015-12-12 23:28:21,270 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0013/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742302_1479{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:28:21,290 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742302_1479{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-12 23:28:21,292 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742302_1479{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 445
2015-12-12 23:28:21,695 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0013/job.split is closed by DFSClient_NONMAPREDUCE_-1110650358_1
2015-12-12 23:28:21,706 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0013/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742303_1480{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:28:21,713 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742303_1480{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 23:28:21,716 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0013/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1110650358_1
2015-12-12 23:28:21,959 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0013/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742304_1481{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:28:21,989 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742304_1481{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 23:28:21,994 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0013/job.xml is closed by DFSClient_NONMAPREDUCE_-1110650358_1
2015-12-12 23:28:29,362 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0013/job_1449939365346_0013_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742305_1482{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:28:29,491 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742305_1482{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 23:28:29,505 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0013/job_1449939365346_0013_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1482804065_1
2015-12-12 23:28:30,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30005 milliseconds
2015-12-12 23:28:30,203 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 7 millisecond(s).
2015-12-12 23:28:44,608 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0013/job_1449939365346_0013_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742306_1483{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:28:44,706 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0013/job_1449939365346_0013_1.jhist for DFSClient_NONMAPREDUCE_-1482804065_1
2015-12-12 23:28:53,480 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_b/_temporary/1/_temporary/attempt_1449939365346_0013_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449939365346_0013_r_000000_0_-1910282508_1
2015-12-12 23:28:54,688 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0013/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1482804065_1
2015-12-12 23:28:54,729 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_b/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1482804065_1
2015-12-12 23:28:54,737 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0013/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1482804065_1
2015-12-12 23:28:54,773 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742306_1483{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14328
2015-12-12 23:28:54,776 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0013/job_1449939365346_0013_1.jhist is closed by DFSClient_NONMAPREDUCE_-1482804065_1
2015-12-12 23:28:54,785 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0013.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742307_1484{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:28:54,794 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742307_1484{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 23:28:54,796 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0013.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1482804065_1
2015-12-12 23:28:54,831 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0013-1449980902266-parallels-chess+taskB-1449980934741-3-1-SUCCEEDED-default-1449980908925.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742308_1485{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:28:54,872 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742308_1485{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 23:28:54,875 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0013-1449980902266-parallels-chess+taskB-1449980934741-3-1-SUCCEEDED-default-1449980908925.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1482804065_1
2015-12-12 23:28:54,894 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0013_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742309_1486{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:28:54,901 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742309_1486{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 23:28:54,904 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0013_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1482804065_1
2015-12-12 23:28:55,952 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742301_1478 127.0.0.1:50010 
2015-12-12 23:28:55,953 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742302_1479 127.0.0.1:50010 
2015-12-12 23:28:55,953 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742303_1480 127.0.0.1:50010 
2015-12-12 23:28:55,953 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742304_1481 127.0.0.1:50010 
2015-12-12 23:28:55,953 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742306_1483 127.0.0.1:50010 
2015-12-12 23:28:55,953 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742305_1482 127.0.0.1:50010 
2015-12-12 23:28:56,216 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742304_1481, blk_1073742305_1482, blk_1073742306_1483, blk_1073742301_1478, blk_1073742302_1479, blk_1073742303_1480]
2015-12-12 23:29:00,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:29:00,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:29:30,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:29:30,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:30:00,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:30:00,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:30:30,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:30:30,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:31:00,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:31:00,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:31:30,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:31:30,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:32:00,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:32:00,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:32:30,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:32:30,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:33:00,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:33:00,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:33:30,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:33:30,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:34:00,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:34:00,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-12 23:34:30,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 23:34:30,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:35:00,203 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:35:00,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:35:30,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:35:30,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:36:00,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:36:00,206 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:36:30,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:36:30,207 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:36:32,485 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 83 Total time for transactions(ms): 22 Number of transactions batched in Syncs: 1 Number of syncs: 58 SyncTimes(ms): 80 
2015-12-12 23:36:36,904 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0014/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742310_1487{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:36:37,027 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742310_1487{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 23:36:37,031 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0014/job.jar is closed by DFSClient_NONMAPREDUCE_440718098_1
2015-12-12 23:36:37,036 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0014/job.jar
2015-12-12 23:36:37,100 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0014/job.split
2015-12-12 23:36:37,113 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0014/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742311_1488{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:36:37,126 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742311_1488{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 23:36:37,133 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0014/job.split is closed by DFSClient_NONMAPREDUCE_440718098_1
2015-12-12 23:36:37,145 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0014/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742312_1489{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:36:37,156 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742312_1489{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 23:36:37,158 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0014/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_440718098_1
2015-12-12 23:36:37,336 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0014/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742313_1490{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:36:37,361 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742313_1490{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 23:36:37,363 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0014/job.xml is closed by DFSClient_NONMAPREDUCE_440718098_1
2015-12-12 23:36:42,915 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0014/job_1449939365346_0014_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742314_1491{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:36:43,076 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742314_1491{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 23:36:43,081 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0014/job_1449939365346_0014_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1334103886_1
2015-12-12 23:36:58,003 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0014/job_1449939365346_0014_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742315_1492{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:36:58,225 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0014/job_1449939365346_0014_1.jhist for DFSClient_NONMAPREDUCE_-1334103886_1
2015-12-12 23:37:00,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30005 milliseconds
2015-12-12 23:37:00,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 12 millisecond(s).
2015-12-12 23:37:08,543 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_b/_temporary/1/_temporary/attempt_1449939365346_0014_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742316_1493{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:37:09,141 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742316_1493{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 23:37:09,194 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_b/_temporary/1/_temporary/attempt_1449939365346_0014_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449939365346_0014_r_000000_0_1603178884_1
2015-12-12 23:37:10,475 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0014/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1334103886_1
2015-12-12 23:37:10,540 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_b/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1334103886_1
2015-12-12 23:37:10,550 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0014/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1334103886_1
2015-12-12 23:37:10,591 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742315_1492{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14351
2015-12-12 23:37:10,594 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0014/job_1449939365346_0014_1.jhist is closed by DFSClient_NONMAPREDUCE_-1334103886_1
2015-12-12 23:37:10,608 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0014.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742317_1494{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:37:10,616 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742317_1494{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 23:37:10,621 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0014.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1334103886_1
2015-12-12 23:37:10,672 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0014-1449981397572-parallels-chess+taskB-1449981430553-3-1-SUCCEEDED-default-1449981402514.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742318_1495{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:37:10,681 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742318_1495{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 23:37:10,684 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0014-1449981397572-parallels-chess+taskB-1449981430553-3-1-SUCCEEDED-default-1449981402514.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1334103886_1
2015-12-12 23:37:10,710 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0014_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742319_1496{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-12 23:37:10,722 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742319_1496{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-12 23:37:10,724 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0014_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1334103886_1
2015-12-12 23:37:11,776 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742310_1487 127.0.0.1:50010 
2015-12-12 23:37:11,776 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742311_1488 127.0.0.1:50010 
2015-12-12 23:37:11,776 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742312_1489 127.0.0.1:50010 
2015-12-12 23:37:11,777 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742313_1490 127.0.0.1:50010 
2015-12-12 23:37:11,777 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742315_1492 127.0.0.1:50010 
2015-12-12 23:37:11,777 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742314_1491 127.0.0.1:50010 
2015-12-12 23:37:14,363 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742310_1487, blk_1073742311_1488, blk_1073742312_1489, blk_1073742313_1490, blk_1073742314_1491, blk_1073742315_1492]
2015-12-12 23:37:30,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:37:30,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:38:00,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:38:00,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:38:30,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:38:30,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:39:00,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:39:00,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:39:30,216 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-12 23:39:30,216 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:40:00,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:40:00,216 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:40:30,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:40:30,216 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:41:00,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:41:00,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:41:30,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:41:30,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:42:00,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:42:00,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:42:30,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:42:30,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:43:00,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:43:00,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:43:30,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:43:30,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:44:00,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:44:00,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:44:30,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:44:30,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:45:00,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:45:00,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:45:30,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:45:30,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:46:00,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:46:00,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:46:30,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:46:30,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:47:00,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:47:00,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:47:30,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:47:30,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:48:00,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:48:00,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:48:30,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:48:30,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:58:51,896 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-12 23:58:51,897 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-12 23:59:21,897 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:59:21,897 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-12 23:59:51,899 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-12 23:59:51,899 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 00:00:21,898 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:00:21,900 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:00:51,900 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:00:51,900 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:01:21,900 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:01:21,901 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:01:51,901 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:01:51,901 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:02:21,901 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:02:21,901 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:02:45,142 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-13 00:02:45,142 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-13 00:02:45,142 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 3852
2015-12-13 00:02:45,142 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 164 Total time for transactions(ms): 31 Number of transactions batched in Syncs: 1 Number of syncs: 115 SyncTimes(ms): 246 
2015-12-13 00:02:45,143 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 164 Total time for transactions(ms): 31 Number of transactions batched in Syncs: 1 Number of syncs: 116 SyncTimes(ms): 247 
2015-12-13 00:02:45,145 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000003852 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000003852-0000000000000004015
2015-12-13 00:02:45,147 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 4016
2015-12-13 00:02:45,448 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 4000.00 KB/s
2015-12-13 00:02:45,448 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000004015 size 20646 bytes.
2015-12-13 00:02:45,461 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 3851
2015-12-13 00:02:45,461 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000003849, cpktTxId=0000000000000003849)
2015-12-13 00:02:51,901 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:02:51,902 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 00:03:21,901 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:03:21,901 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:03:51,900 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:03:51,901 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:04:21,901 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:04:21,902 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:04:51,902 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:04:51,903 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 00:05:21,902 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:05:21,902 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:05:51,903 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:05:51,905 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 00:06:21,903 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:06:21,903 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:06:51,904 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:06:51,905 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 00:07:21,905 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:07:21,905 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:07:51,905 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:07:51,906 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:08:21,906 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:08:21,907 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:08:51,906 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:08:51,907 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:09:21,907 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:09:21,907 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:09:51,907 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:09:51,908 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:10:21,908 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:10:21,909 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:10:51,908 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:10:51,909 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:11:21,908 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:11:21,909 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:11:51,908 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:11:51,909 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:12:21,908 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:12:21,909 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:12:51,909 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:12:51,910 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:13:21,910 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:13:21,911 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:13:51,911 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:13:51,911 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:14:21,911 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:14:21,912 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:14:51,912 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:14:51,913 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:15:21,912 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:15:21,912 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:15:51,912 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:15:51,912 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:16:21,912 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:16:21,913 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:16:51,912 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:16:51,913 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:17:21,913 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:17:21,913 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:17:51,914 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:17:51,914 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:18:21,914 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:18:21,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:18:51,916 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:18:51,916 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:19:21,916 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:19:21,916 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:19:51,916 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:19:51,917 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:20:21,917 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:20:21,917 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:20:51,917 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:20:51,917 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:21:21,917 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:21:21,918 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:21:51,918 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:21:51,919 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:22:21,919 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:22:21,919 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:22:51,919 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:22:51,919 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:23:21,919 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:23:21,919 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:23:51,919 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:23:51,919 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:24:21,919 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:24:21,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:24:51,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:24:51,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:25:21,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:25:21,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:25:51,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:25:51,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:26:21,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:26:21,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:26:51,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:26:51,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:27:21,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:27:21,922 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:27:51,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:27:51,922 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:28:21,922 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:28:21,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:28:51,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:28:51,924 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:29:21,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:29:21,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:29:45,021 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 16 
2015-12-13 00:29:45,154 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0015/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742320_1497{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 00:29:45,304 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742320_1497{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 00:29:45,310 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0015/job.jar is closed by DFSClient_NONMAPREDUCE_-677479107_1
2015-12-13 00:29:45,316 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0015/job.jar
2015-12-13 00:29:45,397 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0015/job.split
2015-12-13 00:29:45,414 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0015/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742321_1498{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 00:29:45,421 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742321_1498{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 00:29:45,428 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0015/job.split is closed by DFSClient_NONMAPREDUCE_-677479107_1
2015-12-13 00:29:45,439 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0015/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742322_1499{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 00:29:45,445 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742322_1499{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 00:29:45,448 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0015/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-677479107_1
2015-12-13 00:29:45,702 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0015/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742323_1500{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 00:29:45,717 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742323_1500{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 00:29:45,722 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0015/job.xml is closed by DFSClient_NONMAPREDUCE_-677479107_1
2015-12-13 00:29:51,922 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:29:51,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 00:29:52,384 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0015/job_1449939365346_0015_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742324_1501{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 00:29:52,522 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742324_1501{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 00:29:52,529 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0015/job_1449939365346_0015_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-795535398_1
2015-12-13 00:30:08,773 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0015/job_1449939365346_0015_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742325_1502{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 00:30:09,306 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0015/job_1449939365346_0015_1.jhist for DFSClient_NONMAPREDUCE_-795535398_1
2015-12-13 00:30:16,135 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1449939365346_0015_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742326_1503{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 00:30:16,277 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742326_1503{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 00:30:16,282 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1449939365346_0015_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1449939365346_0015_r_000000_0_-1490571040_1
2015-12-13 00:30:17,625 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0015/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-795535398_1
2015-12-13 00:30:17,702 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-795535398_1
2015-12-13 00:30:17,709 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0015/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-795535398_1
2015-12-13 00:30:17,737 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742325_1502{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14344
2015-12-13 00:30:17,740 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1449939365346_0015/job_1449939365346_0015_1.jhist is closed by DFSClient_NONMAPREDUCE_-795535398_1
2015-12-13 00:30:17,752 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0015.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742327_1504{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 00:30:17,761 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742327_1504{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 00:30:17,764 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0015.summary_tmp is closed by DFSClient_NONMAPREDUCE_-795535398_1
2015-12-13 00:30:17,820 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0015-1449984586006-parallels-word+count-1449984617710-3-1-SUCCEEDED-default-1449984591917.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742328_1505{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 00:30:17,830 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742328_1505{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 00:30:17,833 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0015-1449984586006-parallels-word+count-1449984617710-3-1-SUCCEEDED-default-1449984591917.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-795535398_1
2015-12-13 00:30:17,863 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0015_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742329_1506{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 00:30:17,872 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742329_1506{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 00:30:17,875 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1449939365346_0015_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-795535398_1
2015-12-13 00:30:18,948 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742320_1497 127.0.0.1:50010 
2015-12-13 00:30:18,949 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742321_1498 127.0.0.1:50010 
2015-12-13 00:30:18,949 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742322_1499 127.0.0.1:50010 
2015-12-13 00:30:18,949 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742323_1500 127.0.0.1:50010 
2015-12-13 00:30:18,949 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742325_1502 127.0.0.1:50010 
2015-12-13 00:30:18,949 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742324_1501 127.0.0.1:50010 
2015-12-13 00:30:21,647 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742320_1497, blk_1073742321_1498, blk_1073742322_1499, blk_1073742323_1500, blk_1073742324_1501, blk_1073742325_1502]
2015-12-13 00:30:21,927 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-13 00:30:21,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 9 millisecond(s).
2015-12-13 00:30:51,927 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 00:30:51,928 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:31:21,927 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 00:31:21,927 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 00:31:23,585 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-13 00:31:23,690 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-13 10:34:12,465 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-13 10:34:12,475 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-13 10:34:12,478 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-13 10:34:12,924 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-13 10:34:13,015 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-13 10:34:13,015 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-13 10:34:13,016 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-13 10:34:13,017 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-13 10:34:13,415 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-13 10:34:13,476 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-13 10:34:13,482 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-13 10:34:13,493 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-13 10:34:13,495 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-13 10:34:13,496 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-13 10:34:13,496 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-13 10:34:13,534 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-13 10:34:13,536 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-13 10:34:13,586 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-13 10:34:13,586 INFO org.mortbay.log: jetty-6.1.26
2015-12-13 10:34:13,929 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-13 10:34:13,999 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-13 10:34:13,999 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-13 10:34:14,048 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-13 10:34:14,054 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-13 10:34:14,097 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-13 10:34:14,097 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-13 10:34:14,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-13 10:34:14,101 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 13 10:34:14
2015-12-13 10:34:14,102 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-13 10:34:14,103 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-13 10:34:14,106 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-13 10:34:14,106 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-13 10:34:14,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-13 10:34:14,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-13 10:34:14,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-13 10:34:14,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-13 10:34:14,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-13 10:34:14,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-13 10:34:14,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-13 10:34:14,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-13 10:34:14,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-13 10:34:14,145 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-13 10:34:14,145 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-13 10:34:14,145 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-13 10:34:14,145 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-13 10:34:14,147 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-13 10:34:14,304 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-13 10:34:14,304 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-13 10:34:14,305 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-13 10:34:14,305 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-13 10:34:14,307 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-13 10:34:14,319 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-13 10:34:14,319 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-13 10:34:14,319 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-13 10:34:14,319 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-13 10:34:14,320 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-13 10:34:14,320 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-13 10:34:14,320 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-13 10:34:14,321 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-13 10:34:14,322 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-13 10:34:14,324 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-13 10:34:14,324 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-13 10:34:14,324 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-13 10:34:14,324 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-13 10:34:14,327 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-13 10:34:14,327 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-13 10:34:14,327 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-13 10:34:14,334 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/name/in_use.lock acquired by nodename 23748@ubuntu
2015-12-13 10:34:14,484 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/parallels/hadoop/tmp/dfs/name/current
2015-12-13 10:34:14,592 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000004016 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000004016-0000000000000004099
2015-12-13 10:34:14,633 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 185 INodes.
2015-12-13 10:34:14,684 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-13 10:34:14,684 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 4015 from /home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000004015
2015-12-13 10:34:14,684 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@47874885 expecting start txid #4016
2015-12-13 10:34:14,684 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000004016-0000000000000004099
2015-12-13 10:34:14,686 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000004016-0000000000000004099' to transaction ID 4016
2015-12-13 10:34:14,716 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000004016-0000000000000004099 of size 1048576 edits # 84 loaded in 0 seconds
2015-12-13 10:34:14,717 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2015-12-13 10:34:14,718 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2015-12-13 10:34:14,786 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 4015
2015-12-13 10:34:14,786 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000003851, cpktTxId=0000000000000003851)
2015-12-13 10:34:14,805 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 4100
2015-12-13 10:34:14,844 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-13 10:34:14,844 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 517 msecs
2015-12-13 10:34:15,009 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-13 10:34:15,014 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-13 10:34:15,024 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-13 10:34:15,054 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-13 10:34:15,064 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-13 10:34:15,064 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-13 10:34:15,065 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 168 blocks to reach the threshold 0.9990 of total blocks 168.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2015-12-13 10:34:15,096 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-13 10:34:15,101 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-13 10:34:15,103 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-13 10:34:15,103 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-13 10:34:15,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-13 10:34:15,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 102238071 milliseconds
2015-12-13 10:34:15,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:34:18,681 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0) storage 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-13 10:34:18,684 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-13 10:34:18,685 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-13 10:34:18,813 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-13 10:34:18,814 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-75e73aff-3510-410a-91be-881360516f94 for DN 127.0.0.1:50010
2015-12-13 10:34:18,852 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 167 has reached the threshold 0.9990 of total blocks 168. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2015-12-13 10:34:18,852 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-13 10:34:18,853 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-75e73aff-3510-410a-91be-881360516f94,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-12-13 10:34:18,853 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 168, hasStaleStorages: false, processing time: 8 msecs
2015-12-13 10:34:18,856 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 168
2015-12-13 10:34:18,856 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-13 10:34:18,856 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 4
2015-12-13 10:34:18,856 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-13 10:34:18,856 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-13 10:34:18,856 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 4 msec
2015-12-13 10:34:38,859 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 168 has reached the threshold 0.9990 of total blocks 168. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2015-12-13 10:34:45,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:34:45,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:34:48,862 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 34 secs
2015-12-13 10:34:48,862 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2015-12-13 10:34:48,862 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2015-12-13 10:34:48,862 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 4 blocks
2015-12-13 10:34:58,061 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742326_1503 127.0.0.1:50010 
2015-12-13 10:35:00,076 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742326_1503]
2015-12-13 10:35:15,109 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:35:15,110 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:35:27,158 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-13 10:35:27,158 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-13 10:35:27,158 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 4100
2015-12-13 10:35:27,158 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 6 
2015-12-13 10:35:27,159 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 7 
2015-12-13 10:35:27,161 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000004100 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000004100-0000000000000004102
2015-12-13 10:35:27,161 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 4103
2015-12-13 10:35:28,695 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 6666.67 KB/s
2015-12-13 10:35:28,695 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000004102 size 21003 bytes.
2015-12-13 10:35:28,698 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 4099
2015-12-13 10:35:28,698 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000004015, cpktTxId=0000000000000004015)
2015-12-13 10:35:45,110 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:35:45,110 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:36:15,111 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:36:15,111 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:36:45,110 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:36:45,111 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:36:49,341 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 8 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 6 
2015-12-13 10:36:49,490 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0001/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742330_1507{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:36:49,826 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742330_1507{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-13 10:36:49,826 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2015-12-13 10:36:49,903 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742330_1507{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 3427
2015-12-13 10:36:50,238 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0001/job.jar is closed by DFSClient_NONMAPREDUCE_951933323_1
2015-12-13 10:36:50,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0001/job.jar
2015-12-13 10:36:50,337 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0001/job.split
2015-12-13 10:36:50,354 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0001/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742331_1508{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:36:50,368 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742331_1508{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:36:50,371 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0001/job.split is closed by DFSClient_NONMAPREDUCE_951933323_1
2015-12-13 10:36:50,384 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0001/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742332_1509{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:36:50,393 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742332_1509{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-13 10:36:50,393 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742332_1509{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 52
2015-12-13 10:36:50,798 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_951933323_1
2015-12-13 10:36:51,052 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0001/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742333_1510{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:36:51,068 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742333_1510{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:36:51,070 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0001/job.xml is closed by DFSClient_NONMAPREDUCE_951933323_1
2015-12-13 10:37:02,732 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0001/job_1450020873492_0001_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742334_1511{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:37:02,883 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742334_1511{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:37:02,890 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0001/job_1450020873492_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-419671786_1
2015-12-13 10:37:15,114 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30004 milliseconds
2015-12-13 10:37:15,116 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:37:20,291 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0001/job_1450020873492_0001_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742335_1512{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:37:20,706 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0001/job_1450020873492_0001_1.jhist for DFSClient_NONMAPREDUCE_-419671786_1
2015-12-13 10:37:45,122 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30007 milliseconds
2015-12-13 10:37:45,127 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 5 millisecond(s).
2015-12-13 10:38:00,933 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 46 Total time for transactions(ms): 22 Number of transactions batched in Syncs: 1 Number of syncs: 30 SyncTimes(ms): 176 
2015-12-13 10:38:01,179 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742335_1512{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13860
2015-12-13 10:38:01,203 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0001/job_1450020873492_0001_1.jhist is closed by DFSClient_NONMAPREDUCE_-419671786_1
2015-12-13 10:38:01,261 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0001.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742336_1513{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:38:01,288 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742336_1513{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:38:01,290 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0001.summary_tmp is closed by DFSClient_NONMAPREDUCE_-419671786_1
2015-12-13 10:38:01,468 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0001-1450021011359-parallels-word+count-1450021080704-0-0-FAILED-default-1450021022111.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742337_1514{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:38:01,484 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742337_1514{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:38:01,488 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0001-1450021011359-parallels-word+count-1450021080704-0-0-FAILED-default-1450021022111.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-419671786_1
2015-12-13 10:38:01,518 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0001_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742338_1515{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:38:01,532 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742338_1515{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:38:01,539 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0001_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-419671786_1
2015-12-13 10:38:02,986 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742330_1507 127.0.0.1:50010 
2015-12-13 10:38:02,989 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742331_1508 127.0.0.1:50010 
2015-12-13 10:38:02,989 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742332_1509 127.0.0.1:50010 
2015-12-13 10:38:02,989 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742333_1510 127.0.0.1:50010 
2015-12-13 10:38:02,989 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742335_1512 127.0.0.1:50010 
2015-12-13 10:38:02,990 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742334_1511 127.0.0.1:50010 
2015-12-13 10:38:03,139 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742330_1507, blk_1073742331_1508, blk_1073742332_1509, blk_1073742333_1510, blk_1073742334_1511, blk_1073742335_1512]
2015-12-13 10:38:11,568 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0002/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742339_1516{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:38:11,798 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742339_1516{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:38:11,814 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0002/job.jar is closed by DFSClient_NONMAPREDUCE_-1786991234_1
2015-12-13 10:38:11,821 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0002/job.jar
2015-12-13 10:38:11,937 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0002/job.split
2015-12-13 10:38:11,953 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0002/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742340_1517{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:38:11,960 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742340_1517{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:38:11,965 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0002/job.split is closed by DFSClient_NONMAPREDUCE_-1786991234_1
2015-12-13 10:38:11,978 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0002/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742341_1518{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:38:11,986 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742341_1518{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:38:11,989 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0002/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1786991234_1
2015-12-13 10:38:12,710 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0002/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742342_1519{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:38:12,753 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742342_1519{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:38:12,767 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0002/job.xml is closed by DFSClient_NONMAPREDUCE_-1786991234_1
2015-12-13 10:38:15,123 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 10:38:15,125 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:38:28,227 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0002/job_1450020873492_0002_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742343_1520{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:38:28,890 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742343_1520{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:38:28,913 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0002/job_1450020873492_0002_1_conf.xml is closed by DFSClient_NONMAPREDUCE_870862644_1
2015-12-13 10:38:45,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30084 milliseconds
2015-12-13 10:38:45,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 41 millisecond(s).
2015-12-13 10:38:50,204 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0002/job_1450020873492_0002_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742344_1521{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:38:50,396 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0002/job_1450020873492_0002_1.jhist for DFSClient_NONMAPREDUCE_870862644_1
2015-12-13 10:39:04,151 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 114 Total time for transactions(ms): 49 Number of transactions batched in Syncs: 1 Number of syncs: 76 SyncTimes(ms): 584 
2015-12-13 10:39:04,559 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0002_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742345_1522{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:39:04,860 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742345_1522{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:39:04,879 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0002_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0002_r_000000_0_-1827108310_1
2015-12-13 10:39:06,231 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0002/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_870862644_1
2015-12-13 10:39:06,295 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_SUCCESS is closed by DFSClient_NONMAPREDUCE_870862644_1
2015-12-13 10:39:06,303 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0002/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_870862644_1
2015-12-13 10:39:06,342 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742344_1521{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14269
2015-12-13 10:39:06,348 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0002/job_1450020873492_0002_1.jhist is closed by DFSClient_NONMAPREDUCE_870862644_1
2015-12-13 10:39:06,371 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0002.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742346_1523{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:39:06,382 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742346_1523{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:39:06,387 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0002.summary_tmp is closed by DFSClient_NONMAPREDUCE_870862644_1
2015-12-13 10:39:06,486 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0002-1450021093167-parallels-word+count-1450021146305-3-1-SUCCEEDED-default-1450021106748.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742347_1524{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:39:06,499 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742347_1524{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-13 10:39:06,502 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742347_1524{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 48429
2015-12-13 10:39:06,903 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0002-1450021093167-parallels-word+count-1450021146305-3-1-SUCCEEDED-default-1450021106748.jhist_tmp is closed by DFSClient_NONMAPREDUCE_870862644_1
2015-12-13 10:39:06,951 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0002_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742348_1525{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:39:06,981 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742348_1525{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:39:07,001 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0002_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_870862644_1
2015-12-13 10:39:08,074 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742339_1516 127.0.0.1:50010 
2015-12-13 10:39:08,075 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742340_1517 127.0.0.1:50010 
2015-12-13 10:39:08,075 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742341_1518 127.0.0.1:50010 
2015-12-13 10:39:08,075 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742342_1519 127.0.0.1:50010 
2015-12-13 10:39:08,075 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742344_1521 127.0.0.1:50010 
2015-12-13 10:39:08,075 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742343_1520 127.0.0.1:50010 
2015-12-13 10:39:09,231 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742339_1516, blk_1073742340_1517, blk_1073742341_1518, blk_1073742342_1519, blk_1073742343_1520, blk_1073742344_1521]
2015-12-13 10:39:15,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30004 milliseconds
2015-12-13 10:39:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:39:45,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 10:39:45,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:40:15,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 10:40:15,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:40:45,216 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:40:45,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:41:15,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 10:41:15,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:41:45,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:41:45,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:42:15,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:42:15,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:42:45,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:42:45,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:43:15,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:43:15,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:43:45,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:43:45,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:44:15,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:44:15,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:44:45,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:44:45,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:45:15,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:45:15,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:45:16,200 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 151 Total time for transactions(ms): 64 Number of transactions batched in Syncs: 2 Number of syncs: 105 SyncTimes(ms): 624 
2015-12-13 10:45:16,200 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742345_1522 127.0.0.1:50010 
2015-12-13 10:45:18,348 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742345_1522]
2015-12-13 10:45:21,691 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0003/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742349_1526{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:45:21,835 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742349_1526{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:45:21,840 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0003/job.jar is closed by DFSClient_NONMAPREDUCE_-1147815938_1
2015-12-13 10:45:21,846 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0003/job.jar
2015-12-13 10:45:21,928 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0003/job.split
2015-12-13 10:45:21,945 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0003/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742350_1527{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:45:21,955 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742350_1527{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:45:21,961 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0003/job.split is closed by DFSClient_NONMAPREDUCE_-1147815938_1
2015-12-13 10:45:21,972 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0003/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742351_1528{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:45:21,979 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742351_1528{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:45:21,983 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0003/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1147815938_1
2015-12-13 10:45:22,285 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0003/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742352_1529{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:45:22,312 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742352_1529{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:45:22,316 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0003/job.xml is closed by DFSClient_NONMAPREDUCE_-1147815938_1
2015-12-13 10:45:29,479 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0003/job_1450020873492_0003_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742353_1530{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:45:29,626 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742353_1530{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:45:29,631 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0003/job_1450020873492_0003_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1945949903_1
2015-12-13 10:45:44,376 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0003/job_1450020873492_0003_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742354_1531{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:45:44,732 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0003/job_1450020873492_0003_1.jhist for DFSClient_NONMAPREDUCE_1945949903_1
2015-12-13 10:45:45,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30005 milliseconds
2015-12-13 10:45:45,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:46:07,842 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0003_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742355_1532{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:46:08,035 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742355_1532{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:46:08,042 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0003_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0003_r_000000_0_1466824740_1
2015-12-13 10:46:09,349 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0003/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_1945949903_1
2015-12-13 10:46:09,393 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_SUCCESS is closed by DFSClient_NONMAPREDUCE_1945949903_1
2015-12-13 10:46:09,400 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0003/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_1945949903_1
2015-12-13 10:46:09,429 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742354_1531{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12151
2015-12-13 10:46:09,432 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0003/job_1450020873492_0003_1.jhist is closed by DFSClient_NONMAPREDUCE_1945949903_1
2015-12-13 10:46:09,446 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0003.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742356_1533{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:46:09,459 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742356_1533{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:46:09,461 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0003.summary_tmp is closed by DFSClient_NONMAPREDUCE_1945949903_1
2015-12-13 10:46:09,518 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0003-1450021522798-parallels-word+count-1450021569401-3-1-SUCCEEDED-default-1450021529037.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742357_1534{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:46:09,538 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742357_1534{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:46:09,541 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0003-1450021522798-parallels-word+count-1450021569401-3-1-SUCCEEDED-default-1450021529037.jhist_tmp is closed by DFSClient_NONMAPREDUCE_1945949903_1
2015-12-13 10:46:09,574 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0003_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742358_1535{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:46:09,595 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742358_1535{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:46:09,598 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0003_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_1945949903_1
2015-12-13 10:46:10,661 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742349_1526 127.0.0.1:50010 
2015-12-13 10:46:10,663 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742350_1527 127.0.0.1:50010 
2015-12-13 10:46:10,664 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742351_1528 127.0.0.1:50010 
2015-12-13 10:46:10,664 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742352_1529 127.0.0.1:50010 
2015-12-13 10:46:10,667 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742354_1531 127.0.0.1:50010 
2015-12-13 10:46:10,667 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742353_1530 127.0.0.1:50010 
2015-12-13 10:46:12,365 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742352_1529, blk_1073742353_1530, blk_1073742354_1531, blk_1073742349_1526, blk_1073742350_1527, blk_1073742351_1528]
2015-12-13 10:46:15,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:46:15,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:46:45,226 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:46:45,227 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:47:15,227 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 10:47:15,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:47:45,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:47:45,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:48:15,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:48:15,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:48:41,103 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 232 Total time for transactions(ms): 71 Number of transactions batched in Syncs: 2 Number of syncs: 162 SyncTimes(ms): 985 
2015-12-13 10:48:41,108 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742355_1532 127.0.0.1:50010 
2015-12-13 10:48:42,412 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742355_1532]
2015-12-13 10:48:45,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:48:45,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:48:46,074 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0004/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742359_1536{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:48:46,230 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742359_1536{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:48:46,236 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0004/job.jar is closed by DFSClient_NONMAPREDUCE_1327220972_1
2015-12-13 10:48:46,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0004/job.jar
2015-12-13 10:48:46,308 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0004/job.split
2015-12-13 10:48:46,329 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0004/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742360_1537{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:48:46,354 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742360_1537{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:48:46,356 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0004/job.split is closed by DFSClient_NONMAPREDUCE_1327220972_1
2015-12-13 10:48:46,367 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0004/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742361_1538{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:48:46,375 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742361_1538{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:48:46,377 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0004/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1327220972_1
2015-12-13 10:48:46,590 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0004/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742362_1539{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:48:46,653 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742362_1539{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:48:46,655 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0004/job.xml is closed by DFSClient_NONMAPREDUCE_1327220972_1
2015-12-13 10:48:51,663 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0004/job_1450020873492_0004_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742363_1540{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:48:51,775 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742363_1540{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:48:51,780 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0004/job_1450020873492_0004_1_conf.xml is closed by DFSClient_NONMAPREDUCE_2080986298_1
2015-12-13 10:49:07,754 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0004/job_1450020873492_0004_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742364_1541{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:49:08,093 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0004/job_1450020873492_0004_1.jhist for DFSClient_NONMAPREDUCE_2080986298_1
2015-12-13 10:49:15,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 10:49:15,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 7 millisecond(s).
2015-12-13 10:49:18,889 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0004_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742365_1542{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:49:19,151 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742365_1542{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:49:19,161 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0004_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0004_r_000000_0_-1196534749_1
2015-12-13 10:49:19,495 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0004/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_2080986298_1
2015-12-13 10:49:19,549 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_SUCCESS is closed by DFSClient_NONMAPREDUCE_2080986298_1
2015-12-13 10:49:19,561 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0004/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_2080986298_1
2015-12-13 10:49:19,594 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742364_1541{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12232
2015-12-13 10:49:19,601 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0004/job_1450020873492_0004_1.jhist is closed by DFSClient_NONMAPREDUCE_2080986298_1
2015-12-13 10:49:19,611 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0004.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742366_1543{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:49:19,620 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742366_1543{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:49:19,623 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0004.summary_tmp is closed by DFSClient_NONMAPREDUCE_2080986298_1
2015-12-13 10:49:19,686 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0004-1450021726890-parallels-word+count-1450021759570-3-1-SUCCEEDED-default-1450021731307.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742367_1544{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:49:19,695 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742367_1544{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:49:19,698 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0004-1450021726890-parallels-word+count-1450021759570-3-1-SUCCEEDED-default-1450021731307.jhist_tmp is closed by DFSClient_NONMAPREDUCE_2080986298_1
2015-12-13 10:49:19,732 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0004_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742368_1545{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:49:19,747 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742368_1545{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:49:19,750 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0004_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_2080986298_1
2015-12-13 10:49:20,800 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742359_1536 127.0.0.1:50010 
2015-12-13 10:49:20,800 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742360_1537 127.0.0.1:50010 
2015-12-13 10:49:20,800 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742361_1538 127.0.0.1:50010 
2015-12-13 10:49:20,800 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742362_1539 127.0.0.1:50010 
2015-12-13 10:49:20,800 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742364_1541 127.0.0.1:50010 
2015-12-13 10:49:20,801 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742363_1540 127.0.0.1:50010 
2015-12-13 10:49:21,870 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742359_1536, blk_1073742360_1537, blk_1073742361_1538, blk_1073742362_1539, blk_1073742363_1540, blk_1073742364_1541]
2015-12-13 10:49:45,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 10:49:45,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:50:15,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:50:15,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:50:45,235 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:50:45,235 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:51:15,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:51:15,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:51:45,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:51:45,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:52:15,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:52:15,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:52:45,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:52:45,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:53:05,132 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 313 Total time for transactions(ms): 81 Number of transactions batched in Syncs: 2 Number of syncs: 219 SyncTimes(ms): 1237 
2015-12-13 10:53:05,132 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742365_1542 127.0.0.1:50010 
2015-12-13 10:53:06,963 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742365_1542]
2015-12-13 10:53:09,606 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0005/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742369_1546{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:53:09,719 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742369_1546{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:53:09,722 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0005/job.jar is closed by DFSClient_NONMAPREDUCE_4402715_1
2015-12-13 10:53:09,729 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0005/job.jar
2015-12-13 10:53:09,807 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0005/job.split
2015-12-13 10:53:09,821 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0005/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742370_1547{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:53:09,833 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742370_1547{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:53:09,835 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0005/job.split is closed by DFSClient_NONMAPREDUCE_4402715_1
2015-12-13 10:53:09,856 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0005/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742371_1548{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:53:09,865 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742371_1548{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:53:09,871 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0005/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_4402715_1
2015-12-13 10:53:10,115 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0005/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742372_1549{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:53:10,133 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742372_1549{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:53:10,136 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0005/job.xml is closed by DFSClient_NONMAPREDUCE_4402715_1
2015-12-13 10:53:15,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:53:15,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:53:16,014 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0005/job_1450020873492_0005_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742373_1550{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:53:16,144 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742373_1550{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:53:16,149 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0005/job_1450020873492_0005_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1442857560_1
2015-12-13 10:53:30,726 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0005/job_1450020873492_0005_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742374_1551{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:53:31,286 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0005/job_1450020873492_0005_1.jhist for DFSClient_NONMAPREDUCE_-1442857560_1
2015-12-13 10:53:44,069 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0005_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742375_1552{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:53:44,329 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742375_1552{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:53:44,356 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0005_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0005_r_000000_0_618554573_1
2015-12-13 10:53:45,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30006 milliseconds
2015-12-13 10:53:45,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 30 millisecond(s).
2015-12-13 10:53:45,682 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0005/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1442857560_1
2015-12-13 10:53:45,744 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1442857560_1
2015-12-13 10:53:45,752 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0005/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1442857560_1
2015-12-13 10:53:45,783 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742374_1551{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12139
2015-12-13 10:53:45,787 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0005/job_1450020873492_0005_1.jhist is closed by DFSClient_NONMAPREDUCE_-1442857560_1
2015-12-13 10:53:45,803 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0005.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742376_1553{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:53:45,813 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742376_1553{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:53:45,820 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0005.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1442857560_1
2015-12-13 10:53:45,913 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0005-1450021990374-parallels-word+count-1450022025754-3-1-SUCCEEDED-default-1450021995679.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742377_1554{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:53:45,933 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742377_1554{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-13 10:53:45,934 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742377_1554{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 49846
2015-12-13 10:53:46,339 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0005-1450021990374-parallels-word+count-1450022025754-3-1-SUCCEEDED-default-1450021995679.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1442857560_1
2015-12-13 10:53:46,406 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0005_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742378_1555{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 10:53:46,426 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742378_1555{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 10:53:46,439 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0005_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1442857560_1
2015-12-13 10:53:47,531 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742369_1546 127.0.0.1:50010 
2015-12-13 10:53:47,531 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742370_1547 127.0.0.1:50010 
2015-12-13 10:53:47,531 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742371_1548 127.0.0.1:50010 
2015-12-13 10:53:47,532 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742372_1549 127.0.0.1:50010 
2015-12-13 10:53:47,532 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742374_1551 127.0.0.1:50010 
2015-12-13 10:53:47,532 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742373_1550 127.0.0.1:50010 
2015-12-13 10:53:48,991 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742369_1546, blk_1073742370_1547, blk_1073742371_1548, blk_1073742372_1549, blk_1073742373_1550, blk_1073742374_1551]
2015-12-13 10:54:15,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:54:15,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:54:45,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:54:45,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 10:55:15,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:55:15,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:55:45,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:55:45,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:56:15,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:56:15,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:56:45,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 10:56:45,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:57:15,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:57:15,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:57:45,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 10:57:45,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 10:58:15,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:58:15,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:58:45,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:58:45,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:59:15,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 10:59:15,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 10:59:45,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 10:59:45,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:00:15,255 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:00:15,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:00:45,257 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 11:00:45,257 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:01:15,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:01:15,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:01:45,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:01:45,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:02:15,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:02:15,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:02:45,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:02:45,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:03:15,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:03:15,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:03:45,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:03:45,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:04:15,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:04:15,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:04:45,263 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:04:45,263 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:05:15,263 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:05:15,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:05:45,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:05:45,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:06:15,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:06:15,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:06:45,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:06:45,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:07:15,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:07:15,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:07:45,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:07:45,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:08:15,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:08:15,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:08:45,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:08:45,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:09:15,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:09:15,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:09:45,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:09:45,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:10:15,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:10:15,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:10:45,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:10:45,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:11:15,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:11:15,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:11:45,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:11:45,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:12:15,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:12:15,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:12:45,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:12:45,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:13:15,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:13:15,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:13:45,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:13:45,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:14:15,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:14:15,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:14:45,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:14:45,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:15:15,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:15:15,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:15:45,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:15:45,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:16:15,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:16:15,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:16:45,275 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:16:45,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:17:15,276 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:17:15,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:17:45,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:17:45,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:18:15,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:18:15,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:18:45,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:18:45,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:19:15,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:19:15,281 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:19:45,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:19:45,281 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:20:15,281 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:20:15,281 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:20:45,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:20:45,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:21:15,281 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:21:15,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:21:45,281 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:21:45,281 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:22:15,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:22:15,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:22:45,281 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:22:45,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:23:15,281 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:23:15,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:23:45,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:23:45,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:24:15,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:24:15,284 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:24:45,284 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:24:45,284 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:25:15,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:25:15,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:25:45,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:25:45,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:26:15,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:26:15,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:26:45,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:26:45,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:27:15,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:27:15,288 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:27:45,288 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:27:45,289 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:28:15,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 11:28:15,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:28:45,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:28:45,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:29:15,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:29:15,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:29:45,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:29:45,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:30:15,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:30:15,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:30:45,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:30:45,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:31:15,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:31:15,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:31:45,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:31:45,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:32:15,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:32:15,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:32:45,296 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:32:45,296 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:33:15,296 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:33:15,296 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:33:45,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:33:45,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:34:15,296 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:34:15,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:34:45,296 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:34:45,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:35:15,296 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:35:15,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 11:35:31,130 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-13 11:35:31,130 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-13 11:35:31,131 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 4103
2015-12-13 11:35:31,131 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 394 Total time for transactions(ms): 104 Number of transactions batched in Syncs: 3 Number of syncs: 276 SyncTimes(ms): 1522 
2015-12-13 11:35:31,133 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 394 Total time for transactions(ms): 104 Number of transactions batched in Syncs: 3 Number of syncs: 277 SyncTimes(ms): 1524 
2015-12-13 11:35:31,134 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000004103 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000004103-0000000000000004496
2015-12-13 11:35:31,136 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 4497
2015-12-13 11:35:31,738 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 7333.33 KB/s
2015-12-13 11:35:31,739 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000004496 size 22978 bytes.
2015-12-13 11:35:31,751 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 4102
2015-12-13 11:35:31,751 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000004099, cpktTxId=0000000000000004099)
2015-12-13 11:35:45,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:35:45,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:36:15,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:36:15,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:36:45,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:36:45,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:37:15,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:37:15,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 11:37:45,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:37:45,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:38:15,300 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:38:15,300 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:38:45,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:38:45,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:39:15,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:39:15,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:39:45,302 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:39:45,302 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:40:15,302 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:40:15,302 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:40:45,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:40:45,302 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:41:15,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:41:15,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:41:45,302 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:41:45,303 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:42:15,303 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:42:15,304 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:42:45,304 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:42:45,305 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:43:15,305 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:43:15,306 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:43:45,306 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:43:45,306 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:44:15,307 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:44:15,308 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:44:45,308 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:44:45,308 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:45:15,309 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:45:15,310 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:45:45,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:45:45,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:46:15,310 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:46:15,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:46:45,310 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:46:45,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:47:15,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:47:15,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:47:45,310 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:47:45,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:48:15,310 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:48:15,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:48:45,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:48:45,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:49:15,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:49:15,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:49:45,313 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:49:45,313 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:50:15,314 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:50:15,315 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:50:45,316 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:50:45,316 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:51:15,316 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:51:15,316 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:51:45,316 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:51:45,317 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:52:15,317 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:52:15,318 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:52:45,319 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 11:52:45,320 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:53:15,320 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:53:15,320 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:53:45,321 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:53:45,321 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:54:15,321 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:54:15,322 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:54:45,322 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:54:45,322 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:55:15,324 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 11:55:15,324 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:55:45,325 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:55:45,326 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 11:56:15,325 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:56:15,326 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:56:45,326 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 11:56:45,327 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:57:15,328 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 11:57:15,328 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 11:57:45,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:57:45,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:58:15,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:58:15,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:58:45,328 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:58:45,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:59:15,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:59:15,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 11:59:45,328 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 11:59:45,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:00:15,328 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:00:15,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:00:45,328 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:00:45,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:01:15,329 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:01:15,330 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:01:16,571 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 183, hasStaleStorages: false, processing time: 35 msecs
2015-12-13 12:01:45,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:01:45,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:02:15,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:02:15,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:02:45,332 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:02:45,332 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:03:15,332 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:03:15,332 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:03:45,332 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:03:45,332 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:04:15,332 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:04:15,333 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:04:45,332 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:04:45,333 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:05:15,333 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:05:15,333 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:05:45,335 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:05:45,335 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:06:15,337 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 12:06:15,337 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:06:45,336 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:06:45,337 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:07:15,338 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:07:15,338 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:07:45,337 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:07:45,338 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:08:15,338 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:08:15,339 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:08:45,339 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:08:45,339 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:09:15,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:09:15,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:09:45,340 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:09:45,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:10:15,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:10:15,342 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:10:45,342 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:10:45,342 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:11:15,343 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:11:15,344 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:11:45,343 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:11:45,344 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:12:15,345 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:12:15,345 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:12:45,345 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:12:45,345 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:13:15,345 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:13:15,346 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:13:45,345 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:13:45,346 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:14:15,346 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:14:15,347 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:14:45,347 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:14:45,348 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:15:15,348 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:15:15,349 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:15:45,348 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:15:45,349 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:16:15,349 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:16:15,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:16:45,349 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:16:45,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:17:15,349 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:17:15,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:17:45,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:17:45,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:18:15,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:18:15,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:18:45,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:18:45,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:19:15,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:19:15,353 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:19:45,353 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:19:45,354 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:20:15,354 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:20:15,355 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:20:45,355 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:20:45,356 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:21:15,356 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:21:15,356 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:21:45,356 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:21:45,356 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:22:15,357 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:22:15,357 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:22:45,357 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:22:45,358 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:23:15,360 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-13 12:23:15,360 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:52:03,930 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:52:03,931 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:52:33,931 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:52:33,931 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:53:03,931 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:53:03,932 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:53:33,933 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 12:53:33,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:54:03,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:54:03,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:54:33,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:54:33,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:55:03,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:55:03,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:55:33,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:55:33,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:56:03,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:56:03,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:56:33,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:56:33,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:57:03,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:57:03,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:57:33,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:57:33,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:58:03,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:58:03,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:58:33,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:58:33,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 12:59:03,937 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 12:59:03,937 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 12:59:33,937 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 12:59:33,937 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:00:03,938 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 13:00:03,939 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:00:33,939 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:00:33,940 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:00:44,243 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 14 
2015-12-13 13:00:44,249 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742375_1552 127.0.0.1:50010 
2015-12-13 13:00:45,160 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742375_1552]
2015-12-13 13:00:49,500 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0006/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742379_1556{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:00:49,662 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742379_1556{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:00:49,666 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0006/job.jar is closed by DFSClient_NONMAPREDUCE_-1427144070_1
2015-12-13 13:00:49,671 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0006/job.jar
2015-12-13 13:00:49,760 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0006/job.split
2015-12-13 13:00:49,778 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0006/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742380_1557{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:00:49,786 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742380_1557{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:00:49,791 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0006/job.split is closed by DFSClient_NONMAPREDUCE_-1427144070_1
2015-12-13 13:00:49,805 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0006/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742381_1558{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:00:49,812 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742381_1558{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:00:49,816 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0006/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1427144070_1
2015-12-13 13:00:50,088 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0006/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742382_1559{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:00:50,107 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742382_1559{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:00:50,110 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0006/job.xml is closed by DFSClient_NONMAPREDUCE_-1427144070_1
2015-12-13 13:00:59,207 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0006/job_1450020873492_0006_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742383_1560{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:00:59,356 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742383_1560{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:00:59,370 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0006/job_1450020873492_0006_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1867177369_1
2015-12-13 13:01:03,974 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30032 milliseconds
2015-12-13 13:01:04,100 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 128 millisecond(s).
2015-12-13 13:01:14,129 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0006/job_1450020873492_0006_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742384_1561{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:01:14,401 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0006/job_1450020873492_0006_1.jhist for DFSClient_NONMAPREDUCE_1867177369_1
2015-12-13 13:01:24,836 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0006_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742385_1562{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:01:25,045 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742385_1562{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:01:25,049 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0006_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0006_r_000000_0_292627908_1
2015-12-13 13:01:26,365 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0006/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_1867177369_1
2015-12-13 13:01:26,406 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_SUCCESS is closed by DFSClient_NONMAPREDUCE_1867177369_1
2015-12-13 13:01:26,414 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0006/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_1867177369_1
2015-12-13 13:01:26,449 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742384_1561{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14258
2015-12-13 13:01:26,452 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0006/job_1450020873492_0006_1.jhist is closed by DFSClient_NONMAPREDUCE_1867177369_1
2015-12-13 13:01:26,463 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0006.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742386_1563{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:01:26,470 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742386_1563{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:01:26,474 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0006.summary_tmp is closed by DFSClient_NONMAPREDUCE_1867177369_1
2015-12-13 13:01:26,541 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0006-1450029650425-parallels-word+count-1450029686416-3-1-SUCCEEDED-default-1450029658676.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742387_1564{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:01:26,557 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742387_1564{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:01:26,561 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0006-1450029650425-parallels-word+count-1450029686416-3-1-SUCCEEDED-default-1450029658676.jhist_tmp is closed by DFSClient_NONMAPREDUCE_1867177369_1
2015-12-13 13:01:26,595 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0006_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742388_1565{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:01:26,606 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742388_1565{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:01:26,609 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0006_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_1867177369_1
2015-12-13 13:01:27,695 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742379_1556 127.0.0.1:50010 
2015-12-13 13:01:27,696 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742380_1557 127.0.0.1:50010 
2015-12-13 13:01:27,696 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742381_1558 127.0.0.1:50010 
2015-12-13 13:01:27,696 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742382_1559 127.0.0.1:50010 
2015-12-13 13:01:27,697 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742384_1561 127.0.0.1:50010 
2015-12-13 13:01:27,697 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742383_1560 127.0.0.1:50010 
2015-12-13 13:01:30,174 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742384_1561, blk_1073742379_1556, blk_1073742380_1557, blk_1073742381_1558, blk_1073742382_1559, blk_1073742383_1560]
2015-12-13 13:01:33,972 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:01:33,973 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:02:03,974 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 13:02:03,975 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:02:33,976 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:02:33,977 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:03:03,977 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:03:03,977 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:03:33,977 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:03:33,978 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:03:51,210 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-13 13:03:51,211 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-13 13:03:51,211 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 4497
2015-12-13 13:03:51,212 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 86 Total time for transactions(ms): 27 Number of transactions batched in Syncs: 0 Number of syncs: 59 SyncTimes(ms): 276 
2015-12-13 13:03:51,216 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 86 Total time for transactions(ms): 27 Number of transactions batched in Syncs: 0 Number of syncs: 60 SyncTimes(ms): 280 
2015-12-13 13:03:51,221 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000004497 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000004497-0000000000000004582
2015-12-13 13:03:51,225 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 4583
2015-12-13 13:03:51,669 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 4400.00 KB/s
2015-12-13 13:03:51,669 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000004582 size 23335 bytes.
2015-12-13 13:03:51,680 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 4496
2015-12-13 13:03:51,680 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000004102, cpktTxId=0000000000000004102)
2015-12-13 13:04:03,979 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:04:03,979 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:04:33,979 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:04:33,981 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:05:03,983 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-13 13:05:03,983 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:05:33,983 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:05:33,984 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:06:03,985 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:06:03,985 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:06:33,986 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:06:33,986 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:07:03,986 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:07:03,988 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 13:07:33,987 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:07:33,987 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:08:03,987 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:08:03,987 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:08:33,989 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 13:08:34,009 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 20 millisecond(s).
2015-12-13 13:09:03,990 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:09:03,991 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:09:33,991 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:09:33,991 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:10:03,991 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:10:03,991 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:10:33,991 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:10:33,992 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:11:03,992 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:11:03,993 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:11:33,994 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:11:33,994 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:12:03,994 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:12:03,995 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:12:33,995 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:12:33,996 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:13:03,996 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:13:03,996 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:13:33,995 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:13:33,996 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:14:03,997 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:14:03,998 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:14:33,997 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:14:33,997 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:15:03,997 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:15:03,998 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:15:33,998 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:15:33,999 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:16:04,000 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:16:04,000 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:16:34,000 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:16:34,001 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:17:04,000 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:17:04,000 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:17:34,000 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:17:34,001 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:18:04,000 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:18:04,001 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:18:34,001 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:18:34,002 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:19:04,002 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:19:04,002 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:19:34,003 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:19:34,003 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:20:04,002 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:20:04,003 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:20:34,003 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:20:34,004 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:21:04,005 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:21:04,006 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:21:34,005 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:21:34,007 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:22:04,007 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 13:22:04,010 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-13 13:22:34,008 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:22:34,008 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:23:04,008 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:23:04,009 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:23:34,009 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:23:34,009 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:24:04,009 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:24:04,010 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:24:34,010 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:24:34,011 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:25:04,011 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:25:04,012 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:25:34,012 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:25:34,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:26:04,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:26:04,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:26:34,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:26:34,014 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 13:27:04,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:27:04,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:27:34,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:27:34,014 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:28:04,014 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:28:04,015 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:28:34,015 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:28:34,016 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:29:04,016 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:29:04,017 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:29:34,017 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:29:34,018 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:30:04,018 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:30:04,019 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:30:34,019 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:30:34,020 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:31:02,143 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 16 
2015-12-13 13:31:02,144 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742385_1562 127.0.0.1:50010 
2015-12-13 13:31:03,687 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742385_1562]
2015-12-13 13:31:04,020 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:31:04,021 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:31:06,973 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0007/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742389_1566{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:31:07,121 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742389_1566{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:31:07,125 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0007/job.jar is closed by DFSClient_NONMAPREDUCE_258690400_1
2015-12-13 13:31:07,130 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0007/job.jar
2015-12-13 13:31:07,207 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0007/job.split
2015-12-13 13:31:07,221 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0007/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742390_1567{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:31:07,232 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742390_1567{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:31:07,235 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0007/job.split is closed by DFSClient_NONMAPREDUCE_258690400_1
2015-12-13 13:31:07,250 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0007/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742391_1568{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:31:07,257 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742391_1568{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:31:07,260 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0007/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_258690400_1
2015-12-13 13:31:07,522 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0007/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742392_1569{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:31:07,536 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742392_1569{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:31:07,538 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0007/job.xml is closed by DFSClient_NONMAPREDUCE_258690400_1
2015-12-13 13:31:32,932 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0008/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742393_1570{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:31:33,078 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742393_1570{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:31:33,082 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0008/job.jar is closed by DFSClient_NONMAPREDUCE_-532726123_1
2015-12-13 13:31:34,020 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:31:34,021 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:31:59,768 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0009/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742394_1571{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:31:59,891 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742394_1571{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:31:59,898 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0009/job.jar is closed by DFSClient_NONMAPREDUCE_-2103325452_1
2015-12-13 13:31:59,904 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0009/job.jar
2015-12-13 13:31:59,962 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0009/job.split
2015-12-13 13:31:59,978 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0009/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742395_1572{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:31:59,985 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742395_1572{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:31:59,992 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0009/job.split is closed by DFSClient_NONMAPREDUCE_-2103325452_1
2015-12-13 13:32:00,004 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0009/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742396_1573{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:32:00,016 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742396_1573{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:32:00,019 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0009/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-2103325452_1
2015-12-13 13:32:00,210 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0009/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742397_1574{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:32:00,231 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742397_1574{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:32:00,233 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0009/job.xml is closed by DFSClient_NONMAPREDUCE_-2103325452_1
2015-12-13 13:32:04,021 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:32:04,021 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:32:05,912 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 68 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 0 Number of syncs: 48 SyncTimes(ms): 59 
2015-12-13 13:32:06,331 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0009/job_1450020873492_0009_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742398_1575{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:32:06,467 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742398_1575{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:32:06,474 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0009/job_1450020873492_0009_1_conf.xml is closed by DFSClient_NONMAPREDUCE_573764308_1
2015-12-13 13:32:19,437 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0009/job_1450020873492_0009_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742399_1576{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:32:19,546 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0009/job_1450020873492_0009_1.jhist for DFSClient_NONMAPREDUCE_573764308_1
2015-12-13 13:32:34,024 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 13:32:34,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 11 millisecond(s).
2015-12-13 13:32:52,505 WARN org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:41266 Call#3 Retry#0: output error
2015-12-13 13:32:52,626 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742399_1576{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13455
2015-12-13 13:32:52,731 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0009/job_1450020873492_0009_1.jhist is closed by DFSClient_NONMAPREDUCE_573764308_1
2015-12-13 13:32:52,937 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0009.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742400_1577{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:32:52,988 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742400_1577{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:32:53,003 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0009.summary_tmp is closed by DFSClient_NONMAPREDUCE_573764308_1
2015-12-13 13:32:53,014 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2573)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:135)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:977)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1042)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2094)
2015-12-13 13:32:53,166 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0009-1450031520458-parallels-task+C-1450031571940-0-0-FAILED-default-1450031525887.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742401_1578{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:32:53,192 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742401_1578{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:32:53,209 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0009-1450031520458-parallels-task+C-1450031571940-0-0-FAILED-default-1450031525887.jhist_tmp is closed by DFSClient_NONMAPREDUCE_573764308_1
2015-12-13 13:32:53,241 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0009_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742402_1579{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:32:53,257 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742402_1579{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:32:53,260 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0009_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_573764308_1
2015-12-13 13:32:54,384 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742394_1571 127.0.0.1:50010 
2015-12-13 13:32:54,384 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742395_1572 127.0.0.1:50010 
2015-12-13 13:32:54,385 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742396_1573 127.0.0.1:50010 
2015-12-13 13:32:54,385 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742397_1574 127.0.0.1:50010 
2015-12-13 13:32:54,385 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742399_1576 127.0.0.1:50010 
2015-12-13 13:32:54,385 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742398_1575 127.0.0.1:50010 
2015-12-13 13:32:54,708 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742394_1571, blk_1073742395_1572, blk_1073742396_1573, blk_1073742397_1574, blk_1073742398_1575, blk_1073742399_1576]
2015-12-13 13:33:02,191 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0010/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742403_1580{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:33:02,321 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742403_1580{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:33:02,333 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0010/job.jar is closed by DFSClient_NONMAPREDUCE_-357452811_1
2015-12-13 13:33:02,338 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0010/job.jar
2015-12-13 13:33:02,418 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0010/job.split
2015-12-13 13:33:02,429 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0010/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742404_1581{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:33:02,445 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742404_1581{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:33:02,449 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0010/job.split is closed by DFSClient_NONMAPREDUCE_-357452811_1
2015-12-13 13:33:02,466 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0010/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742405_1582{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:33:02,475 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742405_1582{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:33:02,478 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0010/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-357452811_1
2015-12-13 13:33:02,743 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0010/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742406_1583{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:33:02,774 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742406_1583{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:33:02,782 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0010/job.xml is closed by DFSClient_NONMAPREDUCE_-357452811_1
2015-12-13 13:33:04,024 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:33:04,025 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 13:33:08,306 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 132 Total time for transactions(ms): 32 Number of transactions batched in Syncs: 0 Number of syncs: 94 SyncTimes(ms): 288 
2015-12-13 13:33:08,692 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0010/job_1450020873492_0010_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742407_1584{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:33:08,809 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742407_1584{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:33:08,813 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0010/job_1450020873492_0010_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-120064290_1
2015-12-13 13:33:21,133 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1538ms
No GCs detected
2015-12-13 13:33:23,655 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0010/job_1450020873492_0010_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742408_1585{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:33:23,842 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0010/job_1450020873492_0010_1.jhist for DFSClient_NONMAPREDUCE_-120064290_1
2015-12-13 13:33:33,266 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0010_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742409_1586{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:33:33,556 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742409_1586{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:33:33,566 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0010_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0010_r_000000_0_1309357671_1
2015-12-13 13:33:33,768 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0010/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-120064290_1
2015-12-13 13:33:33,813 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-120064290_1
2015-12-13 13:33:33,818 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0010/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-120064290_1
2015-12-13 13:33:33,846 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742408_1585{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12228
2015-12-13 13:33:33,850 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0010/job_1450020873492_0010_1.jhist is closed by DFSClient_NONMAPREDUCE_-120064290_1
2015-12-13 13:33:33,861 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0010.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742410_1587{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:33:33,869 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742410_1587{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:33:33,871 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0010.summary_tmp is closed by DFSClient_NONMAPREDUCE_-120064290_1
2015-12-13 13:33:33,914 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0010-1450031583023-parallels-task+C-1450031613821-3-1-SUCCEEDED-default-1450031588290.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742411_1588{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:33:33,921 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742411_1588{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:33:33,924 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0010-1450031583023-parallels-task+C-1450031613821-3-1-SUCCEEDED-default-1450031588290.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-120064290_1
2015-12-13 13:33:33,951 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0010_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742412_1589{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 13:33:33,964 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742412_1589{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 13:33:33,967 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0010_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-120064290_1
2015-12-13 13:33:34,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30005 milliseconds
2015-12-13 13:33:34,062 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 33 millisecond(s).
2015-12-13 13:33:35,011 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742403_1580 127.0.0.1:50010 
2015-12-13 13:33:35,012 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742404_1581 127.0.0.1:50010 
2015-12-13 13:33:35,012 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742405_1582 127.0.0.1:50010 
2015-12-13 13:33:35,012 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742406_1583 127.0.0.1:50010 
2015-12-13 13:33:35,012 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742408_1585 127.0.0.1:50010 
2015-12-13 13:33:35,012 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742407_1584 127.0.0.1:50010 
2015-12-13 13:33:36,729 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742403_1580, blk_1073742404_1581, blk_1073742405_1582, blk_1073742406_1583, blk_1073742407_1584, blk_1073742408_1585]
2015-12-13 13:34:04,030 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 13:34:04,031 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:34:34,032 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 13:34:34,033 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:35:04,033 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:35:04,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:35:34,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:35:34,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:36:04,036 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 13:36:04,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:36:34,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:36:34,038 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:37:04,039 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:37:04,039 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:37:34,039 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:37:34,039 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:38:04,039 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:38:04,040 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:38:34,040 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:38:34,041 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:39:04,042 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:39:04,043 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:39:34,042 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:39:34,043 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:40:04,043 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:40:04,043 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:40:34,044 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:40:34,045 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:41:04,044 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:41:04,045 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:41:34,044 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:41:34,045 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:42:04,044 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:42:04,045 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:42:34,045 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:42:34,046 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:43:04,045 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:43:04,046 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:43:34,045 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:43:34,046 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:44:04,045 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:44:04,046 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:44:34,046 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:44:34,047 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:45:04,047 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:45:04,047 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:45:34,047 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:45:34,047 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:46:04,047 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:46:04,047 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:46:34,047 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:46:34,048 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:47:04,049 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 13:47:04,050 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:47:34,051 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:47:34,052 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:48:04,051 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:48:04,051 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:48:34,051 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:48:34,051 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:49:04,052 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:49:04,054 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:49:34,053 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 13:49:34,055 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:50:04,056 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 13:50:04,056 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:50:34,056 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:50:34,057 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:51:04,057 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:51:04,058 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:51:34,058 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:51:34,059 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:52:04,060 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:52:04,062 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 13:52:34,061 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:52:34,061 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:53:04,062 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 13:53:04,063 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:53:34,063 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:53:34,064 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:54:04,063 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:54:04,064 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:54:34,063 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:54:34,063 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:55:04,063 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:55:04,063 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:55:34,063 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:55:34,064 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:56:04,064 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:56:04,065 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:56:34,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:56:34,066 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:57:04,067 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:57:04,067 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:57:34,067 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:57:34,068 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 13:58:04,069 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:58:04,070 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:58:34,069 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:58:34,069 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:59:04,070 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 13:59:04,070 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 13:59:34,070 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 13:59:34,071 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:00:04,071 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:00:04,071 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:00:34,071 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:00:34,072 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:01:04,072 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:01:04,073 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:01:34,073 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:01:34,075 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 14:02:04,073 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:02:04,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:02:34,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:02:34,075 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:03:04,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:03:04,075 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:03:34,074 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:03:34,075 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:03:53,515 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-13 14:03:53,515 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-13 14:03:53,515 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 4583
2015-12-13 14:03:53,516 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 184 Total time for transactions(ms): 54 Number of transactions batched in Syncs: 0 Number of syncs: 130 SyncTimes(ms): 415 
2015-12-13 14:03:53,521 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 184 Total time for transactions(ms): 54 Number of transactions batched in Syncs: 0 Number of syncs: 131 SyncTimes(ms): 420 
2015-12-13 14:03:53,521 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000004583 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000004583-0000000000000004766
2015-12-13 14:03:53,522 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 4767
2015-12-13 14:03:53,736 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 11500.00 KB/s
2015-12-13 14:03:53,736 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000004766 size 24547 bytes.
2015-12-13 14:03:53,742 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 4582
2015-12-13 14:03:53,742 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000004496, cpktTxId=0000000000000004496)
2015-12-13 14:04:04,075 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:04:04,076 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:04:34,075 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:04:34,075 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:05:04,075 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:05:04,076 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:05:34,077 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:05:34,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:06:04,077 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:06:04,077 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:06:34,077 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:06:34,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:07:04,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:07:04,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:07:34,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:07:34,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 14:08:04,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:08:04,079 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:08:34,079 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:08:34,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:09:04,079 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:09:04,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:09:34,079 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:09:34,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:10:04,081 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:10:04,081 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:10:34,081 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:10:34,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:11:04,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:11:04,083 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:11:34,083 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:11:34,084 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:12:04,084 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:12:04,085 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:12:34,085 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:12:34,086 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:13:04,086 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:13:04,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:13:34,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:13:34,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:14:04,087 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:14:04,088 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:14:34,088 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:14:34,089 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:15:04,089 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:15:04,090 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:15:34,090 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:15:34,091 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:16:04,090 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:16:04,091 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:16:34,091 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:16:34,092 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:17:04,092 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:17:04,092 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:17:34,092 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:17:34,093 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:18:04,092 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:18:04,093 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:18:34,094 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:18:34,094 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:19:04,094 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:19:04,095 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:19:34,096 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:19:34,096 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:20:04,096 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:20:04,097 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:20:34,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:20:34,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:21:04,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:21:04,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:21:34,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:21:34,099 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:22:04,100 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:22:04,100 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:22:34,100 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:22:34,101 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:23:04,101 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:23:04,102 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:23:34,103 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:23:34,103 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:24:04,104 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:24:04,104 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:24:34,105 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:24:34,105 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:25:04,105 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:25:04,106 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:25:34,106 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:25:34,106 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:26:04,106 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:26:04,106 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:26:34,106 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:26:34,106 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:27:04,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:27:04,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:27:34,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:27:34,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:28:04,107 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:28:04,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:28:34,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:28:34,108 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:29:31,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:29:31,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:30:01,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:30:01,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 14:30:31,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:30:31,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:31:01,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:31:01,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:31:31,235 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:31:31,235 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:32:01,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:32:01,235 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:32:31,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:32:31,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:33:01,235 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:33:01,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:33:31,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:33:31,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:34:01,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:34:01,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:34:31,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:34:31,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:35:01,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:35:01,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:35:31,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:35:31,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:36:01,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:36:01,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:36:31,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:36:31,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:37:01,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:37:01,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:37:31,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:37:31,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:38:01,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:38:01,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:38:31,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:38:31,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:39:01,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:39:01,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:39:31,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:39:31,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:40:01,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:40:01,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-13 14:40:31,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:40:31,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:41:01,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:41:01,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:41:31,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:41:31,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:42:01,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:42:01,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:42:31,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:42:31,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:43:01,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:43:01,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:43:31,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:43:31,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:44:01,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:44:01,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:44:31,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:44:31,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:45:01,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:45:01,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:45:31,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:45:31,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:46:01,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:46:01,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:46:31,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:46:31,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:47:01,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:47:01,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:47:31,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:47:31,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:48:01,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:48:01,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:48:31,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:48:31,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:49:01,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:49:01,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:49:31,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:49:31,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:50:01,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:50:01,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:50:31,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:50:31,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:51:01,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:51:01,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:51:31,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:51:31,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:52:01,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:52:01,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 14:52:31,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:52:31,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:53:01,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:53:01,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:53:31,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:53:31,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:54:01,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:54:01,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:54:31,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:54:31,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:55:01,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:55:01,255 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:55:31,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:55:31,255 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:56:01,255 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:56:01,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:56:31,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:56:31,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:57:01,255 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:57:01,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 14:57:27,489 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 12 
2015-12-13 14:57:27,495 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742409_1586 127.0.0.1:50010 
2015-12-13 14:57:29,138 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742409_1586]
2015-12-13 14:57:31,255 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:57:31,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:57:32,161 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0011/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742413_1590{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 14:57:32,308 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742413_1590{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 14:57:32,312 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0011/job.jar is closed by DFSClient_NONMAPREDUCE_565831135_1
2015-12-13 14:57:32,318 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0011/job.jar
2015-12-13 14:57:32,400 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0011/job.split
2015-12-13 14:57:32,412 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0011/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742414_1591{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 14:57:32,424 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742414_1591{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 14:57:32,426 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0011/job.split is closed by DFSClient_NONMAPREDUCE_565831135_1
2015-12-13 14:57:32,441 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0011/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742415_1592{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 14:57:32,451 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742415_1592{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 14:57:32,453 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0011/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_565831135_1
2015-12-13 14:57:32,715 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0011/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742416_1593{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 14:57:32,729 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742416_1593{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 14:57:32,731 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0011/job.xml is closed by DFSClient_NONMAPREDUCE_565831135_1
2015-12-13 14:57:39,056 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0011/job_1450020873492_0011_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742417_1594{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 14:57:39,161 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742417_1594{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 14:57:39,168 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0011/job_1450020873492_0011_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1356742496_1
2015-12-13 14:57:57,676 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0011/job_1450020873492_0011_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742418_1595{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 14:57:57,905 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0011/job_1450020873492_0011_1.jhist for DFSClient_NONMAPREDUCE_1356742496_1
2015-12-13 14:58:01,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 14:58:01,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 26 millisecond(s).
2015-12-13 14:58:04,614 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0011_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742419_1596{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 14:58:04,786 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742419_1596{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 14:58:04,796 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0011_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0011_r_000000_0_1195389825_1
2015-12-13 14:58:06,056 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0011/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_1356742496_1
2015-12-13 14:58:06,109 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_1356742496_1
2015-12-13 14:58:06,118 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0011/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_1356742496_1
2015-12-13 14:58:06,145 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742418_1595{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14353
2015-12-13 14:58:06,148 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0011/job_1450020873492_0011_1.jhist is closed by DFSClient_NONMAPREDUCE_1356742496_1
2015-12-13 14:58:06,158 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0011.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742420_1597{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 14:58:06,170 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742420_1597{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 14:58:06,173 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0011.summary_tmp is closed by DFSClient_NONMAPREDUCE_1356742496_1
2015-12-13 14:58:06,224 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0011-1450036653006-parallels-task+C1-1450036686120-3-1-SUCCEEDED-default-1450036658644.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742421_1598{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 14:58:06,238 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742421_1598{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 14:58:06,240 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0011-1450036653006-parallels-task+C1-1450036686120-3-1-SUCCEEDED-default-1450036658644.jhist_tmp is closed by DFSClient_NONMAPREDUCE_1356742496_1
2015-12-13 14:58:06,262 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0011_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742422_1599{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 14:58:06,331 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742422_1599{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 14:58:06,333 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0011_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_1356742496_1
2015-12-13 14:58:07,403 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742413_1590 127.0.0.1:50010 
2015-12-13 14:58:07,404 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742414_1591 127.0.0.1:50010 
2015-12-13 14:58:07,405 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742415_1592 127.0.0.1:50010 
2015-12-13 14:58:07,405 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742416_1593 127.0.0.1:50010 
2015-12-13 14:58:07,405 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742418_1595 127.0.0.1:50010 
2015-12-13 14:58:07,405 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742417_1594 127.0.0.1:50010 
2015-12-13 14:58:08,173 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742416_1593, blk_1073742417_1594, blk_1073742418_1595, blk_1073742413_1590, blk_1073742414_1591, blk_1073742415_1592]
2015-12-13 14:58:31,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:58:31,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:59:01,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 14:59:01,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 14:59:31,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 14:59:31,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:00:01,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:00:01,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:00:31,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:00:31,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:01:01,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:01:01,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:01:31,261 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:01:31,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:02:01,262 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:02:01,263 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:02:31,263 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:02:31,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:03:01,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:03:01,264 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:03:31,265 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:03:31,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:04:01,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:04:01,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:04:21,640 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-13 15:04:21,640 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-13 15:04:21,640 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 4767
2015-12-13 15:04:21,640 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 86 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 0 Number of syncs: 59 SyncTimes(ms): 125 
2015-12-13 15:04:21,641 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 86 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 0 Number of syncs: 60 SyncTimes(ms): 125 
2015-12-13 15:04:21,644 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000004767 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000004767-0000000000000004852
2015-12-13 15:04:21,645 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 4853
2015-12-13 15:04:22,059 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 2666.67 KB/s
2015-12-13 15:04:22,059 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000004852 size 24912 bytes.
2015-12-13 15:04:22,070 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 4766
2015-12-13 15:04:22,071 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000004582, cpktTxId=0000000000000004582)
2015-12-13 15:04:31,266 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:04:31,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:05:01,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:05:01,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:05:31,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:05:31,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:06:01,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:06:01,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:06:31,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:06:31,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:07:01,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:07:01,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:07:31,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 15:07:31,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:08:01,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:08:01,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:08:31,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:08:31,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:09:01,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:09:01,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:09:31,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:09:31,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:10:01,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:10:01,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:10:31,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:10:31,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:11:01,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:11:01,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:11:31,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:11:31,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:12:01,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-13 15:12:01,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:12:31,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:12:31,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:13:01,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:13:01,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:13:31,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:13:31,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:14:01,277 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:14:01,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:14:31,278 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:14:31,279 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:15:01,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:15:01,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:15:31,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:15:31,281 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:16:01,281 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:16:01,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:16:31,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:16:31,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:17:01,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:17:01,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:17:31,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:17:31,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:18:01,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:18:01,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:18:31,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:18:31,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:19:01,282 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:19:01,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:19:31,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:19:31,284 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:20:01,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:20:01,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:20:31,284 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:20:31,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:21:01,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:21:01,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:21:31,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:21:31,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:22:01,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:22:01,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:22:31,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:22:31,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:23:01,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:23:01,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:23:31,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:23:31,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:24:01,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:24:01,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:24:31,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:24:31,288 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:25:01,288 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:25:01,288 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:25:31,288 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:25:31,289 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:26:01,289 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:26:01,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:26:31,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:26:31,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:27:01,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:27:01,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:27:31,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:27:31,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:28:01,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:28:01,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:28:31,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:28:31,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:29:01,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:29:01,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:29:31,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:29:31,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:30:01,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:30:01,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:30:31,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:30:31,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:31:01,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:31:01,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:31:31,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 15:31:31,296 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 15:32:01,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 15:32:01,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 15:32:31,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 15:32:31,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:11:34,144 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:11:34,144 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:12:04,144 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:12:04,144 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:12:31,749 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-13 16:12:34,144 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:12:34,145 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:12:50,606 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 7 
2015-12-13 16:12:50,615 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742419_1596 127.0.0.1:50010 
2015-12-13 16:12:53,576 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742419_1596]
2015-12-13 16:13:02,617 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0012/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742423_1600{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:13:02,759 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742423_1600{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:13:02,763 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0012/job.jar is closed by DFSClient_NONMAPREDUCE_325369101_1
2015-12-13 16:13:02,769 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0012/job.jar
2015-12-13 16:13:02,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0012/job.split
2015-12-13 16:13:02,863 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0012/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742424_1601{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:13:02,870 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742424_1601{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:13:02,872 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0012/job.split is closed by DFSClient_NONMAPREDUCE_325369101_1
2015-12-13 16:13:02,883 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0012/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742425_1602{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:13:02,889 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742425_1602{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:13:02,892 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0012/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_325369101_1
2015-12-13 16:13:03,155 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0012/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742426_1603{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:13:03,181 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742426_1603{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:13:03,184 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0012/job.xml is closed by DFSClient_NONMAPREDUCE_325369101_1
2015-12-13 16:13:04,144 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:13:04,145 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:13:10,408 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0012/job_1450020873492_0012_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742427_1604{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:13:10,621 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742427_1604{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:13:10,626 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0012/job_1450020873492_0012_1_conf.xml is closed by DFSClient_NONMAPREDUCE_98842116_1
2015-12-13 16:13:28,113 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0012/job_1450020873492_0012_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742428_1605{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:13:28,303 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0012/job_1450020873492_0012_1.jhist for DFSClient_NONMAPREDUCE_98842116_1
2015-12-13 16:13:34,153 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30009 milliseconds
2015-12-13 16:13:34,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 31 millisecond(s).
2015-12-13 16:13:37,886 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0012_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742429_1606{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:13:38,046 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742429_1606{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:13:38,054 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0012_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0012_r_000000_0_32935543_1
2015-12-13 16:13:39,357 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0012/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_98842116_1
2015-12-13 16:13:39,418 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_98842116_1
2015-12-13 16:13:39,425 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0012/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_98842116_1
2015-12-13 16:13:39,451 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742428_1605{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12229
2015-12-13 16:13:39,454 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0012/job_1450020873492_0012_1.jhist is closed by DFSClient_NONMAPREDUCE_98842116_1
2015-12-13 16:13:39,464 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0012.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742430_1607{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:13:39,472 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742430_1607{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:13:39,474 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0012.summary_tmp is closed by DFSClient_NONMAPREDUCE_98842116_1
2015-12-13 16:13:39,525 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0012-1450041183478-parallels-task+C1-1450041219427-3-1-SUCCEEDED-default-1450041189960.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742431_1608{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:13:39,538 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742431_1608{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:13:39,541 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0012-1450041183478-parallels-task+C1-1450041219427-3-1-SUCCEEDED-default-1450041189960.jhist_tmp is closed by DFSClient_NONMAPREDUCE_98842116_1
2015-12-13 16:13:39,568 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0012_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742432_1609{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:13:39,584 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742432_1609{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:13:39,587 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0012_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_98842116_1
2015-12-13 16:13:40,647 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742423_1600 127.0.0.1:50010 
2015-12-13 16:13:40,648 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742424_1601 127.0.0.1:50010 
2015-12-13 16:13:40,648 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742425_1602 127.0.0.1:50010 
2015-12-13 16:13:40,648 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742426_1603 127.0.0.1:50010 
2015-12-13 16:13:40,648 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742428_1605 127.0.0.1:50010 
2015-12-13 16:13:40,648 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742427_1604 127.0.0.1:50010 
2015-12-13 16:13:41,705 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742423_1600, blk_1073742424_1601, blk_1073742425_1602, blk_1073742426_1603, blk_1073742427_1604, blk_1073742428_1605]
2015-12-13 16:14:04,153 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:14:04,154 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 16:14:34,153 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:14:34,154 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:15:04,154 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:15:04,154 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:15:34,155 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:15:34,155 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:16:04,158 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-13 16:16:04,158 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:16:34,158 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:16:34,159 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:17:04,160 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:17:04,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:17:34,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:17:34,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:18:04,162 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:18:04,163 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:18:34,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 16:18:34,164 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:19:04,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:19:04,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:19:34,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:19:34,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:20:04,167 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:20:04,167 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:20:34,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:20:34,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:21:04,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:21:04,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:21:34,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:21:34,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:22:04,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:22:04,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:22:34,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 16:22:34,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:23:04,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:23:04,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:23:34,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:23:34,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:24:04,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:24:04,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:24:34,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:24:34,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:25:04,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:25:04,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:25:34,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:25:34,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:26:04,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:26:04,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:26:34,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:26:34,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:27:01,215 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 86 Total time for transactions(ms): 14 Number of transactions batched in Syncs: 0 Number of syncs: 59 SyncTimes(ms): 245 
2015-12-13 16:27:01,216 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742429_1606 127.0.0.1:50010 
2015-12-13 16:27:03,003 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742429_1606]
2015-12-13 16:27:04,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-13 16:27:04,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:27:05,593 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0013/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742433_1610{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:27:05,733 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742433_1610{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:27:05,739 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0013/job.jar is closed by DFSClient_NONMAPREDUCE_-768707732_1
2015-12-13 16:27:05,749 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0013/job.jar
2015-12-13 16:27:05,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0013/job.split
2015-12-13 16:27:05,841 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0013/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742434_1611{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:27:05,852 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742434_1611{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:27:05,854 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0013/job.split is closed by DFSClient_NONMAPREDUCE_-768707732_1
2015-12-13 16:27:05,870 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0013/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742435_1612{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:27:05,877 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742435_1612{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:27:05,879 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0013/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-768707732_1
2015-12-13 16:27:06,180 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0013/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742436_1613{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:27:06,201 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742436_1613{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:27:06,214 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0013/job.xml is closed by DFSClient_NONMAPREDUCE_-768707732_1
2015-12-13 16:27:11,691 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0013/job_1450020873492_0013_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742437_1614{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:27:11,881 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742437_1614{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:27:11,888 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0013/job_1450020873492_0013_1_conf.xml is closed by DFSClient_NONMAPREDUCE_2002467459_1
2015-12-13 16:27:25,783 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0013/job_1450020873492_0013_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742438_1615{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:27:25,871 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0013/job_1450020873492_0013_1.jhist for DFSClient_NONMAPREDUCE_2002467459_1
2015-12-13 16:27:34,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:27:34,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 5 millisecond(s).
2015-12-13 16:27:35,557 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0013_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742439_1616{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:27:35,776 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742439_1616{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:27:35,781 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0013_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0013_r_000000_0_-1332557209_1
2015-12-13 16:27:37,052 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0013/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_2002467459_1
2015-12-13 16:27:37,091 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_2002467459_1
2015-12-13 16:27:37,097 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0013/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_2002467459_1
2015-12-13 16:27:37,127 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742438_1615{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12217
2015-12-13 16:27:37,130 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0013/job_1450020873492_0013_1.jhist is closed by DFSClient_NONMAPREDUCE_2002467459_1
2015-12-13 16:27:37,142 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0013.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742440_1617{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:27:37,149 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742440_1617{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:27:37,152 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0013.summary_tmp is closed by DFSClient_NONMAPREDUCE_2002467459_1
2015-12-13 16:27:37,187 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0013-1450042026489-parallels-task+C1-1450042057098-3-1-SUCCEEDED-default-1450042031285.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742441_1618{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:27:37,195 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742441_1618{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:27:37,197 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0013-1450042026489-parallels-task+C1-1450042057098-3-1-SUCCEEDED-default-1450042031285.jhist_tmp is closed by DFSClient_NONMAPREDUCE_2002467459_1
2015-12-13 16:27:37,225 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0013_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742442_1619{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:27:37,245 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742442_1619{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:27:37,248 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0013_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_2002467459_1
2015-12-13 16:27:38,315 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742433_1610 127.0.0.1:50010 
2015-12-13 16:27:38,315 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742434_1611 127.0.0.1:50010 
2015-12-13 16:27:38,315 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742435_1612 127.0.0.1:50010 
2015-12-13 16:27:38,315 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742436_1613 127.0.0.1:50010 
2015-12-13 16:27:38,315 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742438_1615 127.0.0.1:50010 
2015-12-13 16:27:38,315 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742437_1614 127.0.0.1:50010 
2015-12-13 16:27:39,024 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742433_1610, blk_1073742434_1611, blk_1073742435_1612, blk_1073742436_1613, blk_1073742437_1614, blk_1073742438_1615]
2015-12-13 16:28:04,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:28:04,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:28:34,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:28:34,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:29:04,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:29:04,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:29:32,663 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 167 Total time for transactions(ms): 31 Number of transactions batched in Syncs: 0 Number of syncs: 116 SyncTimes(ms): 390 
2015-12-13 16:29:32,664 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742439_1616 127.0.0.1:50010 
2015-12-13 16:29:33,057 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742439_1616]
2015-12-13 16:29:34,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:29:34,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:29:36,824 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0014/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742443_1620{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:29:37,009 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742443_1620{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:29:37,015 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0014/job.jar is closed by DFSClient_NONMAPREDUCE_175554112_1
2015-12-13 16:29:37,023 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0014/job.jar
2015-12-13 16:29:37,097 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0014/job.split
2015-12-13 16:29:37,110 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0014/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742444_1621{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:29:37,121 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742444_1621{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:29:37,124 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0014/job.split is closed by DFSClient_NONMAPREDUCE_175554112_1
2015-12-13 16:29:37,141 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0014/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742445_1622{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:29:37,150 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742445_1622{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-13 16:29:37,151 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742445_1622{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 52
2015-12-13 16:29:37,555 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0014/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_175554112_1
2015-12-13 16:29:37,808 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0014/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742446_1623{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:29:37,821 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742446_1623{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:29:37,823 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0014/job.xml is closed by DFSClient_NONMAPREDUCE_175554112_1
2015-12-13 16:29:44,081 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0014/job_1450020873492_0014_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742447_1624{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:29:44,198 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742447_1624{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:29:44,203 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0014/job_1450020873492_0014_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-797443036_1
2015-12-13 16:29:57,651 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0014/job_1450020873492_0014_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742448_1625{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:29:57,773 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0014/job_1450020873492_0014_1.jhist for DFSClient_NONMAPREDUCE_-797443036_1
2015-12-13 16:30:04,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 16:30:04,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 7 millisecond(s).
2015-12-13 16:30:18,184 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0014_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742449_1626{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:30:18,417 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742449_1626{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:30:18,425 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0014_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0014_r_000000_0_-229086529_1
2015-12-13 16:30:19,698 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0014/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-797443036_1
2015-12-13 16:30:19,767 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-797443036_1
2015-12-13 16:30:19,775 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0014/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-797443036_1
2015-12-13 16:30:19,804 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742448_1625{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12217
2015-12-13 16:30:19,813 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0014/job_1450020873492_0014_1.jhist is closed by DFSClient_NONMAPREDUCE_-797443036_1
2015-12-13 16:30:19,840 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0014.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742450_1627{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:30:19,968 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742450_1627{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-13 16:30:19,969 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742450_1627{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 351
2015-12-13 16:30:20,373 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0014.summary_tmp is closed by DFSClient_NONMAPREDUCE_-797443036_1
2015-12-13 16:30:20,435 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0014-1450042178115-parallels-task+C1-1450042219778-3-1-SUCCEEDED-default-1450042183668.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742451_1628{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:30:20,447 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742451_1628{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:30:20,456 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0014-1450042178115-parallels-task+C1-1450042219778-3-1-SUCCEEDED-default-1450042183668.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-797443036_1
2015-12-13 16:30:20,492 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0014_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742452_1629{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:30:20,506 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742452_1629{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:30:20,509 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0014_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-797443036_1
2015-12-13 16:30:21,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742443_1620 127.0.0.1:50010 
2015-12-13 16:30:21,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742444_1621 127.0.0.1:50010 
2015-12-13 16:30:21,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742445_1622 127.0.0.1:50010 
2015-12-13 16:30:21,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742446_1623 127.0.0.1:50010 
2015-12-13 16:30:21,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742448_1625 127.0.0.1:50010 
2015-12-13 16:30:21,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742447_1624 127.0.0.1:50010 
2015-12-13 16:30:22,500 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0015/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742453_1630{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:30:22,524 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742453_1630{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:30:22,526 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0015/job.jar is closed by DFSClient_NONMAPREDUCE_175554112_1
2015-12-13 16:30:22,533 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0015/job.jar
2015-12-13 16:30:22,561 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0015/job.split
2015-12-13 16:30:22,567 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0015/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742454_1631{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:30:22,585 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742454_1631{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:30:22,594 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0015/job.split is closed by DFSClient_NONMAPREDUCE_175554112_1
2015-12-13 16:30:22,614 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0015/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742455_1632{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:30:22,622 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742455_1632{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:30:22,629 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0015/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_175554112_1
2015-12-13 16:30:22,687 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0015/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742456_1633{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:30:22,701 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742456_1633{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:30:22,706 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0015/job.xml is closed by DFSClient_NONMAPREDUCE_175554112_1
2015-12-13 16:30:24,079 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742448_1625, blk_1073742443_1620, blk_1073742444_1621, blk_1073742445_1622, blk_1073742446_1623, blk_1073742447_1624]
2015-12-13 16:30:32,678 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 278 Total time for transactions(ms): 45 Number of transactions batched in Syncs: 2 Number of syncs: 193 SyncTimes(ms): 442 
2015-12-13 16:30:33,040 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0015/job_1450020873492_0015_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742457_1634{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:30:33,164 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742457_1634{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:30:33,173 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0015/job_1450020873492_0015_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-723917045_1
2015-12-13 16:30:34,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 16:30:34,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 8 millisecond(s).
2015-12-13 16:30:38,028 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0015/job_1450020873492_0015_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742458_1635{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:30:38,084 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0015/job_1450020873492_0015_1.jhist for DFSClient_NONMAPREDUCE_-723917045_1
2015-12-13 16:30:55,039 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742458_1635{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12152
2015-12-13 16:30:55,043 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0015/job_1450020873492_0015_1.jhist is closed by DFSClient_NONMAPREDUCE_-723917045_1
2015-12-13 16:30:55,055 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0015.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742459_1636{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:30:55,063 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742459_1636{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:30:55,069 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0015.summary_tmp is closed by DFSClient_NONMAPREDUCE_-723917045_1
2015-12-13 16:30:55,109 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0015-1450042222763-parallels-task+C2-1450042255002-0-0-FAILED-default-1450042232654.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742460_1637{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:30:55,118 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742460_1637{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-13 16:30:55,120 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742460_1637{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 22455
2015-12-13 16:30:55,541 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0015-1450042222763-parallels-task+C2-1450042255002-0-0-FAILED-default-1450042232654.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-723917045_1
2015-12-13 16:30:55,577 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0015_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742461_1638{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:30:55,597 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742461_1638{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:30:55,601 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0015_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-723917045_1
2015-12-13 16:30:56,701 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742453_1630 127.0.0.1:50010 
2015-12-13 16:30:56,702 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742454_1631 127.0.0.1:50010 
2015-12-13 16:30:56,702 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742455_1632 127.0.0.1:50010 
2015-12-13 16:30:56,702 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742456_1633 127.0.0.1:50010 
2015-12-13 16:30:56,702 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742458_1635 127.0.0.1:50010 
2015-12-13 16:30:56,703 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742457_1634 127.0.0.1:50010 
2015-12-13 16:30:57,087 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742453_1630, blk_1073742454_1631, blk_1073742455_1632, blk_1073742456_1633, blk_1073742457_1634, blk_1073742458_1635]
2015-12-13 16:31:04,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 16:31:04,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 16:31:34,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:31:34,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:32:04,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 16:32:04,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:32:34,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:32:34,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:33:04,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:33:04,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:33:34,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:33:34,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:34:04,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:34:04,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:34:34,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:34:34,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 16:35:04,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:35:04,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:35:34,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:35:34,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:35:49,692 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 313 Total time for transactions(ms): 50 Number of transactions batched in Syncs: 3 Number of syncs: 218 SyncTimes(ms): 516 
2015-12-13 16:35:51,792 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742449_1626 127.0.0.1:50010 
2015-12-13 16:35:54,179 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742449_1626]
2015-12-13 16:35:56,089 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0016/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742462_1639{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:35:56,223 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742462_1639{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:35:56,230 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0016/job.jar is closed by DFSClient_NONMAPREDUCE_-1463941228_1
2015-12-13 16:35:56,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0016/job.jar
2015-12-13 16:35:56,321 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0016/job.split
2015-12-13 16:35:56,337 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0016/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742463_1640{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:35:56,349 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742463_1640{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:35:56,351 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0016/job.split is closed by DFSClient_NONMAPREDUCE_-1463941228_1
2015-12-13 16:35:56,369 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0016/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742464_1641{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:35:56,377 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742464_1641{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:35:56,380 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0016/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1463941228_1
2015-12-13 16:35:56,577 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0016/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742465_1642{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:35:56,594 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742465_1642{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:35:56,597 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0016/job.xml is closed by DFSClient_NONMAPREDUCE_-1463941228_1
2015-12-13 16:36:02,678 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0016/job_1450020873492_0016_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742466_1643{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:36:02,815 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742466_1643{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:36:02,823 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0016/job_1450020873492_0016_1_conf.xml is closed by DFSClient_NONMAPREDUCE_983214958_1
2015-12-13 16:36:04,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:36:04,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:36:16,445 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1516ms
No GCs detected
2015-12-13 16:36:18,928 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0016/job_1450020873492_0016_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742467_1644{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:36:19,072 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0016/job_1450020873492_0016_1.jhist for DFSClient_NONMAPREDUCE_983214958_1
2015-12-13 16:36:28,850 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0016_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742468_1645{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:36:29,046 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742468_1645{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:36:29,054 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0016_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0016_r_000000_0_-923351591_1
2015-12-13 16:36:29,419 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0016/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_983214958_1
2015-12-13 16:36:29,458 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_983214958_1
2015-12-13 16:36:29,466 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0016/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_983214958_1
2015-12-13 16:36:29,494 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742467_1644{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14342
2015-12-13 16:36:29,497 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0016/job_1450020873492_0016_1.jhist is closed by DFSClient_NONMAPREDUCE_983214958_1
2015-12-13 16:36:29,510 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0016.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742469_1646{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:36:29,520 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742469_1646{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:36:29,523 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0016.summary_tmp is closed by DFSClient_NONMAPREDUCE_983214958_1
2015-12-13 16:36:29,588 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0016-1450042556887-parallels-task+C1-1450042589468-3-1-SUCCEEDED-default-1450042562249.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742470_1647{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:36:29,610 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742470_1647{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:36:29,612 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0016-1450042556887-parallels-task+C1-1450042589468-3-1-SUCCEEDED-default-1450042562249.jhist_tmp is closed by DFSClient_NONMAPREDUCE_983214958_1
2015-12-13 16:36:29,640 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0016_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742471_1648{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:36:29,652 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742471_1648{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:36:29,655 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0016_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_983214958_1
2015-12-13 16:36:30,728 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742462_1639 127.0.0.1:50010 
2015-12-13 16:36:30,729 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742463_1640 127.0.0.1:50010 
2015-12-13 16:36:30,729 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742464_1641 127.0.0.1:50010 
2015-12-13 16:36:30,730 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742465_1642 127.0.0.1:50010 
2015-12-13 16:36:30,730 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742467_1644 127.0.0.1:50010 
2015-12-13 16:36:30,730 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742466_1643 127.0.0.1:50010 
2015-12-13 16:36:31,424 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742464_1641, blk_1073742465_1642, blk_1073742466_1643, blk_1073742467_1644, blk_1073742462_1639, blk_1073742463_1640]
2015-12-13 16:36:31,865 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0017/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742472_1649{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:36:31,904 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742472_1649{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:36:31,911 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0017/job.jar is closed by DFSClient_NONMAPREDUCE_-1463941228_1
2015-12-13 16:36:31,916 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0017/job.jar
2015-12-13 16:36:31,947 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0017/job.split
2015-12-13 16:36:31,954 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0017/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742473_1650{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:36:31,972 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742473_1650{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:36:31,980 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0017/job.split is closed by DFSClient_NONMAPREDUCE_-1463941228_1
2015-12-13 16:36:31,993 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0017/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742474_1651{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:36:32,005 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742474_1651{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:36:32,012 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0017/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1463941228_1
2015-12-13 16:36:32,083 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0017/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742475_1652{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:36:32,095 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742475_1652{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:36:32,100 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0017/job.xml is closed by DFSClient_NONMAPREDUCE_-1463941228_1
2015-12-13 16:36:34,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-13 16:36:34,209 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 11 millisecond(s).
2015-12-13 16:36:42,066 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0017/job_1450020873492_0017_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742476_1653{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:36:42,152 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742476_1653{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:36:42,161 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0017/job_1450020873492_0017_1_conf.xml is closed by DFSClient_NONMAPREDUCE_837182434_1
2015-12-13 16:36:47,497 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0017/job_1450020873492_0017_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742477_1654{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:36:47,648 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0017/job_1450020873492_0017_1.jhist for DFSClient_NONMAPREDUCE_837182434_1
2015-12-13 16:36:53,963 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 438 Total time for transactions(ms): 71 Number of transactions batched in Syncs: 3 Number of syncs: 303 SyncTimes(ms): 746 
2015-12-13 16:36:54,037 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0017_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742478_1655{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:36:54,218 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742478_1655{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:36:54,225 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0017_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0017_r_000000_0_-70954622_1
2015-12-13 16:36:54,489 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0017/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_837182434_1
2015-12-13 16:36:54,536 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_SUCCESS is closed by DFSClient_NONMAPREDUCE_837182434_1
2015-12-13 16:36:54,543 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0017/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_837182434_1
2015-12-13 16:36:54,566 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742477_1654{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12979
2015-12-13 16:36:54,568 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0017/job_1450020873492_0017_1.jhist is closed by DFSClient_NONMAPREDUCE_837182434_1
2015-12-13 16:36:54,579 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0017.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742479_1656{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:36:54,585 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742479_1656{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:36:54,589 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0017.summary_tmp is closed by DFSClient_NONMAPREDUCE_837182434_1
2015-12-13 16:36:54,615 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0017-1450042592183-parallels-task+C2-1450042614545-1-1-SUCCEEDED-default-1450042601689.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742480_1657{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:36:54,621 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742480_1657{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:36:54,624 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0017-1450042592183-parallels-task+C2-1450042614545-1-1-SUCCEEDED-default-1450042601689.jhist_tmp is closed by DFSClient_NONMAPREDUCE_837182434_1
2015-12-13 16:36:54,646 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0017_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742481_1658{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:36:54,653 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742481_1658{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:36:54,655 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0017_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_837182434_1
2015-12-13 16:36:55,691 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742472_1649 127.0.0.1:50010 
2015-12-13 16:36:55,691 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742473_1650 127.0.0.1:50010 
2015-12-13 16:36:55,691 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742474_1651 127.0.0.1:50010 
2015-12-13 16:36:55,692 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742475_1652 127.0.0.1:50010 
2015-12-13 16:36:55,692 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742477_1654 127.0.0.1:50010 
2015-12-13 16:36:55,692 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742476_1653 127.0.0.1:50010 
2015-12-13 16:36:58,427 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742472_1649, blk_1073742473_1650, blk_1073742474_1651, blk_1073742475_1652, blk_1073742476_1653, blk_1073742477_1654]
2015-12-13 16:37:04,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:37:04,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:37:34,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:37:34,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:38:04,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:38:04,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:38:34,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:38:34,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:38:57,622 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 475 Total time for transactions(ms): 76 Number of transactions batched in Syncs: 3 Number of syncs: 332 SyncTimes(ms): 763 
2015-12-13 16:38:57,624 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742478_1655 127.0.0.1:50010 
2015-12-13 16:38:58,453 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742478_1655]
2015-12-13 16:38:59,629 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742468_1645 127.0.0.1:50010 
2015-12-13 16:39:01,453 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742468_1645]
2015-12-13 16:39:04,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:39:04,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:39:04,560 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0018/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742482_1659{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:39:04,677 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742482_1659{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:39:04,680 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0018/job.jar is closed by DFSClient_NONMAPREDUCE_-1623357415_1
2015-12-13 16:39:04,686 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0018/job.jar
2015-12-13 16:39:04,747 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0018/job.split
2015-12-13 16:39:04,761 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0018/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742483_1660{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:39:04,769 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742483_1660{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:39:04,772 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0018/job.split is closed by DFSClient_NONMAPREDUCE_-1623357415_1
2015-12-13 16:39:04,789 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0018/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742484_1661{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:39:04,800 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742484_1661{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:39:04,804 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0018/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1623357415_1
2015-12-13 16:39:04,980 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0018/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742485_1662{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:39:05,014 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742485_1662{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:39:05,020 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0018/job.xml is closed by DFSClient_NONMAPREDUCE_-1623357415_1
2015-12-13 16:39:10,495 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0018/job_1450020873492_0018_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742486_1663{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:39:10,621 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742486_1663{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:39:10,626 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0018/job_1450020873492_0018_1_conf.xml is closed by DFSClient_NONMAPREDUCE_58113917_1
2015-12-13 16:39:24,635 WARN org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:41879 Call#3 Retry#0: output error
2015-12-13 16:39:24,652 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:270)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:479)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2573)
	at org.apache.hadoop.ipc.Server.access$1900(Server.java:135)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:977)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1042)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2094)
2015-12-13 16:39:27,369 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0018/job_1450020873492_0018_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742487_1664{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:39:27,562 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0018/job_1450020873492_0018_1.jhist for DFSClient_NONMAPREDUCE_58113917_1
2015-12-13 16:39:34,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30007 milliseconds
2015-12-13 16:39:34,263 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 57 millisecond(s).
2015-12-13 16:39:36,987 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0018_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742488_1665{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:39:37,175 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742488_1665{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:39:37,207 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0018_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0018_r_000000_0_1082447436_1
2015-12-13 16:39:37,589 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0018/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_58113917_1
2015-12-13 16:39:37,635 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_58113917_1
2015-12-13 16:39:37,643 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0018/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_58113917_1
2015-12-13 16:39:37,680 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742487_1664{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12229
2015-12-13 16:39:37,800 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0018/job_1450020873492_0018_1.jhist is closed by DFSClient_NONMAPREDUCE_58113917_1
2015-12-13 16:39:37,813 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0018.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742489_1666{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:39:37,827 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742489_1666{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:39:37,830 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0018.summary_tmp is closed by DFSClient_NONMAPREDUCE_58113917_1
2015-12-13 16:39:37,886 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0018-1450042745246-parallels-task+C1-1450042777644-3-1-SUCCEEDED-default-1450042750143.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742490_1667{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:39:37,894 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742490_1667{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:39:37,898 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0018-1450042745246-parallels-task+C1-1450042777644-3-1-SUCCEEDED-default-1450042750143.jhist_tmp is closed by DFSClient_NONMAPREDUCE_58113917_1
2015-12-13 16:39:37,928 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0018_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742491_1668{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:39:37,937 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742491_1668{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:39:37,945 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0018_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_58113917_1
2015-12-13 16:39:38,993 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742482_1659 127.0.0.1:50010 
2015-12-13 16:39:38,993 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742483_1660 127.0.0.1:50010 
2015-12-13 16:39:38,993 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742484_1661 127.0.0.1:50010 
2015-12-13 16:39:38,993 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742485_1662 127.0.0.1:50010 
2015-12-13 16:39:38,994 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742487_1664 127.0.0.1:50010 
2015-12-13 16:39:38,994 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742486_1663 127.0.0.1:50010 
2015-12-13 16:39:39,818 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0019/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742492_1669{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:39:39,840 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742492_1669{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:39:39,844 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0019/job.jar is closed by DFSClient_NONMAPREDUCE_-1623357415_1
2015-12-13 16:39:39,846 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0019/job.jar
2015-12-13 16:39:39,873 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0019/job.split
2015-12-13 16:39:39,878 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0019/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742493_1670{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:39:39,897 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742493_1670{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:39:39,899 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0019/job.split is closed by DFSClient_NONMAPREDUCE_-1623357415_1
2015-12-13 16:39:39,914 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0019/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742494_1671{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:39:39,932 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742494_1671{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:39:39,941 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0019/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1623357415_1
2015-12-13 16:39:40,013 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0019/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742495_1672{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:39:40,028 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742495_1672{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:39:40,031 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0019/job.xml is closed by DFSClient_NONMAPREDUCE_-1623357415_1
2015-12-13 16:39:40,464 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742482_1659, blk_1073742483_1660, blk_1073742484_1661, blk_1073742485_1662, blk_1073742486_1663, blk_1073742487_1664]
2015-12-13 16:39:50,530 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0019/job_1450020873492_0019_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742496_1673{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:39:50,687 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742496_1673{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:39:50,692 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0019/job_1450020873492_0019_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-840323409_1
2015-12-13 16:39:55,603 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0019/job_1450020873492_0019_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742497_1674{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:39:55,620 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0019/job_1450020873492_0019_1.jhist for DFSClient_NONMAPREDUCE_-840323409_1
2015-12-13 16:40:02,327 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 600 Total time for transactions(ms): 94 Number of transactions batched in Syncs: 3 Number of syncs: 417 SyncTimes(ms): 933 
2015-12-13 16:40:02,413 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0019_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742498_1675{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:40:02,589 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742498_1675{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:40:02,593 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0019_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0019_r_000000_0_-1640173744_1
2015-12-13 16:40:02,847 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0019/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-840323409_1
2015-12-13 16:40:02,888 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-840323409_1
2015-12-13 16:40:02,895 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0019/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-840323409_1
2015-12-13 16:40:02,917 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742497_1674{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12978
2015-12-13 16:40:02,921 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0019/job_1450020873492_0019_1.jhist is closed by DFSClient_NONMAPREDUCE_-840323409_1
2015-12-13 16:40:02,930 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0019.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742499_1676{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:40:02,936 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742499_1676{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:40:02,938 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0019.summary_tmp is closed by DFSClient_NONMAPREDUCE_-840323409_1
2015-12-13 16:40:02,973 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0019-1450042780073-parallels-task+C2-1450042802897-1-1-SUCCEEDED-default-1450042790129.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742500_1677{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:40:02,980 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742500_1677{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:40:02,983 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0019-1450042780073-parallels-task+C2-1450042802897-1-1-SUCCEEDED-default-1450042790129.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-840323409_1
2015-12-13 16:40:03,007 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0019_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742501_1678{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:40:03,014 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742501_1678{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:40:03,017 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0019_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-840323409_1
2015-12-13 16:40:04,049 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742492_1669 127.0.0.1:50010 
2015-12-13 16:40:04,049 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742493_1670 127.0.0.1:50010 
2015-12-13 16:40:04,050 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742494_1671 127.0.0.1:50010 
2015-12-13 16:40:04,050 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742495_1672 127.0.0.1:50010 
2015-12-13 16:40:04,050 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742497_1674 127.0.0.1:50010 
2015-12-13 16:40:04,050 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742496_1673 127.0.0.1:50010 
2015-12-13 16:40:04,207 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:40:04,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:40:04,467 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742496_1673, blk_1073742497_1674, blk_1073742492_1669, blk_1073742493_1670, blk_1073742494_1671, blk_1073742495_1672]
2015-12-13 16:40:34,207 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:40:34,209 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 16:41:04,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:41:04,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:41:34,209 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:41:34,209 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:42:04,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:42:04,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:42:34,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:42:34,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 4 millisecond(s).
2015-12-13 16:42:57,034 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-13 16:42:57,034 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-13 16:42:57,034 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 4853
2015-12-13 16:42:57,034 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 637 Total time for transactions(ms): 96 Number of transactions batched in Syncs: 3 Number of syncs: 446 SyncTimes(ms): 953 
2015-12-13 16:42:57,040 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 637 Total time for transactions(ms): 96 Number of transactions batched in Syncs: 3 Number of syncs: 447 SyncTimes(ms): 958 
2015-12-13 16:42:57,048 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000004853 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000004853-0000000000000005489
2015-12-13 16:42:57,050 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 5490
2015-12-13 16:42:57,858 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 6750.00 KB/s
2015-12-13 16:42:57,858 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000005489 size 27934 bytes.
2015-12-13 16:42:57,865 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 4852
2015-12-13 16:42:57,865 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000004766, cpktTxId=0000000000000004766)
2015-12-13 16:43:04,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:43:04,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:43:34,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:43:34,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:44:04,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:44:04,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:44:34,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:44:34,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:45:04,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 16:45:04,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:45:34,216 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:45:34,216 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:46:04,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:46:04,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:46:34,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:46:34,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:47:03,726 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 15 
2015-12-13 16:47:03,727 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742498_1675 127.0.0.1:50010 
2015-12-13 16:47:04,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:47:04,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:47:04,583 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742498_1675]
2015-12-13 16:47:05,783 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742488_1665 127.0.0.1:50010 
2015-12-13 16:47:07,584 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742488_1665]
2015-12-13 16:47:34,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 16:47:34,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:48:04,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:48:04,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:48:34,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:48:34,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:49:04,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 16:49:04,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:49:34,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:49:34,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:50:04,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:50:04,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:50:21,811 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 22 
2015-12-13 16:50:21,978 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0021/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742502_1679{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:50:22,139 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742502_1679{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:50:22,143 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0021/job.jar is closed by DFSClient_NONMAPREDUCE_1988857330_1
2015-12-13 16:50:22,149 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0021/job.jar
2015-12-13 16:50:22,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0021/job.split
2015-12-13 16:50:22,250 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0021/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742503_1680{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:50:22,259 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742503_1680{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:50:22,261 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0021/job.split is closed by DFSClient_NONMAPREDUCE_1988857330_1
2015-12-13 16:50:22,275 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0021/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742504_1681{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:50:22,281 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742504_1681{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:50:22,284 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0021/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1988857330_1
2015-12-13 16:50:22,511 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0021/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742505_1682{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:50:22,529 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742505_1682{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:50:22,531 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0021/job.xml is closed by DFSClient_NONMAPREDUCE_1988857330_1
2015-12-13 16:50:29,078 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0021/job_1450020873492_0021_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742506_1683{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:50:29,221 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742506_1683{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:50:29,227 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0021/job_1450020873492_0021_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1581314340_1
2015-12-13 16:50:34,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:50:34,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 4 millisecond(s).
2015-12-13 16:50:43,764 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0021/job_1450020873492_0021_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742507_1684{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:50:44,289 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0021/job_1450020873492_0021_1.jhist for DFSClient_NONMAPREDUCE_-1581314340_1
2015-12-13 16:50:53,162 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0021_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742508_1685{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:50:53,321 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742508_1685{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:50:53,329 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0021_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0021_r_000000_0_1415239889_1
2015-12-13 16:50:54,641 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0021/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1581314340_1
2015-12-13 16:50:54,684 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1581314340_1
2015-12-13 16:50:54,691 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0021/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1581314340_1
2015-12-13 16:50:54,721 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742507_1684{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12229
2015-12-13 16:50:54,725 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0021/job_1450020873492_0021_1.jhist is closed by DFSClient_NONMAPREDUCE_-1581314340_1
2015-12-13 16:50:54,740 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0021.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742509_1686{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:50:54,747 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742509_1686{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:50:54,753 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0021.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1581314340_1
2015-12-13 16:50:54,800 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0021-1450043422809-parallels-task+C1-1450043454694-3-1-SUCCEEDED-default-1450043428649.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742510_1687{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:50:54,815 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742510_1687{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:50:54,822 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0021-1450043422809-parallels-task+C1-1450043454694-3-1-SUCCEEDED-default-1450043428649.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1581314340_1
2015-12-13 16:50:54,848 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0021_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742511_1688{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:50:54,860 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742511_1688{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:50:54,864 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0021_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1581314340_1
2015-12-13 16:50:55,919 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742502_1679 127.0.0.1:50010 
2015-12-13 16:50:55,920 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742503_1680 127.0.0.1:50010 
2015-12-13 16:50:55,921 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742504_1681 127.0.0.1:50010 
2015-12-13 16:50:55,921 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742505_1682 127.0.0.1:50010 
2015-12-13 16:50:55,921 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742507_1684 127.0.0.1:50010 
2015-12-13 16:50:55,921 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742506_1683 127.0.0.1:50010 
2015-12-13 16:50:56,577 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742502_1679, blk_1073742503_1680, blk_1073742504_1681, blk_1073742505_1682, blk_1073742506_1683, blk_1073742507_1684]
2015-12-13 16:50:57,341 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0022/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742512_1689{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:50:57,368 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742512_1689{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:50:57,372 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0022/job.jar is closed by DFSClient_NONMAPREDUCE_1988857330_1
2015-12-13 16:50:57,376 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0022/job.jar
2015-12-13 16:50:57,420 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0022/job.split
2015-12-13 16:50:57,426 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0022/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742513_1690{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:50:57,437 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742513_1690{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:50:57,443 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0022/job.split is closed by DFSClient_NONMAPREDUCE_1988857330_1
2015-12-13 16:50:57,454 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0022/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742514_1691{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:50:57,466 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742514_1691{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:50:57,470 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0022/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1988857330_1
2015-12-13 16:50:57,548 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0022/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742515_1692{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:50:57,571 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742515_1692{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:50:57,574 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0022/job.xml is closed by DFSClient_NONMAPREDUCE_1988857330_1
2015-12-13 16:51:04,226 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:51:04,226 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:51:07,267 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0022/job_1450020873492_0022_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742516_1693{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:51:07,377 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742516_1693{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:51:07,381 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0022/job_1450020873492_0022_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-816585658_1
2015-12-13 16:51:12,289 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0022/job_1450020873492_0022_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742517_1694{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:51:12,427 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0022/job_1450020873492_0022_1.jhist for DFSClient_NONMAPREDUCE_-816585658_1
2015-12-13 16:51:19,129 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0022_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742518_1695{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:51:19,303 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742518_1695{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:51:19,306 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0022_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0022_r_000000_0_1196280041_1
2015-12-13 16:51:19,542 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0022/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-816585658_1
2015-12-13 16:51:19,587 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-816585658_1
2015-12-13 16:51:19,601 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0022/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-816585658_1
2015-12-13 16:51:19,625 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742517_1694{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12977
2015-12-13 16:51:19,628 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0022/job_1450020873492_0022_1.jhist is closed by DFSClient_NONMAPREDUCE_-816585658_1
2015-12-13 16:51:19,638 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0022.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742519_1696{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:51:19,644 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742519_1696{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:51:19,646 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0022.summary_tmp is closed by DFSClient_NONMAPREDUCE_-816585658_1
2015-12-13 16:51:19,676 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0022-1450043457649-parallels-task+C2-1450043479606-1-1-SUCCEEDED-default-1450043466908.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742520_1697{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:51:19,682 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742520_1697{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:51:19,686 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0022-1450043457649-parallels-task+C2-1450043479606-1-1-SUCCEEDED-default-1450043466908.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-816585658_1
2015-12-13 16:51:19,719 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0022_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742521_1698{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 16:51:19,729 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742521_1698{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 16:51:19,732 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0022_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-816585658_1
2015-12-13 16:51:20,769 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742512_1689 127.0.0.1:50010 
2015-12-13 16:51:20,769 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742513_1690 127.0.0.1:50010 
2015-12-13 16:51:20,769 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742514_1691 127.0.0.1:50010 
2015-12-13 16:51:20,770 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742515_1692 127.0.0.1:50010 
2015-12-13 16:51:20,770 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742517_1694 127.0.0.1:50010 
2015-12-13 16:51:20,771 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742516_1693 127.0.0.1:50010 
2015-12-13 16:51:23,583 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742512_1689, blk_1073742513_1690, blk_1073742514_1691, blk_1073742515_1692, blk_1073742516_1693, blk_1073742517_1694]
2015-12-13 16:51:34,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 16:51:34,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:52:04,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:52:04,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:52:34,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:52:34,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:53:04,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:53:04,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:53:34,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:53:34,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:54:04,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:54:04,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:54:34,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:54:34,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:55:04,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:55:04,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:55:34,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:55:34,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:56:04,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:56:04,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:56:34,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:56:34,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 16:57:04,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:57:04,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:57:34,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:57:34,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:58:04,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:58:04,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:58:34,235 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:58:34,235 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:59:04,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 16:59:04,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 16:59:34,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 16:59:34,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 17:00:04,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 17:00:04,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 17:00:34,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 17:00:34,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 17:01:04,239 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 17:01:04,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 17:01:34,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 17:01:34,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 17:02:04,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 17:02:04,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 17:02:34,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 17:02:34,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 17:03:04,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 17:03:04,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 17:03:34,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 17:03:34,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 17:04:04,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 17:04:04,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 17:04:34,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 17:04:34,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 17:05:04,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 17:05:04,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 17:05:34,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 17:05:34,249 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 17:06:04,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 17:06:04,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 17:06:34,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 17:06:34,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 17:07:04,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 17:07:04,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 17:07:34,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 17:07:34,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 17:08:04,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 17:08:04,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 17:08:34,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 17:08:34,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 17:09:04,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 17:09:04,254 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 18:48:38,738 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* removeDeadDatanode: lost heartbeat from 127.0.0.1:50010
2015-12-13 18:48:38,821 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/127.0.0.1:50010
2015-12-13 18:48:40,420 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0) storage 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-13 18:48:40,421 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/127.0.0.1:50010
2015-12-13 18:48:40,422 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-13 18:48:40,422 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-13 18:48:40,423 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: [DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010 failed.
2015-12-13 18:48:40,429 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-13 18:48:40,466 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 231, hasStaleStorages: false, processing time: 13 msecs
2015-12-13 18:49:05,899 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 18:49:05,899 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 18:49:35,899 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 18:49:35,899 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 18:50:05,899 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 18:50:05,899 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 18:50:35,899 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 18:50:35,900 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 18:51:05,899 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 18:51:05,899 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 18:51:35,899 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 18:51:35,899 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 18:52:05,899 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 18:52:05,901 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 18:52:35,900 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 18:52:35,900 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 18:53:05,900 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 18:53:05,901 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 18:53:35,901 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 18:53:35,902 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 18:54:05,902 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 18:54:05,902 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 18:54:35,903 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 18:54:35,905 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 18:55:05,904 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 18:55:05,904 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 18:55:35,904 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 18:55:35,905 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 18:56:05,905 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 18:56:05,905 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 18:56:35,905 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 18:56:35,905 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 18:57:05,905 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 18:57:05,905 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 18:57:35,906 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 18:57:35,906 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 18:58:05,905 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 18:58:05,906 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 18:58:22,560 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 231, hasStaleStorages: false, processing time: 2 msecs
2015-12-13 18:58:35,907 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 18:58:35,907 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 18:59:05,907 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 18:59:05,908 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 18:59:35,908 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 18:59:35,909 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:00:05,909 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:00:05,910 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:00:35,909 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:00:35,910 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:01:05,910 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:01:05,911 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:01:35,911 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:01:35,911 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:02:05,911 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:02:05,911 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:02:35,911 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:02:35,911 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:03:05,911 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:03:05,912 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:03:35,911 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:03:35,912 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:04:05,913 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:04:05,913 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:04:35,912 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:04:35,913 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:05:05,913 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:05:05,914 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:05:35,914 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:05:35,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:06:05,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:06:05,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:06:35,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:06:35,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:07:05,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:07:05,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:07:35,914 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:07:35,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:08:05,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:08:05,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:08:35,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:08:35,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:09:05,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:09:05,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:09:35,914 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:09:35,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:10:05,914 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:10:05,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:10:35,917 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-13 19:10:35,918 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:11:05,918 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:11:05,918 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:11:35,917 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:11:35,918 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:12:05,918 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:12:05,919 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:12:35,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:12:35,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:13:05,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:13:05,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:13:35,919 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:13:35,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:14:05,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:14:05,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:14:35,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:14:35,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:15:05,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:15:05,922 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:15:35,922 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:15:35,924 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 19:16:05,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:16:05,924 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:16:35,924 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:16:35,924 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:17:05,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:17:05,924 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:17:35,924 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:17:35,925 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:18:05,925 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:18:05,925 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:18:35,925 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:18:35,925 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:19:05,924 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:19:05,925 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:19:35,925 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:19:35,926 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:20:05,926 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:20:05,927 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:20:35,928 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:20:35,928 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:21:05,927 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:21:05,928 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:21:35,928 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:21:35,928 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:22:05,928 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:22:05,928 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:22:30,724 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-13 19:22:30,724 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-13 19:22:30,725 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 5490
2015-12-13 19:22:30,726 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 166 Total time for transactions(ms): 34 Number of transactions batched in Syncs: 0 Number of syncs: 116 SyncTimes(ms): 698 
2015-12-13 19:22:30,728 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 166 Total time for transactions(ms): 34 Number of transactions batched in Syncs: 0 Number of syncs: 117 SyncTimes(ms): 699 
2015-12-13 19:22:30,733 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000005490 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000005490-0000000000000005655
2015-12-13 19:22:30,735 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 5656
2015-12-13 19:22:31,229 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 9000.00 KB/s
2015-12-13 19:22:31,230 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000005655 size 28642 bytes.
2015-12-13 19:22:31,236 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 5489
2015-12-13 19:22:31,237 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000004852, cpktTxId=0000000000000004852)
2015-12-13 19:22:35,927 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:22:35,928 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:23:05,929 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:23:05,929 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:23:35,929 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:23:35,929 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:24:05,930 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:24:05,930 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:24:35,929 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:24:35,930 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:25:05,930 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:25:05,930 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:25:35,931 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:25:35,931 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:26:05,931 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:26:05,931 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:26:35,930 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:26:35,931 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:27:05,930 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:27:05,932 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:27:35,932 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 19:27:35,932 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:28:05,933 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:28:05,933 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:28:35,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:28:35,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:29:05,934 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:29:05,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:29:35,935 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:29:35,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:30:05,936 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:30:05,939 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 19:30:35,938 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 19:30:35,939 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:31:05,940 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:31:05,940 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:31:35,940 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:31:35,941 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 19:32:05,940 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:32:05,940 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:32:35,939 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:32:35,940 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:33:05,940 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:33:05,940 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:33:35,941 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:33:35,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:34:05,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:34:05,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:34:35,941 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:34:35,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:35:05,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:35:05,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:35:35,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:35:35,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:36:05,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:36:05,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:36:35,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:36:35,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:37:05,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:37:05,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:37:35,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:37:35,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:38:05,942 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:38:05,943 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:38:35,943 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:38:35,943 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:39:05,943 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:39:05,944 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:39:35,943 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:39:35,944 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:40:05,944 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:40:05,945 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:40:27,176 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 43 
2015-12-13 19:40:27,178 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742518_1695 127.0.0.1:50010 
2015-12-13 19:40:29,358 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742518_1695]
2015-12-13 19:40:29,361 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742508_1685 127.0.0.1:50010 
2015-12-13 19:40:32,359 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742508_1685]
2015-12-13 19:40:35,946 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 19:40:35,947 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:41:04,899 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0024/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742522_1699{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:41:05,044 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742522_1699{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:41:05,048 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0024/job.jar is closed by DFSClient_NONMAPREDUCE_-239153680_1
2015-12-13 19:41:05,053 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0024/job.jar
2015-12-13 19:41:05,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0024/job.split
2015-12-13 19:41:05,131 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0024/job.split is closed by DFSClient_NONMAPREDUCE_-239153680_1
2015-12-13 19:41:05,947 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:41:05,947 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:41:35,948 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:41:35,948 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 19:41:49,013 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 17 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 0 Number of syncs: 15 SyncTimes(ms): 51 
2015-12-13 19:41:49,127 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0025/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742523_1700{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:41:49,248 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742523_1700{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:41:49,255 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0025/job.jar is closed by DFSClient_NONMAPREDUCE_-1792814267_1
2015-12-13 19:41:49,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0025/job.jar
2015-12-13 19:41:49,324 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0025/job.split
2015-12-13 19:41:49,338 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0025/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742524_1701{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:41:49,348 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742524_1701{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:41:49,349 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0025/job.split is closed by DFSClient_NONMAPREDUCE_-1792814267_1
2015-12-13 19:41:49,365 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0025/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742525_1702{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:41:49,377 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742525_1702{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:41:49,380 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0025/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-1792814267_1
2015-12-13 19:41:49,625 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0025/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742526_1703{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:41:49,648 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742526_1703{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:41:49,649 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0025/job.xml is closed by DFSClient_NONMAPREDUCE_-1792814267_1
2015-12-13 19:41:56,537 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0025/job_1450020873492_0025_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742527_1704{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:41:56,658 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742527_1704{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:41:56,664 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0025/job_1450020873492_0025_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1400787560_1
2015-12-13 19:42:05,947 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:42:05,947 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:42:10,019 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0025/job_1450020873492_0025_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742528_1705{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:42:10,393 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0025/job_1450020873492_0025_1.jhist for DFSClient_NONMAPREDUCE_1400787560_1
2015-12-13 19:42:35,948 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:42:36,019 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 72 millisecond(s).
2015-12-13 19:42:40,934 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742528_1705{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 13448
2015-12-13 19:42:40,957 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0025/job_1450020873492_0025_1.jhist is closed by DFSClient_NONMAPREDUCE_1400787560_1
2015-12-13 19:42:40,993 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0025.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742529_1706{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:42:41,005 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742529_1706{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:42:41,007 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0025.summary_tmp is closed by DFSClient_NONMAPREDUCE_1400787560_1
2015-12-13 19:42:41,078 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0025-1450053709917-parallels-task+C1-1450053760588-0-0-FAILED-default-1450053716103.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742530_1707{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:42:41,088 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742530_1707{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:42:41,091 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0025-1450053709917-parallels-task+C1-1450053760588-0-0-FAILED-default-1450053716103.jhist_tmp is closed by DFSClient_NONMAPREDUCE_1400787560_1
2015-12-13 19:42:41,125 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0025_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742531_1708{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:42:41,133 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742531_1708{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:42:41,136 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0025_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_1400787560_1
2015-12-13 19:42:42,255 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742523_1700 127.0.0.1:50010 
2015-12-13 19:42:42,255 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742524_1701 127.0.0.1:50010 
2015-12-13 19:42:42,255 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742525_1702 127.0.0.1:50010 
2015-12-13 19:42:42,255 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742526_1703 127.0.0.1:50010 
2015-12-13 19:42:42,255 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742528_1705 127.0.0.1:50010 
2015-12-13 19:42:42,255 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742527_1704 127.0.0.1:50010 
2015-12-13 19:42:44,430 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742528_1705, blk_1073742523_1700, blk_1073742524_1701, blk_1073742525_1702, blk_1073742526_1703, blk_1073742527_1704]
2015-12-13 19:43:05,948 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:43:05,949 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:43:35,949 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:43:35,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:43:56,659 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 85 Total time for transactions(ms): 16 Number of transactions batched in Syncs: 0 Number of syncs: 60 SyncTimes(ms): 428 
2015-12-13 19:44:00,855 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0026/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742532_1709{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:01,004 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742532_1709{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:01,008 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0026/job.jar is closed by DFSClient_NONMAPREDUCE_1295307344_1
2015-12-13 19:44:01,013 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0026/job.jar
2015-12-13 19:44:01,082 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0026/job.split
2015-12-13 19:44:01,098 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0026/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742533_1710{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:01,105 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742533_1710{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:01,107 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0026/job.split is closed by DFSClient_NONMAPREDUCE_1295307344_1
2015-12-13 19:44:01,120 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0026/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742534_1711{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:01,132 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742534_1711{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:01,134 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0026/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1295307344_1
2015-12-13 19:44:01,352 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0026/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742535_1712{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:01,363 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742535_1712{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:01,366 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0026/job.xml is closed by DFSClient_NONMAPREDUCE_1295307344_1
2015-12-13 19:44:05,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:44:05,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:44:06,608 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0026/job_1450020873492_0026_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742536_1713{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:06,710 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742536_1713{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:06,714 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0026/job_1450020873492_0026_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-200040914_1
2015-12-13 19:44:20,055 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0026/job_1450020873492_0026_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742537_1714{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:20,466 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0026/job_1450020873492_0026_1.jhist for DFSClient_NONMAPREDUCE_-200040914_1
2015-12-13 19:44:29,386 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0026_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742538_1715{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:29,556 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742538_1715{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:29,561 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0026_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0026_r_000000_0_1450381624_1
2015-12-13 19:44:30,814 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0026/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-200040914_1
2015-12-13 19:44:30,847 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-200040914_1
2015-12-13 19:44:30,853 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0026/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-200040914_1
2015-12-13 19:44:30,886 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742537_1714{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12217
2015-12-13 19:44:30,890 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0026/job_1450020873492_0026_1.jhist is closed by DFSClient_NONMAPREDUCE_-200040914_1
2015-12-13 19:44:30,899 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0026.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742539_1716{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:30,907 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742539_1716{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:30,909 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0026.summary_tmp is closed by DFSClient_NONMAPREDUCE_-200040914_1
2015-12-13 19:44:30,941 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0026-1450053841590-parallels-task+C1-1450053870854-3-1-SUCCEEDED-default-1450053846190.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742540_1717{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:30,950 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742540_1717{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:30,952 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0026-1450053841590-parallels-task+C1-1450053870854-3-1-SUCCEEDED-default-1450053846190.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-200040914_1
2015-12-13 19:44:30,981 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0026_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742541_1718{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:31,021 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742541_1718{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:31,026 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0026_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-200040914_1
2015-12-13 19:44:32,122 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742532_1709 127.0.0.1:50010 
2015-12-13 19:44:32,122 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742533_1710 127.0.0.1:50010 
2015-12-13 19:44:32,122 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742534_1711 127.0.0.1:50010 
2015-12-13 19:44:32,122 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742535_1712 127.0.0.1:50010 
2015-12-13 19:44:32,122 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742537_1714 127.0.0.1:50010 
2015-12-13 19:44:32,122 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742536_1713 127.0.0.1:50010 
2015-12-13 19:44:33,117 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742532_1709, blk_1073742533_1710, blk_1073742534_1711, blk_1073742535_1712, blk_1073742536_1713, blk_1073742537_1714]
2015-12-13 19:44:33,519 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0027/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742542_1719{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:33,561 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742542_1719{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:33,567 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0027/job.jar is closed by DFSClient_NONMAPREDUCE_1295307344_1
2015-12-13 19:44:33,570 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0027/job.jar
2015-12-13 19:44:33,597 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0027/job.split
2015-12-13 19:44:33,603 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0027/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742543_1720{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:33,612 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742543_1720{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:33,618 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0027/job.split is closed by DFSClient_NONMAPREDUCE_1295307344_1
2015-12-13 19:44:33,631 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0027/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742544_1721{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:33,645 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742544_1721{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:33,648 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0027/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1295307344_1
2015-12-13 19:44:33,720 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0027/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742545_1722{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:33,736 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742545_1722{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:33,739 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0027/job.xml is closed by DFSClient_NONMAPREDUCE_1295307344_1
2015-12-13 19:44:35,950 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:44:35,954 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 4 millisecond(s).
2015-12-13 19:44:42,460 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0027/job_1450020873492_0027_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742546_1723{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:42,585 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742546_1723{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:42,590 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0027/job_1450020873492_0027_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1401250137_1
2015-12-13 19:44:47,386 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0027/job_1450020873492_0027_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742547_1724{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:47,437 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0027/job_1450020873492_0027_1.jhist for DFSClient_NONMAPREDUCE_-1401250137_1
2015-12-13 19:44:53,118 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0027_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742548_1725{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:53,254 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742548_1725{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:53,264 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0027_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0027_r_000000_0_-1480492386_1
2015-12-13 19:44:53,555 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0027/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1401250137_1
2015-12-13 19:44:53,602 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1401250137_1
2015-12-13 19:44:53,613 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0027/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1401250137_1
2015-12-13 19:44:53,637 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742547_1724{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12978
2015-12-13 19:44:53,640 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0027/job_1450020873492_0027_1.jhist is closed by DFSClient_NONMAPREDUCE_-1401250137_1
2015-12-13 19:44:53,650 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0027.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742549_1726{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:53,656 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742549_1726{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:53,661 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0027.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1401250137_1
2015-12-13 19:44:53,694 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0027-1450053873770-parallels-task+C2-1450053893621-1-1-SUCCEEDED-default-1450053882126.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742550_1727{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:53,701 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742550_1727{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:53,706 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0027-1450053873770-parallels-task+C2-1450053893621-1-1-SUCCEEDED-default-1450053882126.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1401250137_1
2015-12-13 19:44:53,734 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0027_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742551_1728{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:44:53,741 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742551_1728{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:44:53,744 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0027_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1401250137_1
2015-12-13 19:44:54,781 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742542_1719 127.0.0.1:50010 
2015-12-13 19:44:54,782 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742543_1720 127.0.0.1:50010 
2015-12-13 19:44:54,782 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742544_1721 127.0.0.1:50010 
2015-12-13 19:44:54,782 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742545_1722 127.0.0.1:50010 
2015-12-13 19:44:54,782 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742547_1724 127.0.0.1:50010 
2015-12-13 19:44:54,782 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742546_1723 127.0.0.1:50010 
2015-12-13 19:44:57,122 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742544_1721, blk_1073742545_1722, blk_1073742546_1723, blk_1073742547_1724, blk_1073742542_1719, blk_1073742543_1720]
2015-12-13 19:45:05,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:45:05,953 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 19:45:35,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:45:35,952 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:46:05,952 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:46:05,953 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:46:35,953 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:46:35,954 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:47:05,954 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:47:05,954 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:47:35,956 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:47:35,956 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:48:05,956 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:48:05,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:48:35,957 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:48:35,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:49:06,027 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:49:06,027 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 70 millisecond(s).
2015-12-13 19:49:35,958 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:49:35,960 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 19:50:05,959 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:50:05,960 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 19:50:35,959 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:50:35,959 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:51:05,959 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:51:05,959 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:51:35,959 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:51:35,959 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:52:05,959 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:52:05,960 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:52:35,960 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:52:35,964 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:53:05,963 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-13 19:53:05,964 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:53:35,964 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:53:35,966 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 19:54:05,964 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:54:05,965 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:54:35,966 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 19:54:35,967 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:55:05,967 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:55:05,968 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:55:35,968 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:55:35,968 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:56:05,969 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:56:05,970 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 19:56:35,969 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:56:35,969 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:56:38,267 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 246 Total time for transactions(ms): 33 Number of transactions batched in Syncs: 0 Number of syncs: 173 SyncTimes(ms): 850 
2015-12-13 19:56:38,267 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742548_1725 127.0.0.1:50010 
2015-12-13 19:56:39,291 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742548_1725]
2015-12-13 19:56:40,381 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742538_1715 127.0.0.1:50010 
2015-12-13 19:56:42,291 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742538_1715]
2015-12-13 19:56:45,306 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0028/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742552_1729{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:56:45,427 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742552_1729{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:56:45,432 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0028/job.jar is closed by DFSClient_NONMAPREDUCE_2012034550_1
2015-12-13 19:56:45,439 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0028/job.jar
2015-12-13 19:56:45,521 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0028/job.split
2015-12-13 19:56:45,536 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0028/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742553_1730{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:56:45,553 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742553_1730{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:56:45,555 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0028/job.split is closed by DFSClient_NONMAPREDUCE_2012034550_1
2015-12-13 19:56:45,565 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0028/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742554_1731{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:56:45,571 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742554_1731{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:56:45,574 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0028/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_2012034550_1
2015-12-13 19:56:45,765 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0028/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742555_1732{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:56:45,777 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742555_1732{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:56:45,780 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0028/job.xml is closed by DFSClient_NONMAPREDUCE_2012034550_1
2015-12-13 19:56:52,814 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0028/job_1450020873492_0028_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742556_1733{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:56:52,918 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742556_1733{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:56:52,924 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0028/job_1450020873492_0028_1_conf.xml is closed by DFSClient_NONMAPREDUCE_215337525_1
2015-12-13 19:57:07,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 31209 milliseconds
2015-12-13 19:57:07,198 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2069ms
No GCs detected
2015-12-13 19:57:07,327 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 150 millisecond(s).
2015-12-13 19:57:10,352 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0028/job_1450020873492_0028_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742557_1734{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:57:10,568 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0028/job_1450020873492_0028_1.jhist for DFSClient_NONMAPREDUCE_215337525_1
2015-12-13 19:57:20,403 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0028_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742558_1735{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:57:20,703 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742558_1735{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:57:20,711 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0028_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0028_r_000000_0_-1994037348_1
2015-12-13 19:57:22,004 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0028/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_215337525_1
2015-12-13 19:57:22,058 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_215337525_1
2015-12-13 19:57:22,066 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0028/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_215337525_1
2015-12-13 19:57:22,105 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742557_1734{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12229
2015-12-13 19:57:22,107 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0028/job_1450020873492_0028_1.jhist is closed by DFSClient_NONMAPREDUCE_215337525_1
2015-12-13 19:57:22,119 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0028.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742559_1736{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:57:22,129 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742559_1736{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:57:22,133 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0028.summary_tmp is closed by DFSClient_NONMAPREDUCE_215337525_1
2015-12-13 19:57:22,199 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0028-1450054606102-parallels-task+C1-1450054642069-3-1-SUCCEEDED-default-1450054612373.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742560_1737{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:57:22,208 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742560_1737{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:57:22,215 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0028-1450054606102-parallels-task+C1-1450054642069-3-1-SUCCEEDED-default-1450054612373.jhist_tmp is closed by DFSClient_NONMAPREDUCE_215337525_1
2015-12-13 19:57:22,248 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0028_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742561_1738{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:57:22,256 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742561_1738{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:57:22,259 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0028_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_215337525_1
2015-12-13 19:57:23,307 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742552_1729 127.0.0.1:50010 
2015-12-13 19:57:23,307 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742553_1730 127.0.0.1:50010 
2015-12-13 19:57:23,307 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742554_1731 127.0.0.1:50010 
2015-12-13 19:57:23,307 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742555_1732 127.0.0.1:50010 
2015-12-13 19:57:23,307 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742557_1734 127.0.0.1:50010 
2015-12-13 19:57:23,307 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742556_1733 127.0.0.1:50010 
2015-12-13 19:57:24,161 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0029/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742562_1739{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:57:24,205 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742562_1739{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:57:24,208 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0029/job.jar is closed by DFSClient_NONMAPREDUCE_2012034550_1
2015-12-13 19:57:24,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0029/job.jar
2015-12-13 19:57:24,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0029/job.split
2015-12-13 19:57:24,256 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0029/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742563_1740{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:57:24,274 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742563_1740{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:57:24,278 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0029/job.split is closed by DFSClient_NONMAPREDUCE_2012034550_1
2015-12-13 19:57:24,295 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0029/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742564_1741{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:57:24,308 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742564_1741{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:57:24,407 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0029/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_2012034550_1
2015-12-13 19:57:24,493 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0029/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742565_1742{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:57:24,503 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742565_1742{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:57:24,511 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0029/job.xml is closed by DFSClient_NONMAPREDUCE_2012034550_1
2015-12-13 19:57:25,344 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742552_1729, blk_1073742553_1730, blk_1073742554_1731, blk_1073742555_1732, blk_1073742556_1733, blk_1073742557_1734]
2015-12-13 19:57:34,844 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0029/job_1450020873492_0029_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742566_1743{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:57:34,986 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742566_1743{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:57:34,990 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0029/job_1450020873492_0029_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1807072964_1
2015-12-13 19:57:37,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:57:37,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 19:57:39,902 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0029/job_1450020873492_0029_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742567_1744{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:57:39,903 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 367 Total time for transactions(ms): 49 Number of transactions batched in Syncs: 0 Number of syncs: 256 SyncTimes(ms): 1025 
2015-12-13 19:57:40,003 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0029/job_1450020873492_0029_1.jhist for DFSClient_NONMAPREDUCE_1807072964_1
2015-12-13 19:57:46,841 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0029_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742568_1745{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:57:47,001 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742568_1745{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:57:47,007 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0029_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0029_r_000000_0_1638786630_1
2015-12-13 19:57:47,340 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0029/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_1807072964_1
2015-12-13 19:57:47,390 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_SUCCESS is closed by DFSClient_NONMAPREDUCE_1807072964_1
2015-12-13 19:57:47,402 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0029/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_1807072964_1
2015-12-13 19:57:47,419 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742567_1744{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12977
2015-12-13 19:57:47,421 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0029/job_1450020873492_0029_1.jhist is closed by DFSClient_NONMAPREDUCE_1807072964_1
2015-12-13 19:57:47,430 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0029.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742569_1746{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:57:47,436 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742569_1746{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:57:47,443 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0029.summary_tmp is closed by DFSClient_NONMAPREDUCE_1807072964_1
2015-12-13 19:57:47,476 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0029-1450054644590-parallels-task+C2-1450054667403-1-1-SUCCEEDED-default-1450054654431.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742570_1747{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:57:47,482 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742570_1747{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:57:47,485 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0029-1450054644590-parallels-task+C2-1450054667403-1-1-SUCCEEDED-default-1450054654431.jhist_tmp is closed by DFSClient_NONMAPREDUCE_1807072964_1
2015-12-13 19:57:47,506 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0029_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742571_1748{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 19:57:47,516 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742571_1748{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 19:57:47,518 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0029_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_1807072964_1
2015-12-13 19:57:48,551 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742562_1739 127.0.0.1:50010 
2015-12-13 19:57:48,551 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742563_1740 127.0.0.1:50010 
2015-12-13 19:57:48,551 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742564_1741 127.0.0.1:50010 
2015-12-13 19:57:48,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742565_1742 127.0.0.1:50010 
2015-12-13 19:57:48,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742567_1744 127.0.0.1:50010 
2015-12-13 19:57:48,552 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742566_1743 127.0.0.1:50010 
2015-12-13 19:57:49,349 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742562_1739, blk_1073742563_1740, blk_1073742564_1741, blk_1073742565_1742, blk_1073742566_1743, blk_1073742567_1744]
2015-12-13 19:58:07,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:58:07,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:58:37,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:58:37,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:59:07,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 19:59:07,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 19:59:37,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 19:59:37,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:00:07,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:00:07,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:00:37,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:00:37,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:01:07,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:01:07,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 4 millisecond(s).
2015-12-13 20:01:37,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:01:37,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:02:07,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:02:07,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:02:37,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:02:37,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:03:07,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:03:07,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:03:37,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:03:37,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:04:07,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:04:07,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:04:37,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:04:37,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:05:07,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:05:07,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:05:37,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:05:37,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:06:07,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:06:07,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:06:37,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:06:37,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:07:07,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:07:07,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 20:07:37,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:07:37,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:08:07,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:08:07,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:08:37,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:08:37,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:09:07,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:09:07,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:09:37,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:09:37,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:10:06,668 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 408 Total time for transactions(ms): 53 Number of transactions batched in Syncs: 0 Number of syncs: 287 SyncTimes(ms): 1127 
2015-12-13 20:10:06,669 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742568_1745 127.0.0.1:50010 
2015-12-13 20:10:07,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:10:07,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:10:07,545 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742568_1745]
2015-12-13 20:10:08,794 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742558_1735 127.0.0.1:50010 
2015-12-13 20:10:10,545 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742558_1735]
2015-12-13 20:10:13,274 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0030/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742572_1749{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:10:13,415 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742572_1749{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:10:13,419 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0030/job.jar is closed by DFSClient_NONMAPREDUCE_1705163570_1
2015-12-13 20:10:13,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0030/job.jar
2015-12-13 20:10:13,502 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0030/job.split
2015-12-13 20:10:13,512 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0030/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742573_1750{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:10:13,523 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742573_1750{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:10:13,527 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0030/job.split is closed by DFSClient_NONMAPREDUCE_1705163570_1
2015-12-13 20:10:13,541 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0030/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742574_1751{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:10:13,548 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742574_1751{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:10:13,550 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0030/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1705163570_1
2015-12-13 20:10:13,782 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0030/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742575_1752{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:10:13,800 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742575_1752{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:10:13,802 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0030/job.xml is closed by DFSClient_NONMAPREDUCE_1705163570_1
2015-12-13 20:10:19,380 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0030/job_1450020873492_0030_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742576_1753{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:10:19,504 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742576_1753{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:10:19,511 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0030/job_1450020873492_0030_1_conf.xml is closed by DFSClient_NONMAPREDUCE_2039051372_1
2015-12-13 20:10:34,076 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0030/job_1450020873492_0030_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742577_1754{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:10:34,315 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0030/job_1450020873492_0030_1.jhist for DFSClient_NONMAPREDUCE_2039051372_1
2015-12-13 20:10:37,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30007 milliseconds
2015-12-13 20:10:37,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 43 millisecond(s).
2015-12-13 20:10:43,959 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0030_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742578_1755{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:10:44,206 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742578_1755{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:10:44,221 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0030_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0030_r_000000_0_-792959229_1
2015-12-13 20:10:45,495 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0030/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_2039051372_1
2015-12-13 20:10:45,546 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_2039051372_1
2015-12-13 20:10:45,553 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0030/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_2039051372_1
2015-12-13 20:10:45,595 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742577_1754{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12229
2015-12-13 20:10:45,598 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0030/job_1450020873492_0030_1.jhist is closed by DFSClient_NONMAPREDUCE_2039051372_1
2015-12-13 20:10:45,610 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0030.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742579_1756{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:10:45,619 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742579_1756{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:10:45,621 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0030.summary_tmp is closed by DFSClient_NONMAPREDUCE_2039051372_1
2015-12-13 20:10:45,696 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0030-1450055414052-parallels-task+C1-1450055445555-3-1-SUCCEEDED-default-1450055418958.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742580_1757{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:10:45,726 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742580_1757{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:10:45,729 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0030-1450055414052-parallels-task+C1-1450055445555-3-1-SUCCEEDED-default-1450055418958.jhist_tmp is closed by DFSClient_NONMAPREDUCE_2039051372_1
2015-12-13 20:10:45,768 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0030_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742581_1758{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:10:45,780 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742581_1758{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:10:45,785 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0030_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_2039051372_1
2015-12-13 20:10:46,886 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742572_1749 127.0.0.1:50010 
2015-12-13 20:10:46,887 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742573_1750 127.0.0.1:50010 
2015-12-13 20:10:46,887 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742574_1751 127.0.0.1:50010 
2015-12-13 20:10:46,887 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742575_1752 127.0.0.1:50010 
2015-12-13 20:10:46,888 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742577_1754 127.0.0.1:50010 
2015-12-13 20:10:46,888 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742576_1753 127.0.0.1:50010 
2015-12-13 20:10:47,994 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0031/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742582_1759{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:10:48,031 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742582_1759{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:10:48,034 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0031/job.jar is closed by DFSClient_NONMAPREDUCE_1705163570_1
2015-12-13 20:10:48,042 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0031/job.jar
2015-12-13 20:10:48,073 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0031/job.split
2015-12-13 20:10:48,078 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0031/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742583_1760{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:10:48,097 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742583_1760{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:10:48,102 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0031/job.split is closed by DFSClient_NONMAPREDUCE_1705163570_1
2015-12-13 20:10:48,116 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0031/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742584_1761{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:10:48,127 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742584_1761{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:10:48,131 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0031/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1705163570_1
2015-12-13 20:10:48,190 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0031/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742585_1762{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:10:48,206 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742585_1762{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:10:48,209 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0031/job.xml is closed by DFSClient_NONMAPREDUCE_1705163570_1
2015-12-13 20:10:49,588 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742576_1753, blk_1073742577_1754, blk_1073742572_1749, blk_1073742573_1750, blk_1073742574_1751, blk_1073742575_1752]
2015-12-13 20:10:58,561 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0031/job_1450020873492_0031_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742586_1763{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:10:58,694 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742586_1763{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:10:58,699 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0031/job_1450020873492_0031_1_conf.xml is closed by DFSClient_NONMAPREDUCE_65559680_1
2015-12-13 20:11:03,995 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0031/job_1450020873492_0031_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742587_1764{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:11:04,011 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0031/job_1450020873492_0031_1.jhist for DFSClient_NONMAPREDUCE_65559680_1
2015-12-13 20:11:07,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:11:07,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:11:10,331 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 533 Total time for transactions(ms): 83 Number of transactions batched in Syncs: 0 Number of syncs: 372 SyncTimes(ms): 1312 
2015-12-13 20:11:10,427 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0031_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742588_1765{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:11:10,610 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742588_1765{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:11:10,622 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0031_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0031_r_000000_0_826764021_1
2015-12-13 20:11:10,845 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0031/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_65559680_1
2015-12-13 20:11:10,899 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_SUCCESS is closed by DFSClient_NONMAPREDUCE_65559680_1
2015-12-13 20:11:10,906 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0031/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_65559680_1
2015-12-13 20:11:10,926 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742587_1764{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12978
2015-12-13 20:11:10,930 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0031/job_1450020873492_0031_1.jhist is closed by DFSClient_NONMAPREDUCE_65559680_1
2015-12-13 20:11:10,943 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0031.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742589_1766{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:11:10,949 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742589_1766{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:11:10,952 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0031.summary_tmp is closed by DFSClient_NONMAPREDUCE_65559680_1
2015-12-13 20:11:10,981 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0031-1450055448275-parallels-task+C2-1450055470911-1-1-SUCCEEDED-default-1450055458179.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742590_1767{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:11:10,987 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742590_1767{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:11:10,991 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0031-1450055448275-parallels-task+C2-1450055470911-1-1-SUCCEEDED-default-1450055458179.jhist_tmp is closed by DFSClient_NONMAPREDUCE_65559680_1
2015-12-13 20:11:11,013 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0031_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742591_1768{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:11:11,021 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742591_1768{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:11:11,024 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0031_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_65559680_1
2015-12-13 20:11:12,060 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742582_1759 127.0.0.1:50010 
2015-12-13 20:11:12,060 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742583_1760 127.0.0.1:50010 
2015-12-13 20:11:12,060 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742584_1761 127.0.0.1:50010 
2015-12-13 20:11:12,060 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742585_1762 127.0.0.1:50010 
2015-12-13 20:11:12,060 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742587_1764 127.0.0.1:50010 
2015-12-13 20:11:12,060 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742586_1763 127.0.0.1:50010 
2015-12-13 20:11:13,602 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742582_1759, blk_1073742583_1760, blk_1073742584_1761, blk_1073742585_1762, blk_1073742586_1763, blk_1073742587_1764]
2015-12-13 20:11:37,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:11:37,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:12:07,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 20:12:07,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:12:37,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:12:37,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:13:07,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:13:07,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:13:37,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:13:37,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:14:07,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:14:07,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:14:37,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:14:37,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:15:07,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:15:07,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:15:37,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:15:37,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:15:44,410 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 570 Total time for transactions(ms): 86 Number of transactions batched in Syncs: 0 Number of syncs: 401 SyncTimes(ms): 1332 
2015-12-13 20:15:44,411 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742588_1765 127.0.0.1:50010 
2015-12-13 20:15:46,492 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742578_1755 127.0.0.1:50010 
2015-12-13 20:15:46,698 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742578_1755, blk_1073742588_1765]
2015-12-13 20:15:50,710 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0032/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742592_1769{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:15:50,844 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742592_1769{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:15:50,849 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0032/job.jar is closed by DFSClient_NONMAPREDUCE_-766214267_1
2015-12-13 20:15:50,854 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0032/job.jar
2015-12-13 20:15:50,922 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0032/job.split
2015-12-13 20:15:50,937 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0032/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742593_1770{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:15:50,944 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742593_1770{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:15:50,946 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0032/job.split is closed by DFSClient_NONMAPREDUCE_-766214267_1
2015-12-13 20:15:50,963 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0032/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742594_1771{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:15:50,976 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742594_1771{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:15:50,978 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0032/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-766214267_1
2015-12-13 20:15:51,163 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0032/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742595_1772{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:15:51,184 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742595_1772{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:15:51,190 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0032/job.xml is closed by DFSClient_NONMAPREDUCE_-766214267_1
2015-12-13 20:15:57,110 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0032/job_1450020873492_0032_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742596_1773{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:15:57,250 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742596_1773{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:15:57,264 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0032/job_1450020873492_0032_1_conf.xml is closed by DFSClient_NONMAPREDUCE_931145019_1
2015-12-13 20:16:07,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30014 milliseconds
2015-12-13 20:16:07,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 19 millisecond(s).
2015-12-13 20:16:11,667 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0032/job_1450020873492_0032_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742597_1774{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:16:11,972 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0032/job_1450020873492_0032_1.jhist for DFSClient_NONMAPREDUCE_931145019_1
2015-12-13 20:16:21,403 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0032_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742598_1775{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:16:21,601 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742598_1775{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:16:21,610 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450020873492_0032_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0032_r_000000_0_-242327616_1
2015-12-13 20:16:22,920 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0032/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_931145019_1
2015-12-13 20:16:22,973 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_931145019_1
2015-12-13 20:16:22,981 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0032/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_931145019_1
2015-12-13 20:16:23,017 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742597_1774{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14344
2015-12-13 20:16:23,021 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0032/job_1450020873492_0032_1.jhist is closed by DFSClient_NONMAPREDUCE_931145019_1
2015-12-13 20:16:23,035 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0032.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742599_1776{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:16:23,041 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742599_1776{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:16:23,045 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0032.summary_tmp is closed by DFSClient_NONMAPREDUCE_931145019_1
2015-12-13 20:16:23,093 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0032-1450055751544-parallels-task+C1-1450055782983-3-1-SUCCEEDED-default-1450055756667.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742600_1777{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:16:23,106 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742600_1777{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:16:23,111 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0032-1450055751544-parallels-task+C1-1450055782983-3-1-SUCCEEDED-default-1450055756667.jhist_tmp is closed by DFSClient_NONMAPREDUCE_931145019_1
2015-12-13 20:16:23,165 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0032_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742601_1778{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:16:23,198 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742601_1778{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:16:23,200 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0032_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_931145019_1
2015-12-13 20:16:24,382 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742592_1769 127.0.0.1:50010 
2015-12-13 20:16:24,382 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742593_1770 127.0.0.1:50010 
2015-12-13 20:16:24,383 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742594_1771 127.0.0.1:50010 
2015-12-13 20:16:24,383 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742595_1772 127.0.0.1:50010 
2015-12-13 20:16:24,383 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742597_1774 127.0.0.1:50010 
2015-12-13 20:16:24,383 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742596_1773 127.0.0.1:50010 
2015-12-13 20:16:25,353 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0033/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742602_1779{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:16:25,381 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742602_1779{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:16:25,385 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0033/job.jar is closed by DFSClient_NONMAPREDUCE_-766214267_1
2015-12-13 20:16:25,393 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0033/job.jar
2015-12-13 20:16:25,424 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0033/job.split
2015-12-13 20:16:25,429 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0033/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742603_1780{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:16:25,450 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742603_1780{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:16:25,454 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0033/job.split is closed by DFSClient_NONMAPREDUCE_-766214267_1
2015-12-13 20:16:25,468 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0033/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742604_1781{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:16:25,475 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742604_1781{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:16:25,477 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0033/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-766214267_1
2015-12-13 20:16:25,531 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0033/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742605_1782{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:16:25,545 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742605_1782{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:16:25,550 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0033/job.xml is closed by DFSClient_NONMAPREDUCE_-766214267_1
2015-12-13 20:16:25,709 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742592_1769, blk_1073742593_1770, blk_1073742594_1771, blk_1073742595_1772, blk_1073742596_1773, blk_1073742597_1774]
2015-12-13 20:16:35,331 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0033/job_1450020873492_0033_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742606_1783{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:16:35,433 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742606_1783{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:16:35,439 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0033/job_1450020873492_0033_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1869567362_1
2015-12-13 20:16:37,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 20:16:37,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:16:41,375 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0033/job_1450020873492_0033_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742607_1784{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:16:41,572 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0033/job_1450020873492_0033_1.jhist for DFSClient_NONMAPREDUCE_-1869567362_1
2015-12-13 20:16:48,314 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 695 Total time for transactions(ms): 100 Number of transactions batched in Syncs: 0 Number of syncs: 486 SyncTimes(ms): 1937 
2015-12-13 20:16:48,394 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0033_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742608_1785{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:16:48,570 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742608_1785{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:16:48,572 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450020873492_0033_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450020873492_0033_r_000000_0_-121312067_1
2015-12-13 20:16:48,842 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0033/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1869567362_1
2015-12-13 20:16:48,879 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1869567362_1
2015-12-13 20:16:48,885 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0033/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1869567362_1
2015-12-13 20:16:48,906 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742607_1784{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12977
2015-12-13 20:16:48,910 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450020873492_0033/job_1450020873492_0033_1.jhist is closed by DFSClient_NONMAPREDUCE_-1869567362_1
2015-12-13 20:16:48,920 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0033.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742609_1786{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:16:48,926 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742609_1786{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:16:48,929 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0033.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1869567362_1
2015-12-13 20:16:48,958 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0033-1450055785631-parallels-task+C2-1450055808887-1-1-SUCCEEDED-default-1450055794968.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742610_1787{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:16:48,965 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742610_1787{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:16:48,969 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0033-1450055785631-parallels-task+C2-1450055808887-1-1-SUCCEEDED-default-1450055794968.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1869567362_1
2015-12-13 20:16:48,992 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0033_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742611_1788{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-13 20:16:48,999 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742611_1788{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-13 20:16:49,003 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450020873492_0033_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1869567362_1
2015-12-13 20:16:50,039 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742602_1779 127.0.0.1:50010 
2015-12-13 20:16:50,039 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742603_1780 127.0.0.1:50010 
2015-12-13 20:16:50,040 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742604_1781 127.0.0.1:50010 
2015-12-13 20:16:50,040 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742605_1782 127.0.0.1:50010 
2015-12-13 20:16:50,040 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742607_1784 127.0.0.1:50010 
2015-12-13 20:16:50,040 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742606_1783 127.0.0.1:50010 
2015-12-13 20:16:52,718 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742602_1779, blk_1073742603_1780, blk_1073742604_1781, blk_1073742605_1782, blk_1073742606_1783, blk_1073742607_1784]
2015-12-13 20:17:07,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:17:07,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:17:37,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:17:37,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 20:18:07,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:18:07,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:18:37,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:18:37,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:19:07,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:19:07,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:19:37,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:19:37,221 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:20:07,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:20:07,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-13 20:20:37,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:20:37,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:21:07,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:21:07,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:21:37,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:21:37,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:22:07,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:22:07,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:22:33,219 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-13 20:22:33,219 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-13 20:22:33,219 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 5656
2015-12-13 20:22:33,219 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 732 Total time for transactions(ms): 103 Number of transactions batched in Syncs: 0 Number of syncs: 515 SyncTimes(ms): 1957 
2015-12-13 20:22:33,219 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 732 Total time for transactions(ms): 103 Number of transactions batched in Syncs: 0 Number of syncs: 516 SyncTimes(ms): 1957 
2015-12-13 20:22:33,221 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000005656 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000005656-0000000000000006387
2015-12-13 20:22:33,223 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 6388
2015-12-13 20:22:33,764 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 10333.33 KB/s
2015-12-13 20:22:33,764 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000006387 size 32028 bytes.
2015-12-13 20:22:33,770 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 5655
2015-12-13 20:22:33,771 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000005489, cpktTxId=0000000000000005489)
2015-12-13 20:22:37,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:22:37,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:23:07,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:23:07,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:23:37,223 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:23:37,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:24:07,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:24:07,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:24:37,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:24:37,226 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:25:07,226 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:25:07,227 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:25:37,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:25:37,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:26:07,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:26:07,228 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:26:37,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:26:37,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:27:07,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:27:07,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:27:37,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:27:37,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:28:07,230 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:28:07,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:28:37,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 20:28:37,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:29:07,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:29:07,233 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:29:37,234 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 20:29:37,235 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:30:07,236 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 20:30:07,237 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:30:37,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-13 20:30:37,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:31:07,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:31:07,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:31:37,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:31:37,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:32:07,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:32:07,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:32:37,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:32:37,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:33:07,241 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:33:07,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:33:37,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:33:37,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:34:07,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:34:07,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 9 millisecond(s).
2015-12-13 20:34:37,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:34:37,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:35:07,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:35:07,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:35:37,244 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:35:37,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:36:07,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:36:07,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-13 20:36:37,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:36:37,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:37:07,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:37:07,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:37:37,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-13 20:37:37,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:38:07,246 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-13 20:38:07,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-13 20:38:33,748 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-13 20:38:33,822 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-15 01:11:08,055 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-15 01:11:08,071 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-15 01:11:08,074 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-15 01:11:08,429 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-15 01:11:08,580 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-15 01:11:08,580 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-15 01:11:08,582 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-15 01:11:08,582 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-15 01:11:08,993 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-15 01:11:09,068 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-15 01:11:09,076 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-15 01:11:09,088 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-15 01:11:09,091 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-15 01:11:09,091 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-15 01:11:09,091 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-15 01:11:09,151 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-15 01:11:09,154 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-15 01:11:09,209 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-15 01:11:09,209 INFO org.mortbay.log: jetty-6.1.26
2015-12-15 01:11:09,529 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-15 01:11:09,607 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-15 01:11:09,607 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-15 01:11:09,654 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-15 01:11:09,703 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-15 01:11:09,769 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-15 01:11:09,769 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-15 01:11:09,777 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-15 01:11:09,781 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 15 01:11:09
2015-12-15 01:11:09,784 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-15 01:11:09,784 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-15 01:11:09,787 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-15 01:11:09,788 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-15 01:11:09,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-15 01:11:09,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-15 01:11:09,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-15 01:11:09,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-15 01:11:09,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-15 01:11:09,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-15 01:11:09,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-15 01:11:09,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-15 01:11:09,843 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-15 01:11:09,855 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-15 01:11:09,855 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-15 01:11:09,855 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-15 01:11:09,855 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-15 01:11:09,857 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-15 01:11:10,094 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-15 01:11:10,094 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-15 01:11:10,094 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-15 01:11:10,094 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-15 01:11:10,096 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-15 01:11:10,112 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-15 01:11:10,112 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-15 01:11:10,112 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-15 01:11:10,112 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-15 01:11:10,114 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-15 01:11:10,114 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-15 01:11:10,114 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-15 01:11:10,115 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-15 01:11:10,115 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-15 01:11:10,118 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-15 01:11:10,118 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-15 01:11:10,118 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-15 01:11:10,118 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-15 01:11:10,122 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-15 01:11:10,122 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-15 01:11:10,122 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-15 01:11:10,151 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/name/in_use.lock acquired by nodename 3584@ubuntu
2015-12-15 01:11:10,310 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/parallels/hadoop/tmp/dfs/name/current
2015-12-15 01:11:10,367 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000006388 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000006388-0000000000000006388
2015-12-15 01:11:10,439 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 288 INodes.
2015-12-15 01:11:10,542 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-15 01:11:10,542 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 6387 from /home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000006387
2015-12-15 01:11:10,543 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1ebd94c6 expecting start txid #6388
2015-12-15 01:11:10,543 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000006388-0000000000000006388
2015-12-15 01:11:10,547 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000006388-0000000000000006388' to transaction ID 6388
2015-12-15 01:11:10,549 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000006388-0000000000000006388 of size 1048576 edits # 1 loaded in 0 seconds
2015-12-15 01:11:10,553 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2015-12-15 01:11:10,553 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2015-12-15 01:11:10,640 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 6387
2015-12-15 01:11:10,640 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000005655, cpktTxId=0000000000000005655)
2015-12-15 01:11:10,661 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 6389
2015-12-15 01:11:10,706 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-15 01:11:10,707 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 585 msecs
2015-12-15 01:11:11,038 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-15 01:11:11,046 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-15 01:11:11,059 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-15 01:11:11,101 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-15 01:11:11,118 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-15 01:11:11,118 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-15 01:11:11,119 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 259 blocks to reach the threshold 0.9990 of total blocks 259.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2015-12-15 01:11:11,154 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-15 01:11:11,154 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-15 01:11:11,161 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-15 01:11:11,161 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-15 01:11:11,167 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-15 01:11:11,167 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 10717108 milliseconds
2015-12-15 01:11:11,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-15 01:11:16,081 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0) storage 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-15 01:11:16,088 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-15 01:11:16,089 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-15 01:11:16,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-15 01:11:16,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-75e73aff-3510-410a-91be-881360516f94 for DN 127.0.0.1:50010
2015-12-15 01:11:16,424 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 258 has reached the threshold 0.9990 of total blocks 259. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2015-12-15 01:11:16,424 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-15 01:11:16,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-75e73aff-3510-410a-91be-881360516f94,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-12-15 01:11:16,426 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 259, hasStaleStorages: false, processing time: 25 msecs
2015-12-15 01:11:16,430 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 259
2015-12-15 01:11:16,431 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-15 01:11:16,431 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 7
2015-12-15 01:11:16,431 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-15 01:11:16,431 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-15 01:11:16,431 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 5 msec
2015-12-15 01:11:36,442 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 259 has reached the threshold 0.9990 of total blocks 259. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2015-12-15 01:11:41,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:11:41,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:11:44,517 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.delete from 127.0.0.1:40964 Call#3 Retry#0: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/parallels/chessdata/output_c. Name node is in safe mode.
The reported blocks 259 has reached the threshold 0.9990 of total blocks 259. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 1 seconds.
2015-12-15 01:11:46,446 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 36 secs
2015-12-15 01:11:46,447 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2015-12-15 01:11:46,447 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2015-12-15 01:11:46,447 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 7 blocks
2015-12-15 01:11:47,584 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742598_1775 127.0.0.1:50010 
2015-12-15 01:11:50,134 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742598_1775]
2015-12-15 01:12:11,167 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:12:11,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:12:22,587 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-15 01:12:22,587 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-15 01:12:22,587 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 6389
2015-12-15 01:12:22,587 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 27 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 52 
2015-12-15 01:12:22,588 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 27 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 53 
2015-12-15 01:12:22,590 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000006389 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000006389-0000000000000006391
2015-12-15 01:12:22,592 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 6392
2015-12-15 01:12:23,883 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 7750.00 KB/s
2015-12-15 01:12:23,883 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000006391 size 31824 bytes.
2015-12-15 01:12:23,886 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 6388
2015-12-15 01:12:23,886 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000006387, cpktTxId=0000000000000006387)
2015-12-15 01:12:41,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:12:41,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:13:11,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:13:11,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:13:28,643 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 10 
2015-12-15 01:13:28,646 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742608_1785 127.0.0.1:50010 
2015-12-15 01:13:29,157 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742608_1785]
2015-12-15 01:13:41,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:13:41,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-15 01:14:04,951 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450159889622_0001/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742612_1789{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:14:05,362 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742612_1789{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-15 01:14:05,396 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742612_1789{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 7781
2015-12-15 01:14:05,771 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450159889622_0001/job.jar is closed by DFSClient_NONMAPREDUCE_389115373_1
2015-12-15 01:14:05,783 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450159889622_0001/job.jar
2015-12-15 01:14:05,895 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450159889622_0001/job.split
2015-12-15 01:14:05,913 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450159889622_0001/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742613_1790{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:14:05,924 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742613_1790{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:14:05,930 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450159889622_0001/job.split is closed by DFSClient_NONMAPREDUCE_389115373_1
2015-12-15 01:14:05,946 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450159889622_0001/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742614_1791{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:14:05,954 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742614_1791{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:14:05,956 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450159889622_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_389115373_1
2015-12-15 01:14:06,236 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450159889622_0001/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742615_1792{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:14:06,259 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742615_1792{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:14:06,261 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450159889622_0001/job.xml is closed by DFSClient_NONMAPREDUCE_389115373_1
2015-12-15 01:14:11,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:14:11,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:14:17,179 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450159889622_0001/job_1450159889622_0001_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742616_1793{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:14:17,305 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742616_1793{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:14:17,310 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450159889622_0001/job_1450159889622_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_1903893461_1
2015-12-15 01:14:31,041 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450159889622_0001/job_1450159889622_0001_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742617_1794{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:14:31,044 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 44 Total time for transactions(ms): 42 Number of transactions batched in Syncs: 1 Number of syncs: 28 SyncTimes(ms): 64 
2015-12-15 01:14:31,437 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450159889622_0001/job_1450159889622_0001_1.jhist for DFSClient_NONMAPREDUCE_1903893461_1
2015-12-15 01:14:41,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-15 01:14:41,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 13 millisecond(s).
2015-12-15 01:14:41,872 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450159889622_0001_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742618_1795{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:14:42,184 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742618_1795{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:14:42,194 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450159889622_0001_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450159889622_0001_r_000000_0_1200234909_1
2015-12-15 01:14:43,767 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450159889622_0001/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_1903893461_1
2015-12-15 01:14:43,845 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_1903893461_1
2015-12-15 01:14:43,855 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450159889622_0001/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_1903893461_1
2015-12-15 01:14:43,902 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742617_1794{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14339
2015-12-15 01:14:43,906 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450159889622_0001/job_1450159889622_0001_1.jhist is closed by DFSClient_NONMAPREDUCE_1903893461_1
2015-12-15 01:14:43,934 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450159889622_0001.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742619_1796{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:14:43,948 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742619_1796{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:14:43,952 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450159889622_0001.summary_tmp is closed by DFSClient_NONMAPREDUCE_1903893461_1
2015-12-15 01:14:44,041 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450159889622_0001-1450160046571-parallels-task+C1-1450160083857-3-1-SUCCEEDED-default-1450160056734.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742620_1797{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:14:44,060 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742620_1797{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:14:44,067 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450159889622_0001-1450160046571-parallels-task+C1-1450160083857-3-1-SUCCEEDED-default-1450160056734.jhist_tmp is closed by DFSClient_NONMAPREDUCE_1903893461_1
2015-12-15 01:14:44,099 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450159889622_0001_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742621_1798{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:14:44,115 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742621_1798{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:14:44,119 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450159889622_0001_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_1903893461_1
2015-12-15 01:14:45,270 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742612_1789 127.0.0.1:50010 
2015-12-15 01:14:45,271 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742613_1790 127.0.0.1:50010 
2015-12-15 01:14:45,271 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742614_1791 127.0.0.1:50010 
2015-12-15 01:14:45,271 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742615_1792 127.0.0.1:50010 
2015-12-15 01:14:45,271 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742617_1794 127.0.0.1:50010 
2015-12-15 01:14:45,271 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742616_1793 127.0.0.1:50010 
2015-12-15 01:14:47,289 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742612_1789, blk_1073742613_1790, blk_1073742614_1791, blk_1073742615_1792, blk_1073742616_1793, blk_1073742617_1794]
2015-12-15 01:14:56,251 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: there are no corrupt file blocks.
2015-12-15 01:15:11,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:15:11,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:15:36,162 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-15 01:15:36,164 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
2015-12-15 01:16:12,715 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-15 01:16:12,722 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-15 01:16:12,725 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-12-15 01:16:13,052 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-15 01:16:13,129 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-15 01:16:13,129 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-12-15 01:16:13,131 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-12-15 01:16:13,131 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-12-15 01:16:13,483 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-12-15 01:16:13,554 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-15 01:16:13,560 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-12-15 01:16:13,573 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-15 01:16:13,575 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-12-15 01:16:13,575 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-15 01:16:13,575 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-15 01:16:13,618 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-12-15 01:16:13,622 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-15 01:16:13,655 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-12-15 01:16:13,655 INFO org.mortbay.log: jetty-6.1.26
2015-12-15 01:16:13,966 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-12-15 01:16:14,047 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-15 01:16:14,048 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-12-15 01:16:14,099 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-12-15 01:16:14,112 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-12-15 01:16:14,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-12-15 01:16:14,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-12-15 01:16:14,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-12-15 01:16:14,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Dec 15 01:16:14
2015-12-15 01:16:14,174 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-12-15 01:16:14,174 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-15 01:16:14,177 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-12-15 01:16:14,177 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-12-15 01:16:14,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-12-15 01:16:14,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-12-15 01:16:14,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-12-15 01:16:14,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-12-15 01:16:14,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-12-15 01:16:14,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-12-15 01:16:14,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-12-15 01:16:14,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-12-15 01:16:14,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-12-15 01:16:14,218 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = parallels (auth:SIMPLE)
2015-12-15 01:16:14,218 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-12-15 01:16:14,218 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-12-15 01:16:14,218 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-12-15 01:16:14,219 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-12-15 01:16:14,387 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-12-15 01:16:14,388 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-15 01:16:14,388 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-12-15 01:16:14,388 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-12-15 01:16:14,390 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-12-15 01:16:14,403 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-12-15 01:16:14,403 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-15 01:16:14,403 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-12-15 01:16:14,403 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2015-12-15 01:16:14,404 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-12-15 01:16:14,404 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-12-15 01:16:14,404 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-12-15 01:16:14,405 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-12-15 01:16:14,405 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-12-15 01:16:14,407 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-12-15 01:16:14,407 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-15 01:16:14,407 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-12-15 01:16:14,407 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2015-12-15 01:16:14,412 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-12-15 01:16:14,412 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-12-15 01:16:14,412 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-12-15 01:16:14,421 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/name/in_use.lock acquired by nodename 7081@ubuntu
2015-12-15 01:16:14,535 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /home/parallels/hadoop/tmp/dfs/name/current
2015-12-15 01:16:14,646 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000006392 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000006392-0000000000000006476
2015-12-15 01:16:14,706 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 285 INodes.
2015-12-15 01:16:14,761 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-12-15 01:16:14,762 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 6391 from /home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000006391
2015-12-15 01:16:14,762 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@53ccbdb5 expecting start txid #6392
2015-12-15 01:16:14,762 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000006392-0000000000000006476
2015-12-15 01:16:14,765 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000006392-0000000000000006476' to transaction ID 6392
2015-12-15 01:16:14,802 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000006392-0000000000000006476 of size 1048576 edits # 85 loaded in 0 seconds
2015-12-15 01:16:14,803 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-12-15 01:16:14,810 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 6477
2015-12-15 01:16:14,849 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-12-15 01:16:14,850 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 438 msecs
2015-12-15 01:16:15,073 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-12-15 01:16:15,082 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-15 01:16:15,095 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-12-15 01:16:15,128 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-12-15 01:16:15,140 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-15 01:16:15,140 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-12-15 01:16:15,140 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 261 blocks to reach the threshold 0.9990 of total blocks 261.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2015-12-15 01:16:15,174 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-15 01:16:15,175 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-12-15 01:16:15,178 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-12-15 01:16:15,178 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-12-15 01:16:15,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-12-15 01:16:15,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 11021124 milliseconds
2015-12-15 01:16:15,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 6 millisecond(s).
2015-12-15 01:16:19,147 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0) storage 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-15 01:16:19,151 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-15 01:16:19,151 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-15 01:16:19,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-15 01:16:19,232 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-75e73aff-3510-410a-91be-881360516f94 for DN 127.0.0.1:50010
2015-12-15 01:16:19,277 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 260 has reached the threshold 0.9990 of total blocks 261. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2015-12-15 01:16:19,277 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-12-15 01:16:19,280 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-75e73aff-3510-410a-91be-881360516f94,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-12-15 01:16:19,283 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 261, hasStaleStorages: false, processing time: 11 msecs
2015-12-15 01:16:19,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 261
2015-12-15 01:16:19,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-12-15 01:16:19,284 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 7
2015-12-15 01:16:19,284 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-12-15 01:16:19,284 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-12-15 01:16:19,284 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 6 msec
2015-12-15 01:16:39,284 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 261 has reached the threshold 0.9990 of total blocks 261. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2015-12-15 01:16:39,498 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.delete from 127.0.0.1:41184 Call#3 Retry#0: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/parallels/chessdata/intermediate_output. Name node is in safe mode.
The reported blocks 261 has reached the threshold 0.9990 of total blocks 261. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2015-12-15 01:16:45,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:16:45,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:16:49,289 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 35 secs
2015-12-15 01:16:49,289 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2015-12-15 01:16:49,289 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2015-12-15 01:16:49,290 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 7 blocks
2015-12-15 01:17:15,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:17:15,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:17:24,027 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-15 01:17:24,027 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-15 01:17:24,027 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 6477
2015-12-15 01:17:24,027 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 5 
2015-12-15 01:17:24,028 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 6 
2015-12-15 01:17:24,029 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000006477 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000006477-0000000000000006478
2015-12-15 01:17:24,031 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 6479
2015-12-15 01:17:25,297 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 4428.57 KB/s
2015-12-15 01:17:25,298 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000006478 size 32189 bytes.
2015-12-15 01:17:25,304 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 6391
2015-12-15 01:17:25,305 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000006388, cpktTxId=0000000000000006388)
2015-12-15 01:17:45,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:17:45,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:17:57,946 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742618_1795 127.0.0.1:50010 
2015-12-15 01:18:00,179 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742618_1795]
2015-12-15 01:18:12,817 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0001/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742622_1799{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:18:13,159 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742622_1799{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-15 01:18:13,160 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2015-12-15 01:18:13,184 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742622_1799{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 7767
2015-12-15 01:18:13,569 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0001/job.jar is closed by DFSClient_NONMAPREDUCE_742191953_1
2015-12-15 01:18:13,580 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0001/job.jar
2015-12-15 01:18:13,663 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0001/job.split
2015-12-15 01:18:13,681 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0001/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742623_1800{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:18:13,692 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742623_1800{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:18:13,694 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0001/job.split is closed by DFSClient_NONMAPREDUCE_742191953_1
2015-12-15 01:18:13,708 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0001/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742624_1801{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:18:13,715 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742624_1801{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:18:13,717 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_742191953_1
2015-12-15 01:18:13,966 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0001/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742625_1802{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:18:13,984 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742625_1802{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:18:13,986 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0001/job.xml is closed by DFSClient_NONMAPREDUCE_742191953_1
2015-12-15 01:18:15,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:18:15,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:18:22,837 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0001/job_1450160190262_0001_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742626_1803{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:18:22,975 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742626_1803{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:18:22,980 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0001/job_1450160190262_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_990837136_1
2015-12-15 01:18:40,023 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0001/job_1450160190262_0001_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742627_1804{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:18:40,025 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 42 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 0 Number of syncs: 29 SyncTimes(ms): 128 
2015-12-15 01:18:40,143 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0001/job_1450160190262_0001_1.jhist for DFSClient_NONMAPREDUCE_990837136_1
2015-12-15 01:18:45,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-15 01:18:45,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-15 01:18:50,106 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450160190262_0001_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742628_1805{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:18:50,336 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742628_1805{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:18:50,343 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450160190262_0001_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450160190262_0001_r_000000_0_298606197_1
2015-12-15 01:18:51,675 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0001/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_990837136_1
2015-12-15 01:18:51,733 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_990837136_1
2015-12-15 01:18:51,740 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0001/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_990837136_1
2015-12-15 01:18:51,767 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742627_1804{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12229
2015-12-15 01:18:51,770 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0001/job_1450160190262_0001_1.jhist is closed by DFSClient_NONMAPREDUCE_990837136_1
2015-12-15 01:18:51,781 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0001.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742629_1806{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:18:51,789 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742629_1806{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:18:51,794 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0001.summary_tmp is closed by DFSClient_NONMAPREDUCE_990837136_1
2015-12-15 01:18:51,848 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0001-1450160294279-parallels-task+C1-1450160331742-3-1-SUCCEEDED-default-1450160302348.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742630_1807{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:18:51,859 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742630_1807{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:18:51,862 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0001-1450160294279-parallels-task+C1-1450160331742-3-1-SUCCEEDED-default-1450160302348.jhist_tmp is closed by DFSClient_NONMAPREDUCE_990837136_1
2015-12-15 01:18:51,884 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0001_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742631_1808{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:18:51,891 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742631_1808{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:18:51,896 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0001_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_990837136_1
2015-12-15 01:18:52,978 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742622_1799 127.0.0.1:50010 
2015-12-15 01:18:52,978 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742623_1800 127.0.0.1:50010 
2015-12-15 01:18:52,978 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742624_1801 127.0.0.1:50010 
2015-12-15 01:18:52,978 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742625_1802 127.0.0.1:50010 
2015-12-15 01:18:52,978 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742627_1804 127.0.0.1:50010 
2015-12-15 01:18:52,978 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742626_1803 127.0.0.1:50010 
2015-12-15 01:18:54,121 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0002/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742632_1809{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:18:54,140 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742632_1809{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:18:54,142 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0002/job.jar is closed by DFSClient_NONMAPREDUCE_742191953_1
2015-12-15 01:18:54,145 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0002/job.jar
2015-12-15 01:18:54,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0002/job.split
2015-12-15 01:18:54,168 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0002/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742633_1810{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:18:54,180 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742633_1810{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:18:54,183 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0002/job.split is closed by DFSClient_NONMAPREDUCE_742191953_1
2015-12-15 01:18:54,201 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0002/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742634_1811{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:18:54,211 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742634_1811{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:18:54,215 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0002/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_742191953_1
2015-12-15 01:18:54,285 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0002/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742635_1812{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:18:54,292 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742635_1812{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:18:54,295 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0002/job.xml is closed by DFSClient_NONMAPREDUCE_742191953_1
2015-12-15 01:18:54,431 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742624_1801, blk_1073742625_1802, blk_1073742626_1803, blk_1073742627_1804, blk_1073742622_1799, blk_1073742623_1800]
2015-12-15 01:19:04,644 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0002/job_1450160190262_0002_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742636_1813{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:19:04,798 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742636_1813{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:19:04,806 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0002/job_1450160190262_0002_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1974662525_1
2015-12-15 01:19:10,053 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0002/job_1450160190262_0002_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742637_1814{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:19:10,071 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0002/job_1450160190262_0002_1.jhist for DFSClient_NONMAPREDUCE_-1974662525_1
2015-12-15 01:19:15,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:19:15,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:19:16,513 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450160190262_0002_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742638_1815{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:19:16,709 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742638_1815{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:19:16,715 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450160190262_0002_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450160190262_0002_r_000000_0_988753512_1
2015-12-15 01:19:16,977 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0002/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1974662525_1
2015-12-15 01:19:17,031 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1974662525_1
2015-12-15 01:19:17,037 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0002/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1974662525_1
2015-12-15 01:19:17,054 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742637_1814{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12978
2015-12-15 01:19:17,056 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0002/job_1450160190262_0002_1.jhist is closed by DFSClient_NONMAPREDUCE_-1974662525_1
2015-12-15 01:19:17,066 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0002.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742639_1816{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:19:17,072 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742639_1816{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:19:17,075 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0002.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1974662525_1
2015-12-15 01:19:17,100 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0002-1450160334329-parallels-task+C2-1450160357039-1-1-SUCCEEDED-default-1450160344240.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742640_1817{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:19:17,106 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742640_1817{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:19:17,108 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0002-1450160334329-parallels-task+C2-1450160357039-1-1-SUCCEEDED-default-1450160344240.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1974662525_1
2015-12-15 01:19:17,126 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0002_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742641_1818{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:19:17,133 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742641_1818{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:19:17,136 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0002_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1974662525_1
2015-12-15 01:19:18,169 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742632_1809 127.0.0.1:50010 
2015-12-15 01:19:18,169 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742633_1810 127.0.0.1:50010 
2015-12-15 01:19:18,169 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742634_1811 127.0.0.1:50010 
2015-12-15 01:19:18,169 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742635_1812 127.0.0.1:50010 
2015-12-15 01:19:18,169 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742637_1814 127.0.0.1:50010 
2015-12-15 01:19:18,169 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742636_1813 127.0.0.1:50010 
2015-12-15 01:19:18,437 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742632_1809, blk_1073742633_1810, blk_1073742634_1811, blk_1073742635_1812, blk_1073742636_1813, blk_1073742637_1814]
2015-12-15 01:19:45,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:19:45,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:20:15,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:20:15,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:20:45,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:20:45,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:21:15,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:21:15,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:21:45,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:21:45,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:22:15,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:22:15,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:22:19,668 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 163 Total time for transactions(ms): 29 Number of transactions batched in Syncs: 0 Number of syncs: 116 SyncTimes(ms): 222 
2015-12-15 01:22:19,670 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742638_1815 127.0.0.1:50010 
2015-12-15 01:22:21,494 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742638_1815]
2015-12-15 01:22:22,164 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742628_1805 127.0.0.1:50010 
2015-12-15 01:22:22,321 INFO logs: Aliases are enabled
2015-12-15 01:22:24,494 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742628_1805]
2015-12-15 01:22:27,658 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0003/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742642_1819{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:22:27,900 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742642_1819{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:22:27,904 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0003/job.jar is closed by DFSClient_NONMAPREDUCE_-737380171_1
2015-12-15 01:22:27,921 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0003/job.jar
2015-12-15 01:22:28,051 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0003/job.split
2015-12-15 01:22:28,061 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0003/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742643_1820{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:22:28,071 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742643_1820{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:22:28,073 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0003/job.split is closed by DFSClient_NONMAPREDUCE_-737380171_1
2015-12-15 01:22:28,090 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0003/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742644_1821{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:22:28,096 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742644_1821{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:22:28,101 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0003/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-737380171_1
2015-12-15 01:22:28,428 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0003/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742645_1822{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:22:28,476 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073742645_1822{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-12-15 01:22:28,476 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742645_1822{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 87280
2015-12-15 01:22:28,880 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0003/job.xml is closed by DFSClient_NONMAPREDUCE_-737380171_1
2015-12-15 01:22:35,084 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0003/job_1450160190262_0003_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742646_1823{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:22:35,216 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742646_1823{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:22:35,223 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0003/job_1450160190262_0003_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-637170363_1
2015-12-15 01:22:45,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:22:45,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-15 01:22:51,827 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0003/job_1450160190262_0003_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742647_1824{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:22:52,250 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0003/job_1450160190262_0003_1.jhist for DFSClient_NONMAPREDUCE_-637170363_1
2015-12-15 01:23:02,838 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450160190262_0003_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742648_1825{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:23:03,092 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742648_1825{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:23:03,103 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450160190262_0003_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450160190262_0003_r_000000_0_1558599516_1
2015-12-15 01:23:03,361 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0003/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-637170363_1
2015-12-15 01:23:03,418 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-637170363_1
2015-12-15 01:23:03,423 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0003/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-637170363_1
2015-12-15 01:23:03,450 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742647_1824{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 14341
2015-12-15 01:23:03,453 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0003/job_1450160190262_0003_1.jhist is closed by DFSClient_NONMAPREDUCE_-637170363_1
2015-12-15 01:23:03,462 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0003.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742649_1826{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:23:03,469 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742649_1826{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:23:03,473 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0003.summary_tmp is closed by DFSClient_NONMAPREDUCE_-637170363_1
2015-12-15 01:23:03,537 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0003-1450160549127-parallels-task+C1-1450160583428-3-1-SUCCEEDED-default-1450160554612.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742650_1827{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:23:03,557 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742650_1827{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:23:03,561 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0003-1450160549127-parallels-task+C1-1450160583428-3-1-SUCCEEDED-default-1450160554612.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-637170363_1
2015-12-15 01:23:03,597 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0003_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742651_1828{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:23:03,625 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742651_1828{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:23:03,632 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0003_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-637170363_1
2015-12-15 01:23:04,690 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742642_1819 127.0.0.1:50010 
2015-12-15 01:23:04,690 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742643_1820 127.0.0.1:50010 
2015-12-15 01:23:04,690 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742644_1821 127.0.0.1:50010 
2015-12-15 01:23:04,690 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742645_1822 127.0.0.1:50010 
2015-12-15 01:23:04,690 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742647_1824 127.0.0.1:50010 
2015-12-15 01:23:04,690 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742646_1823 127.0.0.1:50010 
2015-12-15 01:23:05,994 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0004/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742652_1829{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:23:06,015 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742652_1829{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:23:06,018 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0004/job.jar is closed by DFSClient_NONMAPREDUCE_-737380171_1
2015-12-15 01:23:06,025 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0004/job.jar
2015-12-15 01:23:06,063 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0004/job.split
2015-12-15 01:23:06,068 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0004/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742653_1830{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:23:06,095 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742653_1830{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:23:06,106 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0004/job.split is closed by DFSClient_NONMAPREDUCE_-737380171_1
2015-12-15 01:23:06,120 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0004/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742654_1831{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:23:06,133 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742654_1831{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:23:06,136 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0004/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-737380171_1
2015-12-15 01:23:06,190 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0004/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742655_1832{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:23:06,208 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742655_1832{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:23:06,211 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0004/job.xml is closed by DFSClient_NONMAPREDUCE_-737380171_1
2015-12-15 01:23:06,622 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742642_1819, blk_1073742643_1820, blk_1073742644_1821, blk_1073742645_1822, blk_1073742646_1823, blk_1073742647_1824]
2015-12-15 01:23:15,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:23:15,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 9 millisecond(s).
2015-12-15 01:23:17,547 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0004/job_1450160190262_0004_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742656_1833{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:23:17,741 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742656_1833{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:23:17,747 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0004/job_1450160190262_0004_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1688126327_1
2015-12-15 01:23:23,263 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0004/job_1450160190262_0004_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742657_1834{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:23:23,264 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 284 Total time for transactions(ms): 66 Number of transactions batched in Syncs: 1 Number of syncs: 199 SyncTimes(ms): 613 
2015-12-15 01:23:23,292 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0004/job_1450160190262_0004_1.jhist for DFSClient_NONMAPREDUCE_-1688126327_1
2015-12-15 01:23:30,511 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450160190262_0004_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742658_1835{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:23:30,669 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742658_1835{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:23:30,677 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450160190262_0004_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450160190262_0004_r_000000_0_-394416122_1
2015-12-15 01:23:30,987 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0004/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-1688126327_1
2015-12-15 01:23:31,032 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1688126327_1
2015-12-15 01:23:31,038 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0004/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1688126327_1
2015-12-15 01:23:31,060 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742657_1834{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12977
2015-12-15 01:23:31,063 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0004/job_1450160190262_0004_1.jhist is closed by DFSClient_NONMAPREDUCE_-1688126327_1
2015-12-15 01:23:31,071 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0004.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742659_1836{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:23:31,078 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742659_1836{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:23:31,080 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0004.summary_tmp is closed by DFSClient_NONMAPREDUCE_-1688126327_1
2015-12-15 01:23:31,107 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0004-1450160586259-parallels-task+C2-1450160611043-1-1-SUCCEEDED-default-1450160597031.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742660_1837{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:23:31,113 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742660_1837{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:23:31,117 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0004-1450160586259-parallels-task+C2-1450160611043-1-1-SUCCEEDED-default-1450160597031.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-1688126327_1
2015-12-15 01:23:31,137 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0004_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742661_1838{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:23:31,144 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742661_1838{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:23:31,147 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0004_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-1688126327_1
2015-12-15 01:23:32,181 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742652_1829 127.0.0.1:50010 
2015-12-15 01:23:32,181 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742653_1830 127.0.0.1:50010 
2015-12-15 01:23:32,182 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742654_1831 127.0.0.1:50010 
2015-12-15 01:23:32,182 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742655_1832 127.0.0.1:50010 
2015-12-15 01:23:32,182 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742657_1834 127.0.0.1:50010 
2015-12-15 01:23:32,182 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742656_1833 127.0.0.1:50010 
2015-12-15 01:23:33,628 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742656_1833, blk_1073742657_1834, blk_1073742652_1829, blk_1073742653_1830, blk_1073742654_1831, blk_1073742655_1832]
2015-12-15 01:23:45,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:23:45,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:24:15,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:24:15,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:24:45,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:24:45,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:25:15,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:25:15,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:25:45,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:25:45,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:26:15,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:26:15,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:26:26,711 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 325 Total time for transactions(ms): 68 Number of transactions batched in Syncs: 1 Number of syncs: 230 SyncTimes(ms): 644 
2015-12-15 01:26:26,716 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742658_1835 127.0.0.1:50010 
2015-12-15 01:26:27,680 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742658_1835]
2015-12-15 01:26:28,870 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742648_1825 127.0.0.1:50010 
2015-12-15 01:26:30,681 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742648_1825]
2015-12-15 01:26:32,959 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0005/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742662_1839{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:26:33,098 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742662_1839{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:26:33,100 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0005/job.jar is closed by DFSClient_NONMAPREDUCE_-125775680_1
2015-12-15 01:26:33,105 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0005/job.jar
2015-12-15 01:26:33,167 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0005/job.split
2015-12-15 01:26:33,185 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0005/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742663_1840{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:26:33,198 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742663_1840{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:26:33,200 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0005/job.split is closed by DFSClient_NONMAPREDUCE_-125775680_1
2015-12-15 01:26:33,215 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0005/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742664_1841{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:26:33,222 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742664_1841{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:26:33,225 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0005/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-125775680_1
2015-12-15 01:26:33,413 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0005/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742665_1842{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:26:33,447 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742665_1842{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:26:33,454 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0005/job.xml is closed by DFSClient_NONMAPREDUCE_-125775680_1
2015-12-15 01:26:38,910 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0005/job_1450160190262_0005_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742666_1843{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:26:39,046 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742666_1843{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:26:39,051 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0005/job_1450160190262_0005_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-182392407_1
2015-12-15 01:26:45,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:26:45,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:26:53,353 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0005/job_1450160190262_0005_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742667_1844{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:26:53,483 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0005/job_1450160190262_0005_1.jhist for DFSClient_NONMAPREDUCE_-182392407_1
2015-12-15 01:27:02,740 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450160190262_0005_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742668_1845{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:27:02,939 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742668_1845{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:27:02,950 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_temporary/1/_temporary/attempt_1450160190262_0005_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450160190262_0005_r_000000_0_-538118467_1
2015-12-15 01:27:04,179 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0005/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-182392407_1
2015-12-15 01:27:04,227 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/intermediate_output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-182392407_1
2015-12-15 01:27:04,233 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0005/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-182392407_1
2015-12-15 01:27:04,258 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742667_1844{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12229
2015-12-15 01:27:04,263 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0005/job_1450160190262_0005_1.jhist is closed by DFSClient_NONMAPREDUCE_-182392407_1
2015-12-15 01:27:04,280 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0005.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742669_1846{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:27:04,290 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742669_1846{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:27:04,293 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0005.summary_tmp is closed by DFSClient_NONMAPREDUCE_-182392407_1
2015-12-15 01:27:04,342 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0005-1450160793684-parallels-task+C1-1450160824234-3-1-SUCCEEDED-default-1450160798524.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742670_1847{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:27:04,355 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742670_1847{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:27:04,357 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0005-1450160793684-parallels-task+C1-1450160824234-3-1-SUCCEEDED-default-1450160798524.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-182392407_1
2015-12-15 01:27:04,379 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0005_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742671_1848{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:27:04,391 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742671_1848{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:27:04,393 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0005_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-182392407_1
2015-12-15 01:27:05,441 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742662_1839 127.0.0.1:50010 
2015-12-15 01:27:05,441 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742663_1840 127.0.0.1:50010 
2015-12-15 01:27:05,441 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742664_1841 127.0.0.1:50010 
2015-12-15 01:27:05,441 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742665_1842 127.0.0.1:50010 
2015-12-15 01:27:05,441 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742667_1844 127.0.0.1:50010 
2015-12-15 01:27:05,442 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742666_1843 127.0.0.1:50010 
2015-12-15 01:27:06,669 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0006/job.jar. BP-1841603009-127.0.1.1-1449456040486 blk_1073742672_1849{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:27:06,689 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742672_1849{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:27:06,694 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0006/job.jar is closed by DFSClient_NONMAPREDUCE_-125775680_1
2015-12-15 01:27:06,696 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0006/job.jar
2015-12-15 01:27:06,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0006/job.split
2015-12-15 01:27:06,722 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0006/job.split. BP-1841603009-127.0.1.1-1449456040486 blk_1073742673_1850{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:27:06,736 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742673_1850{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:27:06,744 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0006/job.split is closed by DFSClient_NONMAPREDUCE_-125775680_1
2015-12-15 01:27:06,755 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0006/job.splitmetainfo. BP-1841603009-127.0.1.1-1449456040486 blk_1073742674_1851{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:27:06,763 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742674_1851{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:27:06,766 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0006/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_-125775680_1
2015-12-15 01:27:06,820 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0006/job.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742675_1852{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:27:06,833 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742675_1852{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:27:06,840 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0006/job.xml is closed by DFSClient_NONMAPREDUCE_-125775680_1
2015-12-15 01:27:06,898 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742662_1839, blk_1073742663_1840, blk_1073742664_1841, blk_1073742665_1842, blk_1073742666_1843, blk_1073742667_1844]
2015-12-15 01:27:15,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:27:15,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:27:15,764 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0006/job_1450160190262_0006_1_conf.xml. BP-1841603009-127.0.1.1-1449456040486 blk_1073742676_1853{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:27:15,898 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742676_1853{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:27:15,904 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0006/job_1450160190262_0006_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-425851021_1
2015-12-15 01:27:20,580 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0006/job_1450160190262_0006_1.jhist. BP-1841603009-127.0.1.1-1449456040486 blk_1073742677_1854{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:27:20,598 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0006/job_1450160190262_0006_1.jhist for DFSClient_NONMAPREDUCE_-425851021_1
2015-12-15 01:27:26,386 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450160190262_0006_r_000000_0/part-r-00000. BP-1841603009-127.0.1.1-1449456040486 blk_1073742678_1855{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:27:26,530 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742678_1855{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:27:26,540 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_temporary/1/_temporary/attempt_1450160190262_0006_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1450160190262_0006_r_000000_0_-350225901_1
2015-12-15 01:27:26,758 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 456 Total time for transactions(ms): 78 Number of transactions batched in Syncs: 1 Number of syncs: 319 SyncTimes(ms): 801 
2015-12-15 01:27:26,765 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0006/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_-425851021_1
2015-12-15 01:27:26,813 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/parallels/chessdata/output_c/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-425851021_1
2015-12-15 01:27:26,820 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0006/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_-425851021_1
2015-12-15 01:27:26,835 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742677_1854{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 12978
2015-12-15 01:27:26,838 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/parallels/.staging/job_1450160190262_0006/job_1450160190262_0006_1.jhist is closed by DFSClient_NONMAPREDUCE_-425851021_1
2015-12-15 01:27:26,845 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0006.summary_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742679_1856{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:27:26,852 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742679_1856{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:27:26,855 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0006.summary_tmp is closed by DFSClient_NONMAPREDUCE_-425851021_1
2015-12-15 01:27:26,887 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0006-1450160826895-parallels-task+C2-1450160846821-1-1-SUCCEEDED-default-1450160835429.jhist_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742680_1857{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:27:26,895 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742680_1857{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:27:26,899 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0006-1450160826895-parallels-task+C2-1450160846821-1-1-SUCCEEDED-default-1450160835429.jhist_tmp is closed by DFSClient_NONMAPREDUCE_-425851021_1
2015-12-15 01:27:26,921 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0006_conf.xml_tmp. BP-1841603009-127.0.1.1-1449456040486 blk_1073742681_1858{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]}
2015-12-15 01:27:26,929 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073742681_1858{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-12-15 01:27:26,931 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/parallels/job_1450160190262_0006_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_-425851021_1
2015-12-15 01:27:27,963 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742672_1849 127.0.0.1:50010 
2015-12-15 01:27:27,963 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742673_1850 127.0.0.1:50010 
2015-12-15 01:27:27,963 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742674_1851 127.0.0.1:50010 
2015-12-15 01:27:27,964 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742675_1852 127.0.0.1:50010 
2015-12-15 01:27:27,964 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742677_1854 127.0.0.1:50010 
2015-12-15 01:27:27,964 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073742676_1853 127.0.0.1:50010 
2015-12-15 01:27:30,903 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073742672_1849, blk_1073742673_1850, blk_1073742674_1851, blk_1073742675_1852, blk_1073742676_1853, blk_1073742677_1854]
2015-12-15 01:27:45,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:27:45,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:28:15,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:28:15,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:28:45,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:28:45,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:29:15,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:29:15,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:29:45,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:29:45,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:30:15,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:30:15,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:30:45,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:30:45,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:31:15,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:31:15,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:31:45,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:31:45,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:32:15,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:32:15,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:32:45,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:32:45,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:33:15,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:33:15,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:33:45,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:33:45,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:34:15,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:34:15,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:34:45,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:34:45,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:35:15,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:35:15,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:35:45,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:35:45,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:36:15,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:36:15,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-15 01:36:45,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:36:45,203 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:37:15,203 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:37:15,203 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:37:45,203 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:37:45,203 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:38:15,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:38:15,209 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 6 millisecond(s).
2015-12-15 01:38:45,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:38:45,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:39:15,203 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:39:15,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:39:45,203 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:39:45,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:40:15,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:40:15,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:40:45,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:40:45,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:41:15,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:41:15,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:41:45,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:41:45,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:42:15,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:42:15,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:42:45,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:42:45,206 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:43:15,206 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:43:15,207 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:43:45,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:43:45,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:44:15,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:44:15,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:44:45,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:44:45,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:45:15,207 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:45:15,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:45:45,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:45:45,209 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:46:15,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:46:15,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:46:45,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:46:45,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:47:15,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:47:15,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:47:45,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:47:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:48:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:48:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:48:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:48:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:49:15,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:49:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:49:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:49:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:50:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:50:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:50:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:50:45,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-15 01:51:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:51:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:51:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:51:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:52:15,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:52:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:52:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:52:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:53:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:53:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:53:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:53:45,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:54:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:54:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:54:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:54:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:55:15,211 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:55:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:55:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:55:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:56:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:56:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:56:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:56:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:57:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:57:15,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:57:45,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:57:45,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:58:15,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:58:15,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:58:45,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:58:45,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 01:59:15,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 01:59:15,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 01:59:45,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 01:59:45,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:00:15,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:00:15,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:00:45,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:00:45,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:01:15,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:01:15,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:01:45,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:01:45,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:02:15,216 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:02:15,216 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:02:45,216 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:02:45,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:03:15,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:03:15,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:03:45,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:03:45,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:04:15,218 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:04:15,219 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:04:45,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30067 milliseconds
2015-12-15 02:04:45,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-15 02:05:15,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:05:15,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:05:45,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:05:45,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:06:15,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:06:15,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:06:45,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:06:45,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:07:15,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:07:15,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:07:45,288 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:07:45,288 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:08:15,288 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:08:15,289 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:08:45,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:08:45,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:09:15,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:09:15,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:09:45,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:09:45,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:10:15,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:10:15,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:10:45,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:10:45,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:11:15,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:11:15,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:11:45,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:11:45,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:12:15,292 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:12:15,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:12:45,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:12:45,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:13:15,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:13:15,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:13:45,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:13:45,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:14:15,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:14:15,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:14:45,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:14:45,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:15:15,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:15:15,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:15:45,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:15:45,296 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:16:15,295 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:16:15,296 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:16:45,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:16:45,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:17:15,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:17:15,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:17:26,547 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-12-15 02:17:26,547 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-12-15 02:17:26,547 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 6479
2015-12-15 02:17:26,550 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 487 Total time for transactions(ms): 81 Number of transactions batched in Syncs: 1 Number of syncs: 344 SyncTimes(ms): 814 
2015-12-15 02:17:26,556 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 487 Total time for transactions(ms): 81 Number of transactions batched in Syncs: 1 Number of syncs: 345 SyncTimes(ms): 819 
2015-12-15 02:17:26,558 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/parallels/hadoop/tmp/dfs/name/current/edits_inprogress_0000000000000006479 -> /home/parallels/hadoop/tmp/dfs/name/current/edits_0000000000000006479-0000000000000006965
2015-12-15 02:17:26,560 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 6966
2015-12-15 02:17:26,831 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 5500.00 KB/s
2015-12-15 02:17:26,831 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000006965 size 34506 bytes.
2015-12-15 02:17:26,836 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 6478
2015-12-15 02:17:26,836 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/parallels/hadoop/tmp/dfs/name/current/fsimage_0000000000000006391, cpktTxId=0000000000000006391)
2015-12-15 02:17:45,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:17:45,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:18:15,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:18:15,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:18:45,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:18:45,300 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:19:15,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:19:15,300 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:19:45,300 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 02:19:45,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 02:20:15,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:20:15,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:20:45,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:20:45,302 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:21:15,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:21:15,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 02:21:45,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 02:21:45,301 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 03:22:09,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30003 milliseconds
2015-12-15 03:22:09,030 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 04:22:29,629 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* removeDeadDatanode: lost heartbeat from 127.0.0.1:50010
2015-12-15 04:22:29,664 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/127.0.0.1:50010
2015-12-15 04:22:29,873 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0) storage 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-15 04:22:29,873 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/127.0.0.1:50010
2015-12-15 04:22:29,873 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-15 04:22:29,874 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-15 04:22:29,874 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: [DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010 failed.
2015-12-15 04:22:29,877 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-15 04:22:29,891 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 280, hasStaleStorages: false, processing time: 8 msecs
2015-12-15 04:22:34,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 04:22:34,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 04:23:04,411 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 04:23:04,412 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 04:23:34,412 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 04:23:34,413 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 04:24:04,413 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 04:24:04,413 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 06:24:23,474 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* removeDeadDatanode: lost heartbeat from 127.0.0.1:50010
2015-12-15 06:24:23,484 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/127.0.0.1:50010
2015-12-15 06:24:23,712 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0) storage 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-15 06:24:23,712 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/127.0.0.1:50010
2015-12-15 06:24:23,712 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-15 06:24:23,712 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-15 06:24:23,712 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: [DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010 failed.
2015-12-15 06:24:23,714 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-15 06:24:23,724 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 280, hasStaleStorages: false, processing time: 7 msecs
2015-12-15 06:24:28,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 06:24:28,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 06:24:58,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 06:24:58,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 08:25:18,771 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* removeDeadDatanode: lost heartbeat from 127.0.0.1:50010
2015-12-15 08:25:18,784 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/127.0.0.1:50010
2015-12-15 08:25:19,018 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0) storage 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-15 08:25:19,018 INFO org.apache.hadoop.net.NetworkTopology: Removing a node: /default-rack/127.0.0.1:50010
2015-12-15 08:25:19,019 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-12-15 08:25:19,019 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-15 08:25:19,019 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: [DISK]DS-75e73aff-3510-410a-91be-881360516f94:NORMAL:127.0.0.1:50010 failed.
2015-12-15 08:25:19,020 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-12-15 08:25:19,032 INFO BlockStateChange: BLOCK* processReport: from storage DS-75e73aff-3510-410a-91be-881360516f94 node DatanodeRegistration(127.0.0.1, datanodeUuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0), blocks: 280, hasStaleStorages: false, processing time: 7 msecs
2015-12-15 08:25:23,482 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 08:25:23,482 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 08:25:53,482 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 08:25:53,483 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:14:35,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:14:35,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:15:05,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:15:05,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:15:35,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:15:35,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:16:05,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:16:05,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:16:35,171 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:16:35,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:17:05,172 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:17:05,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:17:35,173 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:17:35,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:18:05,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:18:05,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:18:35,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:18:35,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:19:05,174 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:19:05,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:19:35,175 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:19:35,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-15 09:20:05,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:20:05,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-12-15 09:20:35,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:20:35,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:21:05,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:21:05,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:21:35,176 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:21:35,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:22:05,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:22:05,178 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:22:35,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:22:35,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:23:05,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:23:05,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:23:35,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:23:35,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:24:05,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:24:05,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:24:35,179 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:24:35,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-15 09:25:05,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:25:05,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:25:35,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:25:35,180 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:26:05,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:26:05,181 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:26:35,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:26:35,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:27:05,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:27:05,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:27:35,184 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:27:35,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:28:05,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:28:05,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-15 09:28:35,185 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:28:35,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:29:05,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:29:05,186 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:29:35,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:29:35,187 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:30:05,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:30:05,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:30:35,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:30:35,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:31:05,188 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:31:05,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-15 09:31:35,189 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-15 09:31:35,190 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:32:05,191 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:32:05,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-15 09:32:35,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:32:35,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:33:05,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:33:05,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:33:35,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:33:35,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:34:05,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:34:05,193 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:34:35,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:34:35,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:35:05,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:35:05,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:35:35,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:35:35,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-12-15 09:36:05,195 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:36:05,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:36:35,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:36:35,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:37:05,196 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-12-15 09:37:05,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).
2015-12-15 09:37:35,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30002 milliseconds
2015-12-15 09:37:35,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:38:05,198 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:38:05,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:38:35,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:38:35,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:39:05,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-12-15 09:39:05,201 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 0 millisecond(s).
2015-12-15 09:39:32,835 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-15 09:39:32,913 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1
************************************************************/
