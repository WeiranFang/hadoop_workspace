2015-12-04 21:18:41,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-04 21:18:41,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-04 21:18:42,074 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-04 21:18:42,162 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-04 21:18:42,162 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-04 21:18:42,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-04 21:18:42,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-04 21:18:42,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-04 21:18:42,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-04 21:18:42,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-04 21:18:42,286 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-04 21:18:42,289 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-04 21:18:42,301 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-04 21:18:42,303 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-04 21:18:42,303 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-04 21:18:42,304 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-04 21:18:42,320 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-04 21:18:42,326 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-04 21:18:42,326 INFO org.mortbay.log: jetty-6.1.26
2015-12-04 21:18:42,526 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-04 21:18:42,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-04 21:18:42,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-04 21:18:42,749 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-04 21:18:42,764 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-04 21:18:42,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-04 21:18:42,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-04 21:18:42,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-04 21:18:42,831 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-04 21:18:42,843 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-04 21:18:42,844 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-04 21:18:43,166 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-04 21:18:43,181 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-parallels/dfs/data/in_use.lock acquired by nodename 3363@ubuntu
2015-12-04 21:18:43,183 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-parallels/dfs/data is not formatted for BP-1952917173-127.0.1.1-1449292642740
2015-12-04 21:18:43,183 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-12-04 21:18:43,247 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1952917173-127.0.1.1-1449292642740
2015-12-04 21:18:43,247 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-12-04 21:18:43,250 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740 is not formatted.
2015-12-04 21:18:43,250 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-12-04 21:18:43,250 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1952917173-127.0.1.1-1449292642740 directory /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current
2015-12-04 21:18:43,253 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-04 21:18:43,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=288020609;bpid=BP-1952917173-127.0.1.1-1449292642740;lv=-56;nsInfo=lv=-60;cid=CID-fc10af95-6416-4edd-9572-a006bee00a03;nsid=288020609;c=0;bpid=BP-1952917173-127.0.1.1-1449292642740;dnuuid=null
2015-12-04 21:18:43,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 7b1d6000-b128-42b1-b8a1-6bfae41da4eb
2015-12-04 21:18:43,301 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-parallels/dfs/data/current
2015-12-04 21:18:43,302 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-parallels/dfs/data/current, StorageType: DISK
2015-12-04 21:18:43,313 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-04 21:18:43,319 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1449295152318 with interval 21600000
2015-12-04 21:18:43,320 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1952917173-127.0.1.1-1449292642740
2015-12-04 21:18:43,324 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1952917173-127.0.1.1-1449292642740 on volume /tmp/hadoop-parallels/dfs/data/current...
2015-12-04 21:18:43,356 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1952917173-127.0.1.1-1449292642740 on /tmp/hadoop-parallels/dfs/data/current: 31ms
2015-12-04 21:18:43,356 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1952917173-127.0.1.1-1449292642740: 35ms
2015-12-04 21:18:43,356 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1952917173-127.0.1.1-1449292642740 on volume /tmp/hadoop-parallels/dfs/data/current...
2015-12-04 21:18:43,356 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1952917173-127.0.1.1-1449292642740 on volume /tmp/hadoop-parallels/dfs/data/current: 0ms
2015-12-04 21:18:43,357 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-12-04 21:18:43,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1952917173-127.0.1.1-1449292642740 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-04 21:18:43,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1952917173-127.0.1.1-1449292642740 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-04 21:18:43,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-04 21:18:43,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1952917173-127.0.1.1-1449292642740 (Datanode Uuid 7b1d6000-b128-42b1-b8a1-6bfae41da4eb) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-12-04 21:18:43,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1952917173-127.0.1.1-1449292642740 (Datanode Uuid 7b1d6000-b128-42b1-b8a1-6bfae41da4eb) service to localhost/127.0.0.1:9000
2015-12-04 21:18:43,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 50 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@182a3d8b
2015-12-04 21:18:43,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1952917173-127.0.1.1-1449292642740
2015-12-04 21:18:43,605 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-12-04 21:18:43,605 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-04 21:18:43,606 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-12-04 21:18:43,606 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-12-04 21:18:43,608 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1952917173-127.0.1.1-1449292642740
2015-12-04 21:18:43,615 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1952917173-127.0.1.1-1449292642740 to blockPoolScannerMap, new size=1
2015-12-04 21:59:12,326 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1952917173-127.0.1.1-1449292642740 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-12-04 22:09:01,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741825_1001 src: /127.0.0.1:46041 dest: /127.0.0.1:50010
2015-12-04 22:09:01,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46041, dest: /127.0.0.1:50010, bytes: 4436, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741825_1001, duration: 106586753
2015-12-04 22:09:01,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741826_1002 src: /127.0.0.1:46042 dest: /127.0.0.1:50010
2015-12-04 22:09:02,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46042, dest: /127.0.0.1:50010, bytes: 1335, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741826_1002, duration: 3739126
2015-12-04 22:09:02,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741827_1003 src: /127.0.0.1:46043 dest: /127.0.0.1:50010
2015-12-04 22:09:02,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46043, dest: /127.0.0.1:50010, bytes: 318, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741827_1003, duration: 2038902
2015-12-04 22:09:02,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741828_1004 src: /127.0.0.1:46044 dest: /127.0.0.1:50010
2015-12-04 22:09:02,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46044, dest: /127.0.0.1:50010, bytes: 872, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741828_1004, duration: 5055532
2015-12-04 22:09:02,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741829_1005 src: /127.0.0.1:46045 dest: /127.0.0.1:50010
2015-12-04 22:09:02,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46045, dest: /127.0.0.1:50010, bytes: 3670, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741829_1005, duration: 4579338
2015-12-04 22:09:02,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741830_1006 src: /127.0.0.1:46046 dest: /127.0.0.1:50010
2015-12-04 22:09:02,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46046, dest: /127.0.0.1:50010, bytes: 4236, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741830_1006, duration: 2268498
2015-12-04 22:09:02,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741831_1007 src: /127.0.0.1:46047 dest: /127.0.0.1:50010
2015-12-04 22:09:02,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46047, dest: /127.0.0.1:50010, bytes: 2490, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741831_1007, duration: 2505124
2015-12-04 22:09:02,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741832_1008 src: /127.0.0.1:46048 dest: /127.0.0.1:50010
2015-12-04 22:09:02,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46048, dest: /127.0.0.1:50010, bytes: 2598, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741832_1008, duration: 3319641
2015-12-04 22:09:02,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741832_1008, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741833_1009 src: /127.0.0.1:46049 dest: /127.0.0.1:50010
2015-12-04 22:09:02,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46049, dest: /127.0.0.1:50010, bytes: 9683, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741833_1009, duration: 3008858
2015-12-04 22:09:02,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741833_1009, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741834_1010 src: /127.0.0.1:46050 dest: /127.0.0.1:50010
2015-12-04 22:09:02,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46050, dest: /127.0.0.1:50010, bytes: 855, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741834_1010, duration: 5021971
2015-12-04 22:09:02,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741834_1010, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741835_1011 src: /127.0.0.1:46051 dest: /127.0.0.1:50010
2015-12-04 22:09:02,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46051, dest: /127.0.0.1:50010, bytes: 1449, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741835_1011, duration: 1800451
2015-12-04 22:09:02,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741835_1011, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741836_1012 src: /127.0.0.1:46052 dest: /127.0.0.1:50010
2015-12-04 22:09:02,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46052, dest: /127.0.0.1:50010, bytes: 1657, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741836_1012, duration: 2227140
2015-12-04 22:09:02,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741836_1012, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741837_1013 src: /127.0.0.1:46053 dest: /127.0.0.1:50010
2015-12-04 22:09:02,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46053, dest: /127.0.0.1:50010, bytes: 21, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741837_1013, duration: 2860474
2015-12-04 22:09:02,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741837_1013, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741838_1014 src: /127.0.0.1:46054 dest: /127.0.0.1:50010
2015-12-04 22:09:02,340 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46054, dest: /127.0.0.1:50010, bytes: 620, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741838_1014, duration: 2439490
2015-12-04 22:09:02,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741838_1014, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741839_1015 src: /127.0.0.1:46055 dest: /127.0.0.1:50010
2015-12-04 22:09:02,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46055, dest: /127.0.0.1:50010, bytes: 3523, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741839_1015, duration: 3115767
2015-12-04 22:09:02,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741839_1015, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741840_1016 src: /127.0.0.1:46056 dest: /127.0.0.1:50010
2015-12-04 22:09:02,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46056, dest: /127.0.0.1:50010, bytes: 1325, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741840_1016, duration: 2890625
2015-12-04 22:09:02,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741840_1016, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741841_1017 src: /127.0.0.1:46057 dest: /127.0.0.1:50010
2015-12-04 22:09:02,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46057, dest: /127.0.0.1:50010, bytes: 1631, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741841_1017, duration: 5468739
2015-12-04 22:09:02,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741841_1017, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741842_1018 src: /127.0.0.1:46058 dest: /127.0.0.1:50010
2015-12-04 22:09:02,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46058, dest: /127.0.0.1:50010, bytes: 5511, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741842_1018, duration: 1566932
2015-12-04 22:09:02,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741842_1018, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741843_1019 src: /127.0.0.1:46059 dest: /127.0.0.1:50010
2015-12-04 22:09:02,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46059, dest: /127.0.0.1:50010, bytes: 11291, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741843_1019, duration: 2293564
2015-12-04 22:09:02,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741843_1019, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741844_1020 src: /127.0.0.1:46060 dest: /127.0.0.1:50010
2015-12-04 22:09:02,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46060, dest: /127.0.0.1:50010, bytes: 938, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741844_1020, duration: 2695121
2015-12-04 22:09:02,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741844_1020, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741845_1021 src: /127.0.0.1:46061 dest: /127.0.0.1:50010
2015-12-04 22:09:02,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46061, dest: /127.0.0.1:50010, bytes: 1383, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741845_1021, duration: 2460315
2015-12-04 22:09:02,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741845_1021, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741846_1022 src: /127.0.0.1:46062 dest: /127.0.0.1:50010
2015-12-04 22:09:02,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46062, dest: /127.0.0.1:50010, bytes: 4113, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741846_1022, duration: 1637737
2015-12-04 22:09:02,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741846_1022, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741847_1023 src: /127.0.0.1:46063 dest: /127.0.0.1:50010
2015-12-04 22:09:02,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46063, dest: /127.0.0.1:50010, bytes: 758, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741847_1023, duration: 4198401
2015-12-04 22:09:02,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741847_1023, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741848_1024 src: /127.0.0.1:46064 dest: /127.0.0.1:50010
2015-12-04 22:09:02,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46064, dest: /127.0.0.1:50010, bytes: 10, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741848_1024, duration: 1252374
2015-12-04 22:09:02,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741848_1024, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741849_1025 src: /127.0.0.1:46065 dest: /127.0.0.1:50010
2015-12-04 22:09:02,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46065, dest: /127.0.0.1:50010, bytes: 2316, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741849_1025, duration: 1520695
2015-12-04 22:09:02,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741849_1025, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741850_1026 src: /127.0.0.1:46066 dest: /127.0.0.1:50010
2015-12-04 22:09:02,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46066, dest: /127.0.0.1:50010, bytes: 2268, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741850_1026, duration: 3115418
2015-12-04 22:09:02,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741850_1026, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741851_1027 src: /127.0.0.1:46067 dest: /127.0.0.1:50010
2015-12-04 22:09:02,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46067, dest: /127.0.0.1:50010, bytes: 2237, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741851_1027, duration: 2845943
2015-12-04 22:09:02,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741851_1027, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741852_1028 src: /127.0.0.1:46068 dest: /127.0.0.1:50010
2015-12-04 22:09:02,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46068, dest: /127.0.0.1:50010, bytes: 4567, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741852_1028, duration: 1416427
2015-12-04 22:09:02,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741852_1028, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:02,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741853_1029 src: /127.0.0.1:46069 dest: /127.0.0.1:50010
2015-12-04 22:09:02,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46069, dest: /127.0.0.1:50010, bytes: 690, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-507732525_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741853_1029, duration: 4755810
2015-12-04 22:09:02,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741853_1029, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:09:08,579 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741830_1006
2015-12-04 22:09:08,584 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741848_1024
2015-12-04 22:09:08,584 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741825_1001
2015-12-04 22:09:08,585 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741832_1008
2015-12-04 22:09:08,586 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741834_1010
2015-12-04 22:09:08,587 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741844_1020
2015-12-04 22:09:08,587 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741850_1026
2015-12-04 22:09:08,588 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741853_1029
2015-12-04 22:09:08,588 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741841_1017
2015-12-04 22:09:08,589 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741843_1019
2015-12-04 22:09:08,589 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741846_1022
2015-12-04 22:09:08,590 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741852_1028
2015-12-04 22:09:08,590 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741828_1004
2015-12-04 22:09:08,591 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741836_1012
2015-12-04 22:09:08,591 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741827_1003
2015-12-04 22:09:08,592 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741838_1014
2015-12-04 22:09:08,593 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741842_1018
2015-12-04 22:09:08,594 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741847_1023
2015-12-04 22:09:08,594 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741833_1009
2015-12-04 22:09:08,595 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741845_1021
2015-12-04 22:13:55,780 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2015-12-04 22:13:55,781 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741826 for deletion
2015-12-04 22:13:55,781 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2015-12-04 22:13:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741828 for deletion
2015-12-04 22:13:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2015-12-04 22:13:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2015-12-04 22:13:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741831 for deletion
2015-12-04 22:13:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741832_1008 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741832 for deletion
2015-12-04 22:13:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741833_1009 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741833 for deletion
2015-12-04 22:13:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741834_1010 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741834 for deletion
2015-12-04 22:13:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741835_1011 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741835 for deletion
2015-12-04 22:13:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741836_1012 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741836 for deletion
2015-12-04 22:13:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741837_1013 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741837 for deletion
2015-12-04 22:13:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741838_1014 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741838 for deletion
2015-12-04 22:13:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741825_1001 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741825
2015-12-04 22:13:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741839_1015 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741839 for deletion
2015-12-04 22:13:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741840_1016 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741840 for deletion
2015-12-04 22:13:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741841_1017 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741841 for deletion
2015-12-04 22:13:55,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741842_1018 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741842 for deletion
2015-12-04 22:13:55,783 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741843_1019 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741843 for deletion
2015-12-04 22:13:55,783 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741844_1020 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741844 for deletion
2015-12-04 22:13:55,783 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741845_1021 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741845 for deletion
2015-12-04 22:13:55,783 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741846_1022 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741846 for deletion
2015-12-04 22:13:55,783 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741847_1023 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741847 for deletion
2015-12-04 22:13:55,783 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741848_1024 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741848 for deletion
2015-12-04 22:13:55,783 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741849_1025 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741849 for deletion
2015-12-04 22:13:55,783 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741850_1026 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741850 for deletion
2015-12-04 22:13:55,783 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741851_1027 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741851 for deletion
2015-12-04 22:13:55,783 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741852_1028 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741852 for deletion
2015-12-04 22:13:55,783 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741853_1029 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741853 for deletion
2015-12-04 22:13:55,784 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741826_1002 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741826
2015-12-04 22:13:55,784 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741827_1003 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741827
2015-12-04 22:13:55,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741828_1004 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741828
2015-12-04 22:13:55,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741829_1005 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741829
2015-12-04 22:13:55,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741830_1006 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741830
2015-12-04 22:13:55,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741831_1007 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741831
2015-12-04 22:13:55,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741832_1008 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741832
2015-12-04 22:13:55,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741833_1009 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741833
2015-12-04 22:13:55,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741834_1010 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741834
2015-12-04 22:13:55,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741835_1011 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741835
2015-12-04 22:13:55,785 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741836_1012 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741836
2015-12-04 22:13:55,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741837_1013 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741837
2015-12-04 22:13:55,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741838_1014 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741838
2015-12-04 22:13:55,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741839_1015 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741839
2015-12-04 22:13:55,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741840_1016 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741840
2015-12-04 22:13:55,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741841_1017 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741841
2015-12-04 22:13:55,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741842_1018 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741842
2015-12-04 22:13:55,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741843_1019 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741843
2015-12-04 22:13:55,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741844_1020 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741844
2015-12-04 22:13:55,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741845_1021 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741845
2015-12-04 22:13:55,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741846_1022 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741846
2015-12-04 22:13:55,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741847_1023 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741847
2015-12-04 22:13:55,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741848_1024 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741848
2015-12-04 22:13:55,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741849_1025 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741849
2015-12-04 22:13:55,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741850_1026 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741850
2015-12-04 22:13:55,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741851_1027 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741851
2015-12-04 22:13:55,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741852_1028 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741852
2015-12-04 22:13:55,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741853_1029 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741853
2015-12-04 22:15:41,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741854_1030 src: /127.0.0.1:46087 dest: /127.0.0.1:50010
2015-12-04 22:15:41,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46087, dest: /127.0.0.1:50010, bytes: 4436, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741854_1030, duration: 45688887
2015-12-04 22:15:41,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741854_1030, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:41,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741855_1031 src: /127.0.0.1:46088 dest: /127.0.0.1:50010
2015-12-04 22:15:41,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46088, dest: /127.0.0.1:50010, bytes: 1335, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741855_1031, duration: 3263205
2015-12-04 22:15:41,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741855_1031, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:41,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741856_1032 src: /127.0.0.1:46089 dest: /127.0.0.1:50010
2015-12-04 22:15:41,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46089, dest: /127.0.0.1:50010, bytes: 318, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741856_1032, duration: 2659136
2015-12-04 22:15:41,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741856_1032, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:41,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741857_1033 src: /127.0.0.1:46090 dest: /127.0.0.1:50010
2015-12-04 22:15:41,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46090, dest: /127.0.0.1:50010, bytes: 872, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741857_1033, duration: 1447717
2015-12-04 22:15:41,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741857_1033, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:41,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741858_1034 src: /127.0.0.1:46091 dest: /127.0.0.1:50010
2015-12-04 22:15:41,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46091, dest: /127.0.0.1:50010, bytes: 3670, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741858_1034, duration: 1364351
2015-12-04 22:15:41,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741858_1034, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:41,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741859_1035 src: /127.0.0.1:46092 dest: /127.0.0.1:50010
2015-12-04 22:15:41,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46092, dest: /127.0.0.1:50010, bytes: 4236, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741859_1035, duration: 3839346
2015-12-04 22:15:41,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741859_1035, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:41,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741860_1036 src: /127.0.0.1:46093 dest: /127.0.0.1:50010
2015-12-04 22:15:41,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46093, dest: /127.0.0.1:50010, bytes: 2490, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741860_1036, duration: 1208559
2015-12-04 22:15:41,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741860_1036, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:41,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741861_1037 src: /127.0.0.1:46094 dest: /127.0.0.1:50010
2015-12-04 22:15:41,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46094, dest: /127.0.0.1:50010, bytes: 2598, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741861_1037, duration: 3361977
2015-12-04 22:15:41,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741861_1037, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:41,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741862_1038 src: /127.0.0.1:46095 dest: /127.0.0.1:50010
2015-12-04 22:15:41,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46095, dest: /127.0.0.1:50010, bytes: 9683, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741862_1038, duration: 1375165
2015-12-04 22:15:41,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741862_1038, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:41,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741863_1039 src: /127.0.0.1:46096 dest: /127.0.0.1:50010
2015-12-04 22:15:41,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46096, dest: /127.0.0.1:50010, bytes: 855, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741863_1039, duration: 2021180
2015-12-04 22:15:41,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741863_1039, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:41,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741864_1040 src: /127.0.0.1:46097 dest: /127.0.0.1:50010
2015-12-04 22:15:41,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46097, dest: /127.0.0.1:50010, bytes: 1449, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741864_1040, duration: 1615350
2015-12-04 22:15:41,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741864_1040, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:41,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741865_1041 src: /127.0.0.1:46098 dest: /127.0.0.1:50010
2015-12-04 22:15:41,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46098, dest: /127.0.0.1:50010, bytes: 1657, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741865_1041, duration: 2198172
2015-12-04 22:15:41,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741865_1041, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:41,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741866_1042 src: /127.0.0.1:46099 dest: /127.0.0.1:50010
2015-12-04 22:15:42,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46099, dest: /127.0.0.1:50010, bytes: 21, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741866_1042, duration: 1718379
2015-12-04 22:15:42,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741866_1042, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:42,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741867_1043 src: /127.0.0.1:46100 dest: /127.0.0.1:50010
2015-12-04 22:15:42,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46100, dest: /127.0.0.1:50010, bytes: 620, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741867_1043, duration: 1553204
2015-12-04 22:15:42,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741867_1043, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:42,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741868_1044 src: /127.0.0.1:46101 dest: /127.0.0.1:50010
2015-12-04 22:15:42,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46101, dest: /127.0.0.1:50010, bytes: 3523, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741868_1044, duration: 1340618
2015-12-04 22:15:42,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741868_1044, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:42,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741869_1045 src: /127.0.0.1:46102 dest: /127.0.0.1:50010
2015-12-04 22:15:42,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46102, dest: /127.0.0.1:50010, bytes: 1325, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741869_1045, duration: 2203776
2015-12-04 22:15:42,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741869_1045, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:42,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741870_1046 src: /127.0.0.1:46103 dest: /127.0.0.1:50010
2015-12-04 22:15:42,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46103, dest: /127.0.0.1:50010, bytes: 1631, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741870_1046, duration: 6459478
2015-12-04 22:15:42,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741870_1046, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:42,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741871_1047 src: /127.0.0.1:46104 dest: /127.0.0.1:50010
2015-12-04 22:15:42,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46104, dest: /127.0.0.1:50010, bytes: 5511, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741871_1047, duration: 1613899
2015-12-04 22:15:42,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741871_1047, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:42,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741872_1048 src: /127.0.0.1:46105 dest: /127.0.0.1:50010
2015-12-04 22:15:42,161 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46105, dest: /127.0.0.1:50010, bytes: 11291, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741872_1048, duration: 2194191
2015-12-04 22:15:42,161 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741872_1048, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:42,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741873_1049 src: /127.0.0.1:46106 dest: /127.0.0.1:50010
2015-12-04 22:15:42,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46106, dest: /127.0.0.1:50010, bytes: 938, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741873_1049, duration: 1538756
2015-12-04 22:15:42,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741873_1049, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:42,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741874_1050 src: /127.0.0.1:46107 dest: /127.0.0.1:50010
2015-12-04 22:15:42,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46107, dest: /127.0.0.1:50010, bytes: 1383, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741874_1050, duration: 3907559
2015-12-04 22:15:42,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741874_1050, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:42,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741875_1051 src: /127.0.0.1:46108 dest: /127.0.0.1:50010
2015-12-04 22:15:42,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46108, dest: /127.0.0.1:50010, bytes: 4113, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741875_1051, duration: 1547049
2015-12-04 22:15:42,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741875_1051, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:42,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741876_1052 src: /127.0.0.1:46109 dest: /127.0.0.1:50010
2015-12-04 22:15:42,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46109, dest: /127.0.0.1:50010, bytes: 758, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741876_1052, duration: 2818771
2015-12-04 22:15:42,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741876_1052, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:42,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741877_1053 src: /127.0.0.1:46110 dest: /127.0.0.1:50010
2015-12-04 22:15:42,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46110, dest: /127.0.0.1:50010, bytes: 10, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741877_1053, duration: 1077701
2015-12-04 22:15:42,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741877_1053, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:42,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741878_1054 src: /127.0.0.1:46111 dest: /127.0.0.1:50010
2015-12-04 22:15:42,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46111, dest: /127.0.0.1:50010, bytes: 2316, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741878_1054, duration: 7318539
2015-12-04 22:15:42,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741878_1054, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:42,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741879_1055 src: /127.0.0.1:46112 dest: /127.0.0.1:50010
2015-12-04 22:15:42,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46112, dest: /127.0.0.1:50010, bytes: 2268, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741879_1055, duration: 1429555
2015-12-04 22:15:42,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741879_1055, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:42,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741880_1056 src: /127.0.0.1:46113 dest: /127.0.0.1:50010
2015-12-04 22:15:42,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46113, dest: /127.0.0.1:50010, bytes: 2237, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741880_1056, duration: 2619691
2015-12-04 22:15:42,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741880_1056, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:42,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741881_1057 src: /127.0.0.1:46114 dest: /127.0.0.1:50010
2015-12-04 22:15:42,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46114, dest: /127.0.0.1:50010, bytes: 4567, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741881_1057, duration: 1235597
2015-12-04 22:15:42,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741881_1057, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:42,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741882_1058 src: /127.0.0.1:46115 dest: /127.0.0.1:50010
2015-12-04 22:15:42,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46115, dest: /127.0.0.1:50010, bytes: 690, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2051444295_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741882_1058, duration: 1228272
2015-12-04 22:15:42,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741882_1058, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:15:48,628 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741880_1056
2015-12-04 22:15:48,630 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741862_1038
2015-12-04 22:15:48,631 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741877_1053
2015-12-04 22:15:48,633 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741859_1035
2015-12-04 22:15:48,634 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741876_1052
2015-12-04 22:15:48,636 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741864_1040
2015-12-04 22:15:48,637 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741854_1030
2015-12-04 22:15:48,637 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741863_1039
2015-12-04 22:15:48,639 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741872_1048
2015-12-04 22:15:48,639 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741882_1058
2015-12-04 22:15:48,640 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741857_1033
2015-12-04 22:15:48,640 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741870_1046
2015-12-04 22:15:48,641 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741866_1042
2015-12-04 22:15:48,641 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741871_1047
2015-12-04 22:15:48,642 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741867_1043
2015-12-04 22:15:48,643 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741868_1044
2015-12-04 22:15:48,644 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741858_1034
2015-12-04 22:15:48,645 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741878_1054
2015-12-04 22:15:48,645 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741855_1031
2015-12-04 22:15:48,646 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741861_1037
2015-12-04 22:15:48,646 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741856_1032
2015-12-04 22:20:29,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741883_1059 src: /127.0.0.1:46125 dest: /127.0.0.1:50010
2015-12-04 22:20:29,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46125, dest: /127.0.0.1:50010, bytes: 437, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_132275941_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741883_1059, duration: 34243102
2015-12-04 22:20:29,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741883_1059, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:20:30,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1952917173-127.0.1.1-1449292642740:blk_1073741884_1060 src: /127.0.0.1:46126 dest: /127.0.0.1:50010
2015-12-04 22:20:30,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46126, dest: /127.0.0.1:50010, bytes: 197, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_132275941_1, offset: 0, srvID: 7b1d6000-b128-42b1-b8a1-6bfae41da4eb, blockid: BP-1952917173-127.0.1.1-1449292642740:blk_1073741884_1060, duration: 18119444
2015-12-04 22:20:30,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1952917173-127.0.1.1-1449292642740:blk_1073741884_1060, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-04 22:20:31,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741883_1059 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741883 for deletion
2015-12-04 22:20:31,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1952917173-127.0.1.1-1449292642740 blk_1073741883_1059 file /tmp/hadoop-parallels/dfs/data/current/BP-1952917173-127.0.1.1-1449292642740/current/finalized/subdir0/subdir0/blk_1073741883
2015-12-04 22:20:38,681 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1952917173-127.0.1.1-1449292642740:blk_1073741884_1060
2015-12-04 22:32:23,746 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-04 22:32:23,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 13:05:21,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 13:05:21,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 13:05:22,144 FATAL org.apache.hadoop.conf.Configuration: error parsing conf mapred-site.xml
org.xml.sax.SAXParseException; systemId: file:/home/parallels/Development/hadoop-2.6.0/etc/hadoop/mapred-site.xml; lineNumber: 23; columnNumber: 5; The element type "proterty" must be terminated by the matching end-tag "</proterty>".
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2352)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2340)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2411)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2364)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2281)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:888)
	at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:907)
	at org.apache.hadoop.conf.Configuration.getLong(Configuration.java:1180)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:77)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:66)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:271)
	at org.apache.hadoop.security.UserGroupInformation.setConfiguration(UserGroupInformation.java:299)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2152)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2202)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2378)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2402)
2015-12-05 13:05:22,146 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.lang.RuntimeException: org.xml.sax.SAXParseException; systemId: file:/home/parallels/Development/hadoop-2.6.0/etc/hadoop/mapred-site.xml; lineNumber: 23; columnNumber: 5; The element type "proterty" must be terminated by the matching end-tag "</proterty>".
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2517)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2364)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2281)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:888)
	at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:907)
	at org.apache.hadoop.conf.Configuration.getLong(Configuration.java:1180)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:77)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:66)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:271)
	at org.apache.hadoop.security.UserGroupInformation.setConfiguration(UserGroupInformation.java:299)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2152)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2202)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2378)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2402)
Caused by: org.xml.sax.SAXParseException; systemId: file:/home/parallels/Development/hadoop-2.6.0/etc/hadoop/mapred-site.xml; lineNumber: 23; columnNumber: 5; The element type "proterty" must be terminated by the matching end-tag "</proterty>".
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2352)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2340)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2411)
	... 14 more
2015-12-05 13:05:22,148 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 13:05:22,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 13:07:12,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 13:07:12,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 13:07:13,253 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 13:07:13,312 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 13:07:13,312 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-05 13:07:13,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-05 13:07:13,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-05 13:07:13,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-05 13:07:13,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-05 13:07:13,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-05 13:07:13,454 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 13:07:13,457 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-05 13:07:13,467 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 13:07:13,468 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-05 13:07:13,469 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 13:07:13,469 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 13:07:13,482 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 13:07:13,487 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-05 13:07:13,487 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 13:07:13,694 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-05 13:07:13,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-05 13:07:13,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-05 13:07:13,913 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-05 13:07:13,934 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-05 13:07:13,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-05 13:07:13,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-05 13:07:14,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-05 13:07:14,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-05 13:07:14,033 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-05 13:07:14,034 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-05 13:07:15,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:16,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:17,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:18,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:19,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:20,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:21,160 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:22,160 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:23,161 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:24,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:24,164 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:07:30,166 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:31,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:32,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:33,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:34,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:35,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:36,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:37,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:38,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:39,174 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:39,176 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:07:45,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:46,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:47,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:48,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:49,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:50,185 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:51,185 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:52,187 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:53,189 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:54,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:07:54,192 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:08:00,193 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:01,194 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:02,195 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:03,198 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:04,200 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:05,201 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:06,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:07,203 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:08,205 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:09,207 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:09,207 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:08:15,209 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:16,210 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:17,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:18,214 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:19,214 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:20,215 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:21,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:22,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:23,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:24,221 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:24,222 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:08:30,223 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:31,224 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:32,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:33,226 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:34,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:35,228 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:36,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:37,230 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:38,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:39,232 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:39,233 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:08:45,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:46,236 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:47,237 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:48,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:49,239 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:50,241 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:51,242 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:52,242 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:53,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:54,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:08:54,246 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:09:00,248 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:01,248 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:02,250 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:03,251 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:04,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:05,254 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:06,255 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:07,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:08,257 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:09,257 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:09,258 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:09:15,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:16,262 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:17,263 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:18,264 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:19,265 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:20,266 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:21,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:22,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:23,269 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:24,270 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:24,271 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:09:30,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:31,275 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:32,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:33,278 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:34,279 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:35,280 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:36,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:37,283 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:38,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:39,285 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:39,286 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:09:45,288 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:46,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:47,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:48,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:49,293 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:50,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:51,295 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:52,296 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:53,298 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:54,299 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:09:54,300 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:10:00,302 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:01,303 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:02,304 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:03,306 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:04,306 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:05,307 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:06,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:07,310 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:08,311 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:09,313 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:09,315 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:10:15,317 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:16,318 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:17,319 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:18,320 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:19,321 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:20,321 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:21,322 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:22,323 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:23,324 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:24,325 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:24,326 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:10:30,329 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:31,330 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:32,331 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:33,332 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:34,334 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:35,335 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:36,336 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:37,337 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:38,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:39,339 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:39,340 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:10:45,342 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:46,343 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:47,343 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:48,344 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:49,345 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:50,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:51,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:52,348 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:53,349 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:54,349 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:10:54,350 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:11:00,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:01,354 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:02,354 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:03,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:04,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:05,357 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:06,359 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:07,360 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:08,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:09,363 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:09,364 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:11:15,367 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:16,368 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:17,369 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:18,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:19,371 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:20,372 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:21,374 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:22,375 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:23,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:24,377 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:24,378 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:11:30,379 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:31,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:32,383 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:33,384 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:34,385 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:35,387 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:36,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:37,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:38,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:39,390 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:39,391 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:11:45,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:46,395 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:47,395 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:48,396 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:49,397 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:50,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:51,399 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:52,401 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:53,403 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:54,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:11:54,405 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:12:00,407 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:01,408 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:02,410 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:03,410 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:04,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:05,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:06,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:07,414 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:08,414 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:09,416 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:09,417 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:12:15,418 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:16,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:17,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:18,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:19,422 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:20,424 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:21,425 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:22,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:23,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:24,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:24,429 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:12:30,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:31,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:32,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:33,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:34,434 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:35,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:36,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:37,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:38,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:39,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:39,443 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:12:45,446 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:46,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:47,448 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:48,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:49,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:50,452 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:51,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:52,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:53,457 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:54,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:12:54,460 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:13:00,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:01,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:02,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:03,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:04,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:05,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:06,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:07,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:08,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:09,472 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:09,473 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:13:15,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:16,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:17,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:18,479 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:19,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:20,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:21,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:22,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:23,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:24,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:24,490 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:13:30,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:31,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:32,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:33,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:34,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:35,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:36,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:37,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:38,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:39,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:39,505 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:13:45,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:46,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:47,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:48,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:49,511 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:50,511 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:51,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:52,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:53,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:54,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:13:54,515 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:14:00,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:01,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:02,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:03,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:04,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:05,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:06,525 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:07,526 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:08,527 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:09,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:09,530 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:14:15,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:16,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:17,535 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:18,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:19,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:20,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:21,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:22,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:23,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:24,543 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:24,544 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:14:30,545 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:31,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:32,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:33,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:34,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:35,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:36,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:37,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:38,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:39,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:39,557 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:14:45,560 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:46,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:47,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:48,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:49,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:50,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:51,566 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:52,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:53,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:54,570 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:14:54,571 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:15:00,573 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:01,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:02,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:03,577 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:04,578 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:05,580 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:06,582 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:07,583 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:08,584 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:09,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:09,586 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:15:15,588 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:16,589 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:17,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:18,592 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:19,593 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:20,594 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:21,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:22,597 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:23,597 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:24,598 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:24,599 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:15:30,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:31,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:32,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:33,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:34,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:35,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:36,609 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:37,610 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:38,611 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:39,611 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:15:39,612 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 13:15:44,893 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-05 13:15:44,912 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-parallels/dfs/data/in_use.lock acquired by nodename 17526@ubuntu
2015-12-05 13:15:44,913 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-parallels/dfs/data is not formatted for BP-1595073404-127.0.1.1-1449350125252
2015-12-05 13:15:44,913 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-12-05 13:15:44,966 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1595073404-127.0.1.1-1449350125252
2015-12-05 13:15:44,966 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-12-05 13:15:44,967 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-parallels/dfs/data/current/BP-1595073404-127.0.1.1-1449350125252 is not formatted.
2015-12-05 13:15:44,967 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-12-05 13:15:44,967 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1595073404-127.0.1.1-1449350125252 directory /tmp/hadoop-parallels/dfs/data/current/BP-1595073404-127.0.1.1-1449350125252/current
2015-12-05 13:15:44,969 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-05 13:15:44,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=889546093;bpid=BP-1595073404-127.0.1.1-1449350125252;lv=-56;nsInfo=lv=-60;cid=CID-8e6ab86f-c97b-4c9f-9515-70a87b98ae37;nsid=889546093;c=0;bpid=BP-1595073404-127.0.1.1-1449350125252;dnuuid=null
2015-12-05 13:15:44,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID aac9c00d-3ead-4052-8364-863df82ca9d6
2015-12-05 13:15:45,031 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-parallels/dfs/data/current
2015-12-05 13:15:45,032 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-parallels/dfs/data/current, StorageType: DISK
2015-12-05 13:15:45,037 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-05 13:15:45,041 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1449368899041 with interval 21600000
2015-12-05 13:15:45,043 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1595073404-127.0.1.1-1449350125252
2015-12-05 13:15:45,043 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1595073404-127.0.1.1-1449350125252 on volume /tmp/hadoop-parallels/dfs/data/current...
2015-12-05 13:15:45,079 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1595073404-127.0.1.1-1449350125252 on /tmp/hadoop-parallels/dfs/data/current: 35ms
2015-12-05 13:15:45,079 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1595073404-127.0.1.1-1449350125252: 36ms
2015-12-05 13:15:45,083 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1595073404-127.0.1.1-1449350125252 on volume /tmp/hadoop-parallels/dfs/data/current...
2015-12-05 13:15:45,083 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1595073404-127.0.1.1-1449350125252 on volume /tmp/hadoop-parallels/dfs/data/current: 0ms
2015-12-05 13:15:45,083 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-12-05 13:15:45,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1595073404-127.0.1.1-1449350125252 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-05 13:15:45,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1595073404-127.0.1.1-1449350125252 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-05 13:15:45,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-05 13:15:45,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1595073404-127.0.1.1-1449350125252 (Datanode Uuid aac9c00d-3ead-4052-8364-863df82ca9d6) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-12-05 13:15:45,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1595073404-127.0.1.1-1449350125252 (Datanode Uuid aac9c00d-3ead-4052-8364-863df82ca9d6) service to localhost/127.0.0.1:9000
2015-12-05 13:15:45,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 48 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@26484683
2015-12-05 13:15:45,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1595073404-127.0.1.1-1449350125252
2015-12-05 13:15:45,319 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-12-05 13:15:45,319 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:15:45,321 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-12-05 13:15:45,321 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-12-05 13:15:45,321 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1595073404-127.0.1.1-1449350125252
2015-12-05 13:15:45,325 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1595073404-127.0.1.1-1449350125252 to blockPoolScannerMap, new size=1
2015-12-05 13:17:21,189 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "ubuntu/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-12-05 13:17:25,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:17:26,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:17:27,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:17:28,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:17:28,710 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 13:17:28,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 13:18:15,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 13:18:15,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 13:18:16,343 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 13:18:16,399 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 13:18:16,399 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-05 13:18:16,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-05 13:18:16,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-05 13:18:16,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-05 13:18:16,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-05 13:18:16,440 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-05 13:18:16,537 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 13:18:16,540 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-05 13:18:16,548 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 13:18:16,550 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-05 13:18:16,550 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 13:18:16,550 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 13:18:16,563 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 13:18:16,567 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-05 13:18:16,568 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 13:18:16,755 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-05 13:18:16,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-05 13:18:16,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-05 13:18:16,983 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-05 13:18:16,997 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-05 13:18:17,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-05 13:18:17,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-05 13:18:17,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-05 13:18:17,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-05 13:18:17,079 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-05 13:18:17,084 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-05 13:18:17,378 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-05 13:18:17,383 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-parallels/dfs/data/in_use.lock acquired by nodename 21268@ubuntu
2015-12-05 13:18:17,450 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1595073404-127.0.1.1-1449350125252
2015-12-05 13:18:17,450 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-12-05 13:18:17,450 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-05 13:18:17,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=889546093;bpid=BP-1595073404-127.0.1.1-1449350125252;lv=-56;nsInfo=lv=-60;cid=CID-8e6ab86f-c97b-4c9f-9515-70a87b98ae37;nsid=889546093;c=0;bpid=BP-1595073404-127.0.1.1-1449350125252;dnuuid=aac9c00d-3ead-4052-8364-863df82ca9d6
2015-12-05 13:18:17,485 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-parallels/dfs/data/current
2015-12-05 13:18:17,487 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-parallels/dfs/data/current, StorageType: DISK
2015-12-05 13:18:17,518 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-05 13:18:17,521 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1449362932521 with interval 21600000
2015-12-05 13:18:17,527 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1595073404-127.0.1.1-1449350125252
2015-12-05 13:18:17,528 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1595073404-127.0.1.1-1449350125252 on volume /tmp/hadoop-parallels/dfs/data/current...
2015-12-05 13:18:17,539 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /tmp/hadoop-parallels/dfs/data/current/BP-1595073404-127.0.1.1-1449350125252/current: 24576
2015-12-05 13:18:17,541 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1595073404-127.0.1.1-1449350125252 on /tmp/hadoop-parallels/dfs/data/current: 13ms
2015-12-05 13:18:17,541 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1595073404-127.0.1.1-1449350125252: 14ms
2015-12-05 13:18:17,541 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1595073404-127.0.1.1-1449350125252 on volume /tmp/hadoop-parallels/dfs/data/current...
2015-12-05 13:18:17,541 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1595073404-127.0.1.1-1449350125252 on volume /tmp/hadoop-parallels/dfs/data/current: 0ms
2015-12-05 13:18:17,541 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 0ms
2015-12-05 13:18:17,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1595073404-127.0.1.1-1449350125252 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-05 13:18:17,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1595073404-127.0.1.1-1449350125252 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-05 13:18:17,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-05 13:18:17,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1595073404-127.0.1.1-1449350125252 (Datanode Uuid aac9c00d-3ead-4052-8364-863df82ca9d6) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=4
2015-12-05 13:18:17,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1595073404-127.0.1.1-1449350125252 (Datanode Uuid aac9c00d-3ead-4052-8364-863df82ca9d6) service to localhost/127.0.0.1:9000
2015-12-05 13:18:17,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 66 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@c99c7ce
2015-12-05 13:18:17,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1595073404-127.0.1.1-1449350125252
2015-12-05 13:18:17,744 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-12-05 13:18:17,744 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 13:18:17,745 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-12-05 13:18:17,745 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-12-05 13:18:17,747 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1595073404-127.0.1.1-1449350125252
2015-12-05 13:18:17,751 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1595073404-127.0.1.1-1449350125252 to blockPoolScannerMap, new size=1
2015-12-05 13:25:04,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741825_1001 src: /127.0.0.1:39648 dest: /127.0.0.1:50010
2015-12-05 13:25:05,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39648, dest: /127.0.0.1:50010, bytes: 4436, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741825_1001, duration: 207988509
2015-12-05 13:25:05,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:05,742 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741826_1002 src: /127.0.0.1:39649 dest: /127.0.0.1:50010
2015-12-05 13:25:05,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39649, dest: /127.0.0.1:50010, bytes: 1335, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741826_1002, duration: 5047099
2015-12-05 13:25:05,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:05,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741827_1003 src: /127.0.0.1:39650 dest: /127.0.0.1:50010
2015-12-05 13:25:05,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39650, dest: /127.0.0.1:50010, bytes: 318, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741827_1003, duration: 5535984
2015-12-05 13:25:05,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:05,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741828_1004 src: /127.0.0.1:39651 dest: /127.0.0.1:50010
2015-12-05 13:25:05,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39651, dest: /127.0.0.1:50010, bytes: 872, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741828_1004, duration: 8793431
2015-12-05 13:25:05,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:05,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741829_1005 src: /127.0.0.1:39652 dest: /127.0.0.1:50010
2015-12-05 13:25:05,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39652, dest: /127.0.0.1:50010, bytes: 3670, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741829_1005, duration: 2310843
2015-12-05 13:25:05,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:05,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741830_1006 src: /127.0.0.1:39653 dest: /127.0.0.1:50010
2015-12-05 13:25:05,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39653, dest: /127.0.0.1:50010, bytes: 4236, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741830_1006, duration: 6917285
2015-12-05 13:25:05,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:06,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741831_1007 src: /127.0.0.1:39654 dest: /127.0.0.1:50010
2015-12-05 13:25:06,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39654, dest: /127.0.0.1:50010, bytes: 2490, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741831_1007, duration: 12804628
2015-12-05 13:25:06,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:06,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741832_1008 src: /127.0.0.1:39655 dest: /127.0.0.1:50010
2015-12-05 13:25:06,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39655, dest: /127.0.0.1:50010, bytes: 2598, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741832_1008, duration: 3005077
2015-12-05 13:25:06,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741832_1008, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:06,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741833_1009 src: /127.0.0.1:39656 dest: /127.0.0.1:50010
2015-12-05 13:25:06,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39656, dest: /127.0.0.1:50010, bytes: 9683, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741833_1009, duration: 3756741
2015-12-05 13:25:06,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741833_1009, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:06,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741834_1010 src: /127.0.0.1:39657 dest: /127.0.0.1:50010
2015-12-05 13:25:06,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39657, dest: /127.0.0.1:50010, bytes: 855, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741834_1010, duration: 2901530
2015-12-05 13:25:06,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741834_1010, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:06,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741835_1011 src: /127.0.0.1:39658 dest: /127.0.0.1:50010
2015-12-05 13:25:06,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39658, dest: /127.0.0.1:50010, bytes: 1449, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741835_1011, duration: 3347916
2015-12-05 13:25:06,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741835_1011, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:06,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741836_1012 src: /127.0.0.1:39659 dest: /127.0.0.1:50010
2015-12-05 13:25:06,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39659, dest: /127.0.0.1:50010, bytes: 1657, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741836_1012, duration: 5471708
2015-12-05 13:25:06,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741836_1012, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:06,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741837_1013 src: /127.0.0.1:39660 dest: /127.0.0.1:50010
2015-12-05 13:25:06,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39660, dest: /127.0.0.1:50010, bytes: 21, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741837_1013, duration: 2604107
2015-12-05 13:25:06,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741837_1013, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:06,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741838_1014 src: /127.0.0.1:39661 dest: /127.0.0.1:50010
2015-12-05 13:25:06,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39661, dest: /127.0.0.1:50010, bytes: 620, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741838_1014, duration: 3549851
2015-12-05 13:25:06,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741838_1014, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:07,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741839_1015 src: /127.0.0.1:39662 dest: /127.0.0.1:50010
2015-12-05 13:25:07,017 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39662, dest: /127.0.0.1:50010, bytes: 3523, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741839_1015, duration: 2537882
2015-12-05 13:25:07,017 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741839_1015, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:07,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741840_1016 src: /127.0.0.1:39663 dest: /127.0.0.1:50010
2015-12-05 13:25:07,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39663, dest: /127.0.0.1:50010, bytes: 1325, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741840_1016, duration: 2668725
2015-12-05 13:25:07,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741840_1016, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:07,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741841_1017 src: /127.0.0.1:39664 dest: /127.0.0.1:50010
2015-12-05 13:25:07,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39664, dest: /127.0.0.1:50010, bytes: 1631, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741841_1017, duration: 3518484
2015-12-05 13:25:07,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741841_1017, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:07,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741842_1018 src: /127.0.0.1:39665 dest: /127.0.0.1:50010
2015-12-05 13:25:07,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39665, dest: /127.0.0.1:50010, bytes: 5511, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741842_1018, duration: 2475888
2015-12-05 13:25:07,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741842_1018, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:07,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741843_1019 src: /127.0.0.1:39666 dest: /127.0.0.1:50010
2015-12-05 13:25:07,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39666, dest: /127.0.0.1:50010, bytes: 11291, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741843_1019, duration: 8178401
2015-12-05 13:25:07,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741843_1019, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:07,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741844_1020 src: /127.0.0.1:39667 dest: /127.0.0.1:50010
2015-12-05 13:25:07,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39667, dest: /127.0.0.1:50010, bytes: 938, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741844_1020, duration: 3815407
2015-12-05 13:25:07,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741844_1020, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:07,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741845_1021 src: /127.0.0.1:39668 dest: /127.0.0.1:50010
2015-12-05 13:25:07,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39668, dest: /127.0.0.1:50010, bytes: 1383, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741845_1021, duration: 9221755
2015-12-05 13:25:07,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741845_1021, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:07,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741846_1022 src: /127.0.0.1:39669 dest: /127.0.0.1:50010
2015-12-05 13:25:07,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39669, dest: /127.0.0.1:50010, bytes: 4113, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741846_1022, duration: 11109688
2015-12-05 13:25:07,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741846_1022, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:07,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741847_1023 src: /127.0.0.1:39670 dest: /127.0.0.1:50010
2015-12-05 13:25:07,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39670, dest: /127.0.0.1:50010, bytes: 850, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741847_1023, duration: 3556490
2015-12-05 13:25:07,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741847_1023, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:07,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741848_1024 src: /127.0.0.1:39671 dest: /127.0.0.1:50010
2015-12-05 13:25:07,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39671, dest: /127.0.0.1:50010, bytes: 758, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741848_1024, duration: 5938290
2015-12-05 13:25:07,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741848_1024, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:07,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741849_1025 src: /127.0.0.1:39672 dest: /127.0.0.1:50010
2015-12-05 13:25:07,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39672, dest: /127.0.0.1:50010, bytes: 10, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741849_1025, duration: 4878353
2015-12-05 13:25:07,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741849_1025, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:07,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741850_1026 src: /127.0.0.1:39673 dest: /127.0.0.1:50010
2015-12-05 13:25:07,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39673, dest: /127.0.0.1:50010, bytes: 2316, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741850_1026, duration: 2694733
2015-12-05 13:25:07,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741850_1026, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:07,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741851_1027 src: /127.0.0.1:39674 dest: /127.0.0.1:50010
2015-12-05 13:25:07,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39674, dest: /127.0.0.1:50010, bytes: 2268, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741851_1027, duration: 2448335
2015-12-05 13:25:07,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741851_1027, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:07,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741852_1028 src: /127.0.0.1:39675 dest: /127.0.0.1:50010
2015-12-05 13:25:07,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39675, dest: /127.0.0.1:50010, bytes: 2237, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741852_1028, duration: 1995801
2015-12-05 13:25:07,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741852_1028, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:07,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741853_1029 src: /127.0.0.1:39676 dest: /127.0.0.1:50010
2015-12-05 13:25:07,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39676, dest: /127.0.0.1:50010, bytes: 4567, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741853_1029, duration: 2248402
2015-12-05 13:25:07,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741853_1029, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:07,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741854_1030 src: /127.0.0.1:39677 dest: /127.0.0.1:50010
2015-12-05 13:25:07,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39677, dest: /127.0.0.1:50010, bytes: 800, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2054405698_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741854_1030, duration: 2084156
2015-12-05 13:25:07,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741854_1030, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:25:12,589 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1595073404-127.0.1.1-1449350125252:blk_1073741844_1020
2015-12-05 13:25:12,600 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1595073404-127.0.1.1-1449350125252:blk_1073741839_1015
2015-12-05 13:25:12,601 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1595073404-127.0.1.1-1449350125252:blk_1073741846_1022
2015-12-05 13:25:12,602 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1595073404-127.0.1.1-1449350125252:blk_1073741828_1004
2015-12-05 13:25:12,603 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1595073404-127.0.1.1-1449350125252:blk_1073741838_1014
2015-12-05 13:25:12,604 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1595073404-127.0.1.1-1449350125252:blk_1073741832_1008
2015-12-05 13:25:12,605 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1595073404-127.0.1.1-1449350125252:blk_1073741851_1027
2015-12-05 13:25:12,606 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1595073404-127.0.1.1-1449350125252:blk_1073741842_1018
2015-12-05 13:25:12,607 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1595073404-127.0.1.1-1449350125252:blk_1073741835_1011
2015-12-05 13:26:56,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741855_1031 src: /127.0.0.1:39697 dest: /127.0.0.1:50010
2015-12-05 13:26:56,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39697, dest: /127.0.0.1:50010, bytes: 3751, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1183674985_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741855_1031, duration: 39834058
2015-12-05 13:26:56,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741855_1031, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:26:56,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741856_1032 src: /127.0.0.1:39698 dest: /127.0.0.1:50010
2015-12-05 13:26:56,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39698, dest: /127.0.0.1:50010, bytes: 423, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1183674985_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741856_1032, duration: 1858742
2015-12-05 13:26:56,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741856_1032, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:26:56,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741857_1033 src: /127.0.0.1:39699 dest: /127.0.0.1:50010
2015-12-05 13:26:56,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39699, dest: /127.0.0.1:50010, bytes: 87600, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1183674985_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741857_1033, duration: 2529715
2015-12-05 13:26:56,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741857_1033, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:27:02,807 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1595073404-127.0.1.1-1449350125252:blk_1073741857_1033
2015-12-05 13:27:02,809 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1595073404-127.0.1.1-1449350125252:blk_1073741856_1032
2015-12-05 13:27:02,810 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1595073404-127.0.1.1-1449350125252:blk_1073741855_1031
2015-12-05 13:27:08,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741858_1034 src: /127.0.0.1:39708 dest: /127.0.0.1:50010
2015-12-05 13:27:08,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39708, dest: /127.0.0.1:50010, bytes: 104194, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2041276604_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741858_1034, duration: 57617795
2015-12-05 13:27:08,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741858_1034, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:27:18,478 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1595073404-127.0.1.1-1449350125252:blk_1073741858_1034
2015-12-05 13:27:50,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1595073404-127.0.1.1-1449350125252:blk_1073741859_1035 src: /127.0.0.1:39750 dest: /127.0.0.1:50010
2015-12-05 13:28:13,213 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4441ms
GC pool 'Copy' had collection(s): count=1 time=4646ms
2015-12-05 13:28:14,333 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "ubuntu/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-12-05 13:28:16,866 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:17,943 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:19,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:20,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:21,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:22,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:23,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:24,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:25,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:26,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:26,165 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:28:27,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:28,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:29,288 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:30,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:31,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:32,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:33,292 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:34,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:35,296 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:36,297 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:36,308 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:28:37,384 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:38,387 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:39,390 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:40,393 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:41,396 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:42,399 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:43,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:44,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:45,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:46,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:46,418 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:28:47,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:48,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:49,428 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:50,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:51,430 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:52,440 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:53,441 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:54,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:55,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:56,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:56,544 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:28:57,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:58,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:28:59,588 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:00,591 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:01,591 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:02,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:03,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:04,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:05,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:06,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:06,622 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:29:07,628 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:08,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:09,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:10,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:11,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:12,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:13,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:14,653 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:15,654 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:16,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:16,724 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:29:17,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:18,825 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:19,837 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:20,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:21,843 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:22,845 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:23,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:24,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:25,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:26,851 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:26,852 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:29:27,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:28,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:29,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:30,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39750, dest: /127.0.0.1:50010, bytes: 195248, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2041276604_1, offset: 0, srvID: aac9c00d-3ead-4052-8364-863df82ca9d6, blockid: BP-1595073404-127.0.1.1-1449350125252:blk_1073741859_1035, duration: 99429829178
2015-12-05 13:29:30,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1595073404-127.0.1.1-1449350125252:blk_1073741859_1035, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 13:29:30,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:31,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:32,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:33,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:34,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:35,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:36,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:36,886 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:29:37,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:38,029 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1595073404-127.0.1.1-1449350125252:blk_1073741859_1035
2015-12-05 13:29:38,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:39,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:40,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:41,898 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:42,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:43,900 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:44,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:45,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:46,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:46,904 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:29:47,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:48,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:49,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:50,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:51,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:52,914 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:53,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:54,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:55,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:56,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:56,920 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:29:57,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:58,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:29:59,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:00,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:01,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:02,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:03,933 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:04,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:05,935 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:06,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:06,937 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:30:07,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:08,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:09,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:10,949 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:11,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:12,952 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:13,953 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:14,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:15,955 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:16,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:16,958 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:30:17,962 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:18,963 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:19,963 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:20,964 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:21,965 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:22,965 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:23,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:24,967 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:25,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:26,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:26,969 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:30:27,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:28,976 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:29,977 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:30,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:31,979 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:32,982 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:33,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:34,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:35,984 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:36,985 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:36,986 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:30:37,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:38,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:39,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:40,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:42,000 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:43,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:44,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:45,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:46,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:47,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:47,007 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:30:48,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:49,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:50,014 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:51,016 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:52,016 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:53,017 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:54,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:55,019 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:56,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:57,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:57,041 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:30:58,046 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:30:59,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:00,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:01,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:02,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:03,052 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:04,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:05,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:06,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:07,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:07,060 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:31:08,065 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:09,066 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:10,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:11,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:12,069 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:13,069 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:14,070 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:15,072 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:16,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:17,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:17,076 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:31:18,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:19,084 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:20,085 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:21,087 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:22,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:23,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:24,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:25,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:26,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:27,094 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:27,096 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:31:28,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:29,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:30,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:31,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:32,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:33,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:34,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:35,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:36,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:37,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:37,111 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:31:38,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:39,117 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:40,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:41,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:42,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:43,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:44,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:45,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:46,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:47,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:47,126 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:31:48,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:49,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:50,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:51,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:52,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:53,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:54,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:55,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:56,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:57,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:57,145 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-05 13:31:58,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 13:31:58,951 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 13:31:59,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 18:29:26,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 18:29:26,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 18:29:26,756 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 18:29:26,818 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 18:29:26,818 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-05 18:29:26,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-05 18:29:26,831 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-05 18:29:26,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-05 18:29:26,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-05 18:29:26,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-05 18:29:26,932 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 18:29:26,935 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-05 18:29:26,945 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 18:29:26,947 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-05 18:29:26,947 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 18:29:26,947 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 18:29:26,960 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 18:29:26,965 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-05 18:29:26,965 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 18:29:27,151 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-05 18:29:27,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-05 18:29:27,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-05 18:29:27,389 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-05 18:29:27,411 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-05 18:29:27,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-05 18:29:27,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-05 18:29:27,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-05 18:29:27,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-05 18:29:27,507 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-05 18:29:27,512 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-05 18:29:28,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:29,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:30,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:31,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:32,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:33,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:34,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:35,624 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:36,624 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:37,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:37,627 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:29:43,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:44,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:45,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:46,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:47,634 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:48,635 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:49,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:50,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:51,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:52,641 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:52,643 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:29:58,645 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:29:59,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:00,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:01,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:02,652 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:03,654 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:04,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:05,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:06,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:07,659 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:07,660 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:30:13,662 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:14,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:15,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:16,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:17,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:18,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:19,671 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:20,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:21,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:22,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:22,675 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:30:28,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:29,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:30,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:31,681 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:32,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:33,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:34,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:35,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:36,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:37,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:37,691 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:30:43,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:44,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:45,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:46,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:47,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:48,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:49,698 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:50,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:51,701 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:52,703 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:52,704 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:30:58,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:30:59,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:00,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:01,709 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:02,710 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:03,712 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:04,713 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:05,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:06,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:07,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:07,718 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:31:13,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:14,722 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:15,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:16,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:17,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:18,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:19,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:20,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:21,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:22,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:22,735 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:31:28,738 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:29,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:30,740 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:31,741 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:32,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:33,743 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:34,745 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:35,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:31:35,811 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 18:31:35,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 18:53:20,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 18:53:20,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 18:53:21,100 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 18:53:21,159 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 18:53:21,159 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-05 18:53:21,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-05 18:53:21,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-05 18:53:21,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-05 18:53:21,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-05 18:53:21,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-05 18:53:21,285 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 18:53:21,288 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-05 18:53:21,298 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 18:53:21,300 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-05 18:53:21,300 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 18:53:21,300 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 18:53:21,314 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 18:53:21,319 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-05 18:53:21,319 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 18:53:21,503 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-05 18:53:21,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-05 18:53:21,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-05 18:53:21,708 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-05 18:53:21,728 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-05 18:53:21,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-05 18:53:21,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-05 18:53:21,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-05 18:53:21,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-05 18:53:21,826 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-05 18:53:21,827 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-05 18:53:22,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:23,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:24,912 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:25,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:26,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:27,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:28,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:29,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:30,919 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:31,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:31,921 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:53:37,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:38,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:39,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:40,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:41,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:42,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:43,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:44,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:45,933 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:46,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:46,935 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:53:52,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:53,939 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:54,940 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:55,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:56,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:57,943 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:58,944 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:53:59,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:00,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:01,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:01,949 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:54:07,952 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:08,953 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:09,955 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:10,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:11,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:12,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:13,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:14,962 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:15,964 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:16,964 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:16,965 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:54:22,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:23,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:24,970 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:25,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:26,972 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:27,974 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:28,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:29,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:30,979 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:31,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:31,985 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:54:37,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:38,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:39,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:40,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:41,992 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:42,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:43,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:44,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:45,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:46,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:47,000 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:54:53,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:54,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:55,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:56,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:57,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:58,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:54:59,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:00,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:01,014 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:02,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:02,016 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:55:08,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:09,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:10,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:11,023 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:12,024 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:13,026 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:14,027 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:15,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:16,029 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:17,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:17,032 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:55:23,035 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:24,036 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:25,037 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:26,039 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:27,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:28,041 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:29,042 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:30,044 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:31,045 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:32,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:32,049 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:55:38,050 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:39,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:40,052 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:41,053 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:42,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:43,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:44,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:45,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:46,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:47,060 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:47,061 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:55:53,064 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:54,065 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:55,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:56,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:57,070 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:58,071 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:55:59,071 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:00,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:01,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:02,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:02,077 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:56:08,079 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:09,080 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:10,081 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:11,083 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:12,084 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:13,085 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:14,087 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:15,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:16,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:17,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:17,093 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:56:23,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:24,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:25,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:26,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:27,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:28,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:29,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:30,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:31,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:32,107 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:32,108 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:56:38,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:39,111 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:40,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:41,114 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:42,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:43,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:44,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:45,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:46,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:47,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:47,123 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:56:53,127 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:54,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:55,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:56,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:57,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:58,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:56:59,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:00,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:01,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:02,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:02,140 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:57:08,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:09,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:10,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:11,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:12,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:13,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:14,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:15,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:16,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:17,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:17,156 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:57:23,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:24,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:25,160 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:26,161 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:27,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:28,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:29,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:30,167 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:31,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:32,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:32,170 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:57:38,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:39,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:40,173 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:41,174 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:42,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:43,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:44,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:45,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:46,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:47,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:47,196 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:57:53,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:54,214 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:55,215 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:56,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:57,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:58,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:57:59,221 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:00,222 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:01,223 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:02,223 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:02,227 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:58:08,228 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:09,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:10,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:11,232 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:12,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:13,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:14,235 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:15,236 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:16,237 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:17,239 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:17,241 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:58:23,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:24,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:25,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:26,248 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:27,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:28,251 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:29,251 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:30,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:31,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:32,254 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:32,255 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:58:38,257 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:39,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:40,259 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:41,259 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:42,260 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:43,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:44,262 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:45,263 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:46,264 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:47,266 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:47,270 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:58:53,272 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:54,273 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:55,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:56,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:57,277 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:58,278 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:58:59,280 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:00,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:01,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:02,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:02,284 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:59:08,286 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:09,286 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:10,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:11,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:12,292 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:13,293 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:14,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:15,295 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:16,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:17,327 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:17,329 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:59:23,337 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:24,337 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:25,339 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:26,340 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:27,341 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:28,342 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:29,342 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:30,344 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:31,345 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:32,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:32,347 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:59:38,350 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:39,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:40,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:41,353 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:42,354 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:43,355 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:44,355 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:45,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:46,357 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:47,357 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:47,359 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 18:59:53,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:54,363 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:55,370 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:56,371 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:57,372 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:58,373 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 18:59:59,374 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:00,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:01,377 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:02,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:02,379 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:00:08,380 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:09,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:10,382 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:11,384 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:12,385 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:13,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:14,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:15,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:16,390 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:17,392 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:17,392 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:00:23,397 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:24,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:25,399 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:26,400 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:27,401 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:28,402 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:29,403 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:30,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:31,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:32,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:32,407 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:00:38,409 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:39,410 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:40,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:41,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:42,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:43,414 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:44,415 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:45,416 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:46,417 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:47,418 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:47,419 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:00:53,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:54,422 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:55,424 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:56,425 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:57,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:58,428 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:00:59,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:00,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:01,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:02,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:02,434 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:01:08,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:09,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:10,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:11,440 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:12,441 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:13,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:14,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:15,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:16,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:17,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:17,446 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:01:23,448 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:24,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:25,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:26,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:27,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:28,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:29,456 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:30,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:31,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:32,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:32,461 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:01:38,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:39,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:40,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:41,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:42,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:43,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:44,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:45,473 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:46,474 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:47,474 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:47,475 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:01:53,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:54,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:55,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:56,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:57,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:58,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:01:59,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:00,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:01,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:02,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:02,504 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:02:08,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:09,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:10,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:11,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:12,510 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:13,511 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:14,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:15,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:16,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:17,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:17,519 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:02:23,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:24,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:25,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:26,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:27,525 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:28,525 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:29,527 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:30,528 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:31,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:32,530 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:32,531 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:02:38,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:39,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:40,534 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:41,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:42,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:43,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:44,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:45,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:46,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:47,543 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:47,544 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:02:53,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:54,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:55,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:56,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:57,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:58,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:02:59,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:00,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:01,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:02,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:02,559 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:03:08,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:09,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:10,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:11,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:12,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:13,566 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:14,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:15,568 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:16,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:17,570 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:17,571 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:03:23,573 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:24,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:25,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:26,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:27,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:28,578 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:29,580 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:30,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:31,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:32,582 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:32,582 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:03:38,588 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:39,589 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:40,589 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:41,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:42,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:43,591 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:44,592 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:45,593 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:46,593 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:47,594 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:47,596 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:03:53,598 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:54,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:55,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:56,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:57,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:58,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:03:59,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:00,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:01,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:02,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:02,606 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:04:08,607 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:09,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:10,609 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:11,610 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:12,611 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:13,611 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:14,613 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:15,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:16,615 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:17,616 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:17,617 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:04:23,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:24,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:25,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:26,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:27,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:28,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:29,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:30,624 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:31,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:32,626 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:32,627 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:04:38,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:39,630 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:40,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:41,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:42,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:43,635 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:44,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:45,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:46,640 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:47,641 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:47,642 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:04:53,645 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:54,645 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:55,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:56,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:57,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:58,652 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:04:59,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:00,657 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:01,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:02,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:02,659 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:05:08,661 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:09,662 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:10,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:11,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:12,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:13,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:14,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:15,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:16,671 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:17,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:17,673 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:05:23,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:24,676 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:25,676 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:26,677 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:27,678 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:28,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:29,681 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:30,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:31,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:32,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:32,684 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:05:38,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:39,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:40,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:41,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:42,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:43,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:44,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:45,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:46,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:47,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:47,696 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:05:53,698 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:54,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:55,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:56,701 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:57,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:58,703 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:05:59,704 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:00,705 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:01,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:02,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:02,708 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:06:08,710 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:09,711 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:10,712 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:11,713 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:12,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:13,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:14,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:15,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:16,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:17,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:17,720 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:06:23,722 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:24,722 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:25,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:26,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:27,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:28,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:29,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:30,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:31,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:32,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:32,733 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:06:38,735 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:39,735 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:40,736 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:41,737 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:42,738 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:43,738 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:44,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:45,740 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:46,741 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:47,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:47,743 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:06:53,745 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:54,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:55,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:56,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:57,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:58,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:06:59,751 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:00,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:01,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:02,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:02,756 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:07:08,758 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:09,759 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:10,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:11,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:12,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:13,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:14,766 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:15,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:16,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:17,768 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:17,769 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:07:23,770 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:24,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:25,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:26,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:27,774 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:28,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:29,776 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:30,778 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:31,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:32,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:32,781 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:07:38,783 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:39,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:40,785 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:41,786 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:42,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:43,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:44,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:45,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:46,793 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:47,793 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:07:47,794 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2015-12-05 19:07:53,118 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-05 19:07:53,131 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-parallels/dfs/data/in_use.lock acquired by nodename 4714@ubuntu
2015-12-05 19:07:53,132 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-parallels/dfs/data is not formatted for BP-1164591051-127.0.1.1-1449371237756
2015-12-05 19:07:53,132 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-12-05 19:07:53,228 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1164591051-127.0.1.1-1449371237756
2015-12-05 19:07:53,228 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-12-05 19:07:53,229 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-parallels/dfs/data/current/BP-1164591051-127.0.1.1-1449371237756 is not formatted.
2015-12-05 19:07:53,229 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-12-05 19:07:53,229 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1164591051-127.0.1.1-1449371237756 directory /tmp/hadoop-parallels/dfs/data/current/BP-1164591051-127.0.1.1-1449371237756/current
2015-12-05 19:07:53,231 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-05 19:07:53,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=208897031;bpid=BP-1164591051-127.0.1.1-1449371237756;lv=-56;nsInfo=lv=-60;cid=CID-4c805491-133b-45fe-ad2f-46e6c9c45a4a;nsid=208897031;c=0;bpid=BP-1164591051-127.0.1.1-1449371237756;dnuuid=null
2015-12-05 19:07:53,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 70e40ee2-5188-4a05-ac21-5d90b743dd79
2015-12-05 19:07:53,280 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-parallels/dfs/data/current
2015-12-05 19:07:53,281 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-parallels/dfs/data/current, StorageType: DISK
2015-12-05 19:07:53,300 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-05 19:07:53,305 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1449378170305 with interval 21600000
2015-12-05 19:07:53,306 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1164591051-127.0.1.1-1449371237756
2015-12-05 19:07:53,306 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1164591051-127.0.1.1-1449371237756 on volume /tmp/hadoop-parallels/dfs/data/current...
2015-12-05 19:07:53,405 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1164591051-127.0.1.1-1449371237756 on /tmp/hadoop-parallels/dfs/data/current: 99ms
2015-12-05 19:07:53,406 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1164591051-127.0.1.1-1449371237756: 100ms
2015-12-05 19:07:53,406 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1164591051-127.0.1.1-1449371237756 on volume /tmp/hadoop-parallels/dfs/data/current...
2015-12-05 19:07:53,406 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1164591051-127.0.1.1-1449371237756 on volume /tmp/hadoop-parallels/dfs/data/current: 0ms
2015-12-05 19:07:53,406 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-12-05 19:07:53,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1164591051-127.0.1.1-1449371237756 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-05 19:07:53,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1164591051-127.0.1.1-1449371237756 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-05 19:07:53,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-05 19:07:53,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1164591051-127.0.1.1-1449371237756 (Datanode Uuid 70e40ee2-5188-4a05-ac21-5d90b743dd79) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-12-05 19:07:53,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1164591051-127.0.1.1-1449371237756 (Datanode Uuid 70e40ee2-5188-4a05-ac21-5d90b743dd79) service to localhost/127.0.0.1:9000
2015-12-05 19:07:53,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 51 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@4d911f2b
2015-12-05 19:07:53,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1164591051-127.0.1.1-1449371237756
2015-12-05 19:07:53,629 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-12-05 19:07:53,629 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 19:07:53,629 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-12-05 19:07:53,630 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-12-05 19:07:53,630 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1164591051-127.0.1.1-1449371237756
2015-12-05 19:07:53,634 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1164591051-127.0.1.1-1449371237756 to blockPoolScannerMap, new size=1
2015-12-05 19:21:35,600 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "ubuntu/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-12-05 19:21:39,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:21:40,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 19:21:41,533 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 19:21:41,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 19:22:39,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 19:22:39,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 19:22:39,595 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 19:22:39,651 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 19:22:39,651 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-05 19:22:39,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-05 19:22:39,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-05 19:22:39,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-05 19:22:39,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-05 19:22:39,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-05 19:22:39,774 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 19:22:39,777 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-05 19:22:39,785 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 19:22:39,786 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-05 19:22:39,786 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 19:22:39,786 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 19:22:39,799 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 19:22:39,803 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-05 19:22:39,803 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 19:22:39,971 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-05 19:22:40,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-05 19:22:40,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-05 19:22:40,149 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-05 19:22:40,162 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-05 19:22:40,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-05 19:22:40,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-05 19:22:40,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-05 19:22:40,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-05 19:22:40,244 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-05 19:22:40,244 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-05 19:22:40,528 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-05 19:22:40,538 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-parallels/dfs/data/in_use.lock acquired by nodename 12585@ubuntu
2015-12-05 19:22:40,576 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1164591051-127.0.1.1-1449371237756
2015-12-05 19:22:40,576 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-12-05 19:22:40,577 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-05 19:22:40,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=208897031;bpid=BP-1164591051-127.0.1.1-1449371237756;lv=-56;nsInfo=lv=-60;cid=CID-4c805491-133b-45fe-ad2f-46e6c9c45a4a;nsid=208897031;c=0;bpid=BP-1164591051-127.0.1.1-1449371237756;dnuuid=70e40ee2-5188-4a05-ac21-5d90b743dd79
2015-12-05 19:22:40,609 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-parallels/dfs/data/current
2015-12-05 19:22:40,610 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-parallels/dfs/data/current, StorageType: DISK
2015-12-05 19:22:40,639 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-05 19:22:40,642 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1449385596642 with interval 21600000
2015-12-05 19:22:40,643 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1164591051-127.0.1.1-1449371237756
2015-12-05 19:22:40,644 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1164591051-127.0.1.1-1449371237756 on volume /tmp/hadoop-parallels/dfs/data/current...
2015-12-05 19:22:40,665 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /tmp/hadoop-parallels/dfs/data/current/BP-1164591051-127.0.1.1-1449371237756/current: 24576
2015-12-05 19:22:40,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1164591051-127.0.1.1-1449371237756 on /tmp/hadoop-parallels/dfs/data/current: 23ms
2015-12-05 19:22:40,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1164591051-127.0.1.1-1449371237756: 24ms
2015-12-05 19:22:40,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1164591051-127.0.1.1-1449371237756 on volume /tmp/hadoop-parallels/dfs/data/current...
2015-12-05 19:22:40,668 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1164591051-127.0.1.1-1449371237756 on volume /tmp/hadoop-parallels/dfs/data/current: 0ms
2015-12-05 19:22:40,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-12-05 19:22:40,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1164591051-127.0.1.1-1449371237756 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-05 19:22:40,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1164591051-127.0.1.1-1449371237756 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-05 19:22:40,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-05 19:22:40,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1164591051-127.0.1.1-1449371237756 (Datanode Uuid 70e40ee2-5188-4a05-ac21-5d90b743dd79) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=8
2015-12-05 19:22:40,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1164591051-127.0.1.1-1449371237756 (Datanode Uuid 70e40ee2-5188-4a05-ac21-5d90b743dd79) service to localhost/127.0.0.1:9000
2015-12-05 19:22:40,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 41 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@5a51176c
2015-12-05 19:22:40,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1164591051-127.0.1.1-1449371237756
2015-12-05 19:22:40,864 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-12-05 19:22:40,864 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 19:22:40,865 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-12-05 19:22:40,865 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-12-05 19:22:40,867 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1164591051-127.0.1.1-1449371237756
2015-12-05 19:22:40,870 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1164591051-127.0.1.1-1449371237756 to blockPoolScannerMap, new size=1
2015-12-05 19:51:44,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1164591051-127.0.1.1-1449371237756:blk_1073741825_1001 src: /127.0.0.1:34390 dest: /127.0.0.1:50010
2015-12-05 19:51:44,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:34390, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_682048098_1, offset: 0, srvID: 70e40ee2-5188-4a05-ac21-5d90b743dd79, blockid: BP-1164591051-127.0.1.1-1449371237756:blk_1073741825_1001, duration: 142026450
2015-12-05 19:51:44,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1164591051-127.0.1.1-1449371237756:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 19:51:50,875 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1164591051-127.0.1.1-1449371237756:blk_1073741825_1001
2015-12-05 19:51:52,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1164591051-127.0.1.1-1449371237756:blk_1073741826_1002 src: /127.0.0.1:34393 dest: /127.0.0.1:50010
2015-12-05 19:51:52,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:34393, dest: /127.0.0.1:50010, bytes: 28, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1010199021_1, offset: 0, srvID: 70e40ee2-5188-4a05-ac21-5d90b743dd79, blockid: BP-1164591051-127.0.1.1-1449371237756:blk_1073741826_1002, duration: 61220130
2015-12-05 19:51:52,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1164591051-127.0.1.1-1449371237756:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 19:52:00,886 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1164591051-127.0.1.1-1449371237756:blk_1073741826_1002
2015-12-05 20:12:38,240 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "ubuntu/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-12-05 20:12:42,237 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:43,239 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:44,240 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:45,241 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:46,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:47,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:48,247 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:49,248 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:50,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 20:12:50,854 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 20:12:50,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 20:16:37,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 20:16:37,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 20:16:37,958 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Invalid dfs.datanode.data.dir /usr/local/hadoop/tmp/dfs/data : 
java.io.FileNotFoundException: File file:/usr/local/hadoop/tmp/dfs/data does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:139)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:156)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:2239)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2281)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2263)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2155)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2202)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2378)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2402)
2015-12-05 20:16:37,961 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.io.IOException: All directories in dfs.datanode.data.dir are invalid: "/usr/local/hadoop/tmp/dfs/data" 
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2290)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2263)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2155)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2202)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2378)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2402)
2015-12-05 20:16:37,962 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 20:16:37,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 20:25:04,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 20:25:04,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 20:25:05,351 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Invalid dfs.datanode.data.dir /usr/local/hadoop/tmp/dfs/data : 
java.io.FileNotFoundException: File file:/usr/local/hadoop/tmp/dfs/data does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:139)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:156)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:2239)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2281)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2263)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2155)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2202)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2378)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2402)
2015-12-05 20:25:05,353 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.io.IOException: All directories in dfs.datanode.data.dir are invalid: "/usr/local/hadoop/tmp/dfs/data" 
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2290)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2263)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2155)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2202)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2378)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2402)
2015-12-05 20:25:05,354 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 20:25:05,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 20:35:03,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 20:35:03,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 20:35:03,850 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Invalid dfs.datanode.data.dir /usr/local/hadoop/tmp/dfs/data : 
java.io.FileNotFoundException: File file:/usr/local/hadoop/tmp/dfs/data does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:139)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:156)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:2239)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2281)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2263)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2155)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2202)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2378)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2402)
2015-12-05 20:35:03,853 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.io.IOException: All directories in dfs.datanode.data.dir are invalid: "/usr/local/hadoop/tmp/dfs/data" 
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2290)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2263)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2155)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2202)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2378)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2402)
2015-12-05 20:35:03,854 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 20:35:03,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 20:45:53,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 20:45:53,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 20:45:54,063 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Invalid dfs.datanode.data.dir /usr/local/hadoop/tmp/dfs/data : 
java.io.FileNotFoundException: File file:/usr/local/hadoop/tmp/dfs/data does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:139)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:156)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:2239)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2281)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2263)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2155)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2202)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2378)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2402)
2015-12-05 20:45:54,066 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.io.IOException: All directories in dfs.datanode.data.dir are invalid: "/usr/local/hadoop/tmp/dfs/data" 
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2290)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2263)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2155)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2202)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2378)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2402)
2015-12-05 20:45:54,068 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 20:45:54,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 20:49:28,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 20:49:28,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 20:49:28,819 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Invalid dfs.datanode.data.dir /usr/local/hadoop/tmp/dfs/data : 
java.io.FileNotFoundException: File file:/usr/local/hadoop/tmp/dfs/data does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:139)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:156)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:2239)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2281)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2263)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2155)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2202)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2378)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2402)
2015-12-05 20:49:28,821 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.io.IOException: All directories in dfs.datanode.data.dir are invalid: "/usr/local/hadoop/tmp/dfs/data" 
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2290)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2263)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2155)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2202)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2378)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2402)
2015-12-05 20:49:28,823 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-12-05 20:49:28,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 21:13:42,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 21:13:42,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 21:13:42,575 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 21:13:42,643 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 21:13:42,643 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-05 21:13:42,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-05 21:13:42,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-05 21:13:42,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-05 21:13:42,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-05 21:13:42,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-05 21:13:42,761 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 21:13:42,764 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-05 21:13:42,773 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 21:13:42,775 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-05 21:13:42,775 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 21:13:42,775 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 21:13:42,789 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 21:13:42,793 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-05 21:13:42,794 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 21:13:42,975 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-05 21:13:43,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-05 21:13:43,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-05 21:13:43,175 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-05 21:13:43,187 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-05 21:13:43,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-05 21:13:43,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-05 21:13:43,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-05 21:13:43,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-05 21:13:43,270 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-05 21:13:43,271 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-05 21:13:43,574 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-05 21:13:43,585 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 19557@ubuntu
2015-12-05 21:13:43,586 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/parallels/hadoop/tmp/dfs/data is not formatted for BP-1777036329-127.0.1.1-1449378795593
2015-12-05 21:13:43,586 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-12-05 21:13:43,643 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1777036329-127.0.1.1-1449378795593
2015-12-05 21:13:43,643 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-12-05 21:13:43,644 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593 is not formatted.
2015-12-05 21:13:43,644 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-12-05 21:13:43,644 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1777036329-127.0.1.1-1449378795593 directory /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current
2015-12-05 21:13:43,646 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-05 21:13:43,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1710165876;bpid=BP-1777036329-127.0.1.1-1449378795593;lv=-56;nsInfo=lv=-60;cid=CID-64431893-233f-4618-b6db-9870599dcfca;nsid=1710165876;c=0;bpid=BP-1777036329-127.0.1.1-1449378795593;dnuuid=null
2015-12-05 21:13:43,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 3ad84b8a-5322-4d61-8b83-0f8f71d68d89
2015-12-05 21:13:43,685 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /home/parallels/hadoop/tmp/dfs/data/current
2015-12-05 21:13:43,685 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/parallels/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-05 21:13:43,697 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-05 21:13:43,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1449389004700 with interval 21600000
2015-12-05 21:13:43,700 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1777036329-127.0.1.1-1449378795593
2015-12-05 21:13:43,701 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1777036329-127.0.1.1-1449378795593 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-05 21:13:43,726 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1777036329-127.0.1.1-1449378795593 on /home/parallels/hadoop/tmp/dfs/data/current: 25ms
2015-12-05 21:13:43,726 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1777036329-127.0.1.1-1449378795593: 26ms
2015-12-05 21:13:43,727 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1777036329-127.0.1.1-1449378795593 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-05 21:13:43,727 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1777036329-127.0.1.1-1449378795593 on volume /home/parallels/hadoop/tmp/dfs/data/current: 0ms
2015-12-05 21:13:43,727 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-12-05 21:13:43,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1777036329-127.0.1.1-1449378795593 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-05 21:13:43,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1777036329-127.0.1.1-1449378795593 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-05 21:13:43,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-05 21:13:43,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1777036329-127.0.1.1-1449378795593 (Datanode Uuid 3ad84b8a-5322-4d61-8b83-0f8f71d68d89) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-12-05 21:13:43,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1777036329-127.0.1.1-1449378795593 (Datanode Uuid 3ad84b8a-5322-4d61-8b83-0f8f71d68d89) service to localhost/127.0.0.1:9000
2015-12-05 21:13:43,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 39 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@351e3492
2015-12-05 21:13:43,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1777036329-127.0.1.1-1449378795593
2015-12-05 21:13:43,967 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-12-05 21:13:43,967 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 21:13:43,968 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-12-05 21:13:43,968 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-12-05 21:13:43,970 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1777036329-127.0.1.1-1449378795593
2015-12-05 21:13:43,976 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1777036329-127.0.1.1-1449378795593 to blockPoolScannerMap, new size=1
2015-12-05 21:19:53,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1777036329-127.0.1.1-1449378795593:blk_1073741825_1001 src: /127.0.0.1:33659 dest: /127.0.0.1:50010
2015-12-05 21:19:53,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:33659, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1046888939_1, offset: 0, srvID: 3ad84b8a-5322-4d61-8b83-0f8f71d68d89, blockid: BP-1777036329-127.0.1.1-1449378795593:blk_1073741825_1001, duration: 119610676
2015-12-05 21:19:53,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1777036329-127.0.1.1-1449378795593:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 21:19:58,758 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1777036329-127.0.1.1-1449378795593:blk_1073741825_1001
2015-12-05 21:20:06,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1777036329-127.0.1.1-1449378795593:blk_1073741826_1002 src: /127.0.0.1:33662 dest: /127.0.0.1:50010
2015-12-05 21:20:06,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:33662, dest: /127.0.0.1:50010, bytes: 28, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_626213952_1, offset: 0, srvID: 3ad84b8a-5322-4d61-8b83-0f8f71d68d89, blockid: BP-1777036329-127.0.1.1-1449378795593:blk_1073741826_1002, duration: 61353951
2015-12-05 21:20:06,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1777036329-127.0.1.1-1449378795593:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 21:20:13,769 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1777036329-127.0.1.1-1449378795593:blk_1073741826_1002
2015-12-05 21:20:25,865 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "ubuntu/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-12-05 21:20:29,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 21:20:30,172 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 21:20:30,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 21:22:23,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 21:22:23,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 21:22:24,301 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 21:22:24,367 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 21:22:24,368 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-05 21:22:24,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-05 21:22:24,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-05 21:22:24,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-05 21:22:24,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-05 21:22:24,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-05 21:22:24,510 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 21:22:24,513 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-05 21:22:24,521 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 21:22:24,523 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-05 21:22:24,523 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 21:22:24,523 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 21:22:24,544 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 21:22:24,549 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-05 21:22:24,549 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 21:22:24,742 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-05 21:22:24,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-05 21:22:24,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-05 21:22:24,935 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-05 21:22:24,949 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-05 21:22:24,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-05 21:22:24,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-05 21:22:25,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-05 21:22:25,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-05 21:22:25,031 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-05 21:22:25,032 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-05 21:22:25,321 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-05 21:22:25,326 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 3181@ubuntu
2015-12-05 21:22:25,404 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1777036329-127.0.1.1-1449378795593
2015-12-05 21:22:25,404 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-12-05 21:22:25,405 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-05 21:22:25,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1710165876;bpid=BP-1777036329-127.0.1.1-1449378795593;lv=-56;nsInfo=lv=-60;cid=CID-64431893-233f-4618-b6db-9870599dcfca;nsid=1710165876;c=0;bpid=BP-1777036329-127.0.1.1-1449378795593;dnuuid=3ad84b8a-5322-4d61-8b83-0f8f71d68d89
2015-12-05 21:22:25,474 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /home/parallels/hadoop/tmp/dfs/data/current
2015-12-05 21:22:25,475 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/parallels/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-05 21:22:25,608 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-05 21:22:25,611 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1449400643611 with interval 21600000
2015-12-05 21:22:25,612 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1777036329-127.0.1.1-1449378795593
2015-12-05 21:22:25,617 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1777036329-127.0.1.1-1449378795593 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-05 21:22:25,653 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current: 24648
2015-12-05 21:22:25,662 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1777036329-127.0.1.1-1449378795593 on /home/parallels/hadoop/tmp/dfs/data/current: 45ms
2015-12-05 21:22:25,662 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1777036329-127.0.1.1-1449378795593: 49ms
2015-12-05 21:22:25,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1777036329-127.0.1.1-1449378795593 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-05 21:22:25,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1777036329-127.0.1.1-1449378795593 on volume /home/parallels/hadoop/tmp/dfs/data/current: 13ms
2015-12-05 21:22:25,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 17ms
2015-12-05 21:22:25,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1777036329-127.0.1.1-1449378795593 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-05 21:22:25,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1777036329-127.0.1.1-1449378795593 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-05 21:22:25,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-05 21:22:25,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1777036329-127.0.1.1-1449378795593 (Datanode Uuid 3ad84b8a-5322-4d61-8b83-0f8f71d68d89) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=19
2015-12-05 21:22:25,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1777036329-127.0.1.1-1449378795593 (Datanode Uuid 3ad84b8a-5322-4d61-8b83-0f8f71d68d89) service to localhost/127.0.0.1:9000
2015-12-05 21:22:25,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 2 blocks total. Took 2 msec to generate and 73 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@62fb3d51
2015-12-05 21:22:25,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1777036329-127.0.1.1-1449378795593
2015-12-05 21:22:25,997 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-12-05 21:22:25,999 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 21:22:26,000 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-12-05 21:22:26,001 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-12-05 21:22:26,003 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1777036329-127.0.1.1-1449378795593
2015-12-05 21:22:26,010 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1777036329-127.0.1.1-1449378795593 to blockPoolScannerMap, new size=1
2015-12-05 21:32:58,920 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2015-12-05 21:32:58,922 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current/finalized/subdir0/subdir0/blk_1073741826 for deletion
2015-12-05 21:32:58,923 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1777036329-127.0.1.1-1449378795593 blk_1073741825_1001 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current/finalized/subdir0/subdir0/blk_1073741825
2015-12-05 21:32:58,924 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1777036329-127.0.1.1-1449378795593 blk_1073741826_1002 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current/finalized/subdir0/subdir0/blk_1073741826
2015-12-05 21:34:43,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1777036329-127.0.1.1-1449378795593:blk_1073741827_1003 src: /127.0.0.1:32996 dest: /127.0.0.1:50010
2015-12-05 21:34:43,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:32996, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-216749264_1, offset: 0, srvID: 3ad84b8a-5322-4d61-8b83-0f8f71d68d89, blockid: BP-1777036329-127.0.1.1-1449378795593:blk_1073741827_1003, duration: 80703250
2015-12-05 21:34:43,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1777036329-127.0.1.1-1449378795593:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 21:34:50,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1777036329-127.0.1.1-1449378795593:blk_1073741828_1004 src: /127.0.0.1:32999 dest: /127.0.0.1:50010
2015-12-05 21:34:50,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:32999, dest: /127.0.0.1:50010, bytes: 28, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-847673488_1, offset: 0, srvID: 3ad84b8a-5322-4d61-8b83-0f8f71d68d89, blockid: BP-1777036329-127.0.1.1-1449378795593:blk_1073741828_1004, duration: 62905873
2015-12-05 21:34:50,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1777036329-127.0.1.1-1449378795593:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 21:40:29,017 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 2 blocks total. Took 0 msec to generate and 48 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@310b93a3
2015-12-05 21:40:29,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1777036329-127.0.1.1-1449378795593
2015-12-05 22:34:38,511 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "ubuntu/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-12-05 22:34:42,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 22:34:43,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 22:34:44,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 22:34:44,858 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 22:34:44,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-05 22:42:17,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.8.0_65
************************************************************/
2015-12-05 22:42:17,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-05 22:42:18,163 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-05 22:42:18,234 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-05 22:42:18,234 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-05 22:42:18,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-05 22:42:18,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-05 22:42:18,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-05 22:42:18,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-05 22:42:18,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-05 22:42:18,345 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-05 22:42:18,348 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-05 22:42:18,357 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-05 22:42:18,358 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-05 22:42:18,359 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-05 22:42:18,359 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-05 22:42:18,373 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-05 22:42:18,377 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-05 22:42:18,377 INFO org.mortbay.log: jetty-6.1.26
2015-12-05 22:42:18,574 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-05 22:42:18,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-05 22:42:18,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-05 22:42:18,780 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-05 22:42:18,793 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-05 22:42:18,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-05 22:42:18,831 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-05 22:42:18,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-05 22:42:18,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-05 22:42:18,870 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-05 22:42:18,875 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-05 22:42:19,181 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-05 22:42:19,186 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 20648@ubuntu
2015-12-05 22:42:19,241 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1777036329-127.0.1.1-1449378795593
2015-12-05 22:42:19,241 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-12-05 22:42:19,242 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-05 22:42:19,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1710165876;bpid=BP-1777036329-127.0.1.1-1449378795593;lv=-56;nsInfo=lv=-60;cid=CID-64431893-233f-4618-b6db-9870599dcfca;nsid=1710165876;c=0;bpid=BP-1777036329-127.0.1.1-1449378795593;dnuuid=3ad84b8a-5322-4d61-8b83-0f8f71d68d89
2015-12-05 22:42:19,279 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /home/parallels/hadoop/tmp/dfs/data/current
2015-12-05 22:42:19,280 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/parallels/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-05 22:42:19,330 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-05 22:42:19,333 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1449394147333 with interval 21600000
2015-12-05 22:42:19,334 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1777036329-127.0.1.1-1449378795593
2015-12-05 22:42:19,336 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1777036329-127.0.1.1-1449378795593 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-05 22:42:19,351 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current: 57344
2015-12-05 22:42:19,352 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1777036329-127.0.1.1-1449378795593 on /home/parallels/hadoop/tmp/dfs/data/current: 17ms
2015-12-05 22:42:19,353 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1777036329-127.0.1.1-1449378795593: 18ms
2015-12-05 22:42:19,353 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1777036329-127.0.1.1-1449378795593 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-05 22:42:19,358 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1777036329-127.0.1.1-1449378795593 on volume /home/parallels/hadoop/tmp/dfs/data/current: 4ms
2015-12-05 22:42:19,358 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2015-12-05 22:42:19,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1777036329-127.0.1.1-1449378795593 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-05 22:42:19,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1777036329-127.0.1.1-1449378795593 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-05 22:42:19,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-05 22:42:19,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1777036329-127.0.1.1-1449378795593 (Datanode Uuid 3ad84b8a-5322-4d61-8b83-0f8f71d68d89) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=42
2015-12-05 22:42:19,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1777036329-127.0.1.1-1449378795593 (Datanode Uuid 3ad84b8a-5322-4d61-8b83-0f8f71d68d89) service to localhost/127.0.0.1:9000
2015-12-05 22:42:19,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 2 blocks total. Took 2 msec to generate and 54 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@28136d7c
2015-12-05 22:42:19,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1777036329-127.0.1.1-1449378795593
2015-12-05 22:42:19,592 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-12-05 22:42:19,592 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-05 22:42:19,593 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-12-05 22:42:19,593 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-12-05 22:42:19,596 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1777036329-127.0.1.1-1449378795593
2015-12-05 22:42:19,599 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1777036329-127.0.1.1-1449378795593 to blockPoolScannerMap, new size=1
2015-12-05 22:42:24,358 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1777036329-127.0.1.1-1449378795593:blk_1073741828_1004
2015-12-05 22:42:24,360 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1777036329-127.0.1.1-1449378795593:blk_1073741827_1003
2015-12-05 22:44:12,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1777036329-127.0.1.1-1449378795593:blk_1073741829_1005 src: /127.0.0.1:33204 dest: /127.0.0.1:50010
2015-12-05 22:44:12,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:33204, dest: /127.0.0.1:50010, bytes: 3074, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_329235791_1, offset: 0, srvID: 3ad84b8a-5322-4d61-8b83-0f8f71d68d89, blockid: BP-1777036329-127.0.1.1-1449378795593:blk_1073741829_1005, duration: 78475103
2015-12-05 22:44:12,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1777036329-127.0.1.1-1449378795593:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 22:44:13,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1777036329-127.0.1.1-1449378795593:blk_1073741830_1006 src: /127.0.0.1:33205 dest: /127.0.0.1:50010
2015-12-05 22:44:13,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:33205, dest: /127.0.0.1:50010, bytes: 255, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_329235791_1, offset: 0, srvID: 3ad84b8a-5322-4d61-8b83-0f8f71d68d89, blockid: BP-1777036329-127.0.1.1-1449378795593:blk_1073741830_1006, duration: 3031371
2015-12-05 22:44:13,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1777036329-127.0.1.1-1449378795593:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 22:44:13,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1777036329-127.0.1.1-1449378795593:blk_1073741831_1007 src: /127.0.0.1:33206 dest: /127.0.0.1:50010
2015-12-05 22:44:13,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:33206, dest: /127.0.0.1:50010, bytes: 31, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_329235791_1, offset: 0, srvID: 3ad84b8a-5322-4d61-8b83-0f8f71d68d89, blockid: BP-1777036329-127.0.1.1-1449378795593:blk_1073741831_1007, duration: 1093551
2015-12-05 22:44:13,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1777036329-127.0.1.1-1449378795593:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 22:44:13,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1777036329-127.0.1.1-1449378795593:blk_1073741832_1008 src: /127.0.0.1:33207 dest: /127.0.0.1:50010
2015-12-05 22:44:13,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:33207, dest: /127.0.0.1:50010, bytes: 87273, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_329235791_1, offset: 0, srvID: 3ad84b8a-5322-4d61-8b83-0f8f71d68d89, blockid: BP-1777036329-127.0.1.1-1449378795593:blk_1073741832_1008, duration: 8675856
2015-12-05 22:44:13,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1777036329-127.0.1.1-1449378795593:blk_1073741832_1008, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 22:44:19,396 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1777036329-127.0.1.1-1449378795593:blk_1073741830_1006
2015-12-05 22:44:19,414 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1777036329-127.0.1.1-1449378795593:blk_1073741832_1008
2015-12-05 22:44:19,415 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1777036329-127.0.1.1-1449378795593:blk_1073741831_1007
2015-12-05 22:44:19,424 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1777036329-127.0.1.1-1449378795593:blk_1073741829_1005
2015-12-05 22:44:24,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1777036329-127.0.1.1-1449378795593:blk_1073741833_1009 src: /127.0.0.1:33215 dest: /127.0.0.1:50010
2015-12-05 22:44:24,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:33215, dest: /127.0.0.1:50010, bytes: 103819, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1394527982_1, offset: 0, srvID: 3ad84b8a-5322-4d61-8b83-0f8f71d68d89, blockid: BP-1777036329-127.0.1.1-1449378795593:blk_1073741833_1009, duration: 49708699
2015-12-05 22:44:24,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1777036329-127.0.1.1-1449378795593:blk_1073741833_1009, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 22:44:34,521 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1777036329-127.0.1.1-1449378795593:blk_1073741833_1009
2015-12-05 22:44:44,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1777036329-127.0.1.1-1449378795593:blk_1073741834_1010 src: /127.0.0.1:33231 dest: /127.0.0.1:50010
2015-12-05 22:44:53,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1777036329-127.0.1.1-1449378795593:blk_1073741835_1011 src: /127.0.0.1:33241 dest: /127.0.0.1:50010
2015-12-05 22:44:53,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:33241, dest: /127.0.0.1:50010, bytes: 41, op: HDFS_WRITE, cliID: DFSClient_attempt_1449384245710_0001_r_000000_0_-268524602_1, offset: 0, srvID: 3ad84b8a-5322-4d61-8b83-0f8f71d68d89, blockid: BP-1777036329-127.0.1.1-1449378795593:blk_1073741835_1011, duration: 52640720
2015-12-05 22:44:53,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1777036329-127.0.1.1-1449378795593:blk_1073741835_1011, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 22:44:54,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:33231, dest: /127.0.0.1:50010, bytes: 40141, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1394527982_1, offset: 0, srvID: 3ad84b8a-5322-4d61-8b83-0f8f71d68d89, blockid: BP-1777036329-127.0.1.1-1449378795593:blk_1073741834_1010, duration: 10361046662
2015-12-05 22:44:54,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1777036329-127.0.1.1-1449378795593:blk_1073741834_1010, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 22:44:54,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1777036329-127.0.1.1-1449378795593:blk_1073741836_1012 src: /127.0.0.1:33244 dest: /127.0.0.1:50010
2015-12-05 22:44:54,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:33244, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1394527982_1, offset: 0, srvID: 3ad84b8a-5322-4d61-8b83-0f8f71d68d89, blockid: BP-1777036329-127.0.1.1-1449378795593:blk_1073741836_1012, duration: 6098347
2015-12-05 22:44:54,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1777036329-127.0.1.1-1449378795593:blk_1073741836_1012, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 22:44:54,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1777036329-127.0.1.1-1449378795593:blk_1073741837_1013 src: /127.0.0.1:33246 dest: /127.0.0.1:50010
2015-12-05 22:44:54,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:33246, dest: /127.0.0.1:50010, bytes: 40141, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1394527982_1, offset: 0, srvID: 3ad84b8a-5322-4d61-8b83-0f8f71d68d89, blockid: BP-1777036329-127.0.1.1-1449378795593:blk_1073741837_1013, duration: 10352929
2015-12-05 22:44:54,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1777036329-127.0.1.1-1449378795593:blk_1073741837_1013, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 22:44:54,699 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1777036329-127.0.1.1-1449378795593:blk_1073741838_1014 src: /127.0.0.1:33247 dest: /127.0.0.1:50010
2015-12-05 22:44:54,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:33247, dest: /127.0.0.1:50010, bytes: 103819, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1394527982_1, offset: 0, srvID: 3ad84b8a-5322-4d61-8b83-0f8f71d68d89, blockid: BP-1777036329-127.0.1.1-1449378795593:blk_1073741838_1014, duration: 16365121
2015-12-05 22:44:54,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1777036329-127.0.1.1-1449378795593:blk_1073741838_1014, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-05 22:44:58,488 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2015-12-05 22:44:58,495 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2015-12-05 22:44:58,497 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current/finalized/subdir0/subdir0/blk_1073741831 for deletion
2015-12-05 22:44:58,497 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741832_1008 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current/finalized/subdir0/subdir0/blk_1073741832 for deletion
2015-12-05 22:44:58,497 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741833_1009 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current/finalized/subdir0/subdir0/blk_1073741833 for deletion
2015-12-05 22:44:58,497 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741834_1010 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current/finalized/subdir0/subdir0/blk_1073741834 for deletion
2015-12-05 22:44:58,498 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1777036329-127.0.1.1-1449378795593 blk_1073741829_1005 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current/finalized/subdir0/subdir0/blk_1073741829
2015-12-05 22:44:58,498 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1777036329-127.0.1.1-1449378795593 blk_1073741830_1006 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current/finalized/subdir0/subdir0/blk_1073741830
2015-12-05 22:44:58,498 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1777036329-127.0.1.1-1449378795593 blk_1073741831_1007 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current/finalized/subdir0/subdir0/blk_1073741831
2015-12-05 22:44:58,498 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1777036329-127.0.1.1-1449378795593 blk_1073741832_1008 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current/finalized/subdir0/subdir0/blk_1073741832
2015-12-05 22:44:58,499 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1777036329-127.0.1.1-1449378795593 blk_1073741833_1009 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current/finalized/subdir0/subdir0/blk_1073741833
2015-12-05 22:44:58,499 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1777036329-127.0.1.1-1449378795593 blk_1073741834_1010 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1777036329-127.0.1.1-1449378795593/current/finalized/subdir0/subdir0/blk_1073741834
2015-12-05 22:44:59,646 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1777036329-127.0.1.1-1449378795593:blk_1073741835_1011
2015-12-05 22:44:59,647 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1777036329-127.0.1.1-1449378795593:blk_1073741836_1012
2015-12-05 22:45:04,655 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1777036329-127.0.1.1-1449378795593:blk_1073741838_1014
2015-12-05 22:45:04,657 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1777036329-127.0.1.1-1449378795593:blk_1073741837_1013
2015-12-05 22:49:43,463 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "ubuntu/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy12.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-12-05 22:49:47,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-05 22:49:48,260 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-05 22:49:48,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-06 18:11:50,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-06 18:11:50,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-06 18:11:50,925 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-06 18:11:51,029 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-06 18:11:51,029 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-06 18:11:51,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-06 18:11:51,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-06 18:11:51,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-06 18:11:51,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-06 18:11:51,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-06 18:11:51,186 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-06 18:11:51,190 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-06 18:11:51,200 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-06 18:11:51,202 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-06 18:11:51,202 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-06 18:11:51,202 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-06 18:11:51,222 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-06 18:11:51,225 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-06 18:11:51,225 INFO org.mortbay.log: jetty-6.1.26
2015-12-06 18:11:51,509 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-06 18:11:51,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-06 18:11:51,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-06 18:11:51,707 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-06 18:11:51,722 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-06 18:11:51,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-06 18:11:51,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-06 18:11:51,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-06 18:11:51,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-06 18:11:51,808 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-06 18:11:51,809 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-06 18:11:52,192 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-06 18:11:52,197 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 3593@ubuntu
2015-12-06 18:11:52,201 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: Incompatible clusterIDs in /home/parallels/hadoop/tmp/dfs/data: namenode clusterID = CID-2b2fe76c-355a-4e5a-9954-d92b07fabf04; datanode clusterID = CID-64431893-233f-4618-b6db-9870599dcfca
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:646)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:320)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:403)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:422)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1311)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1276)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:314)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:828)
	at java.lang.Thread.run(Thread.java:745)
2015-12-06 18:11:52,211 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2015-12-06 18:11:52,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-12-06 18:11:54,212 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-12-06 18:11:54,215 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-12-06 18:11:54,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-06 18:29:17,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-06 18:29:17,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-06 18:29:17,866 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-06 18:29:17,951 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-06 18:29:17,951 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-06 18:29:17,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-06 18:29:17,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-06 18:29:18,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-06 18:29:18,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-06 18:29:18,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-06 18:29:18,103 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-06 18:29:18,107 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-06 18:29:18,115 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-06 18:29:18,117 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-06 18:29:18,117 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-06 18:29:18,117 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-06 18:29:18,134 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-06 18:29:18,137 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-06 18:29:18,137 INFO org.mortbay.log: jetty-6.1.26
2015-12-06 18:29:18,379 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-06 18:29:18,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-06 18:29:18,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-06 18:29:18,597 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-06 18:29:18,617 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-06 18:29:18,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-06 18:29:18,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-06 18:29:18,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-06 18:29:18,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-06 18:29:18,721 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-06 18:29:18,727 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-06 18:29:19,023 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-06 18:29:19,029 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 8062@ubuntu
2015-12-06 18:29:19,032 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: Incompatible clusterIDs in /home/parallels/hadoop/tmp/dfs/data: namenode clusterID = CID-2b2fe76c-355a-4e5a-9954-d92b07fabf04; datanode clusterID = CID-64431893-233f-4618-b6db-9870599dcfca
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:646)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:320)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:403)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:422)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1311)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1276)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:314)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:828)
	at java.lang.Thread.run(Thread.java:745)
2015-12-06 18:29:19,034 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2015-12-06 18:29:19,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-12-06 18:29:21,037 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-12-06 18:29:21,039 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-12-06 18:29:21,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-06 18:33:23,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-06 18:33:23,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-06 18:33:24,320 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-06 18:33:24,377 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-06 18:33:24,378 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-06 18:33:24,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-06 18:33:24,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-06 18:33:24,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-06 18:33:24,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-06 18:33:24,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-06 18:33:24,514 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-06 18:33:24,517 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-06 18:33:24,526 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-06 18:33:24,528 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-06 18:33:24,528 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-06 18:33:24,528 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-06 18:33:24,547 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-06 18:33:24,558 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-06 18:33:24,558 INFO org.mortbay.log: jetty-6.1.26
2015-12-06 18:33:24,836 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-06 18:33:24,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-06 18:33:24,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-06 18:33:25,035 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-06 18:33:25,053 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-06 18:33:25,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-06 18:33:25,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-06 18:33:25,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-06 18:33:25,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-06 18:33:25,119 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-06 18:33:25,120 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-06 18:33:25,398 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-06 18:33:25,402 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 9845@ubuntu
2015-12-06 18:33:25,405 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: Incompatible clusterIDs in /home/parallels/hadoop/tmp/dfs/data: namenode clusterID = CID-2b2fe76c-355a-4e5a-9954-d92b07fabf04; datanode clusterID = CID-64431893-233f-4618-b6db-9870599dcfca
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:646)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:320)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:403)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:422)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1311)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1276)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:314)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:828)
	at java.lang.Thread.run(Thread.java:745)
2015-12-06 18:33:25,413 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2015-12-06 18:33:25,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2015-12-06 18:33:27,514 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-12-06 18:33:27,515 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-12-06 18:33:27,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-06 18:41:11,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-06 18:41:11,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-06 18:41:12,118 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-06 18:41:12,168 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-06 18:41:12,168 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-06 18:41:12,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-06 18:41:12,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-06 18:41:12,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-06 18:41:12,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-06 18:41:12,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-06 18:41:12,287 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-06 18:41:12,290 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-06 18:41:12,299 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-06 18:41:12,302 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-06 18:41:12,302 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-06 18:41:12,302 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-06 18:41:12,320 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-06 18:41:12,323 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-06 18:41:12,323 INFO org.mortbay.log: jetty-6.1.26
2015-12-06 18:41:12,539 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-06 18:41:12,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-06 18:41:12,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-06 18:41:12,774 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-06 18:41:12,794 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-06 18:41:12,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-06 18:41:12,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-06 18:41:12,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-06 18:41:12,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-06 18:41:12,863 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-06 18:41:12,864 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-06 18:41:13,196 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-06 18:41:13,206 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 12301@ubuntu
2015-12-06 18:41:13,208 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/parallels/hadoop/tmp/dfs/data is not formatted for BP-1841603009-127.0.1.1-1449456040486
2015-12-06 18:41:13,208 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-12-06 18:41:13,290 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1841603009-127.0.1.1-1449456040486
2015-12-06 18:41:13,290 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-12-06 18:41:13,290 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486 is not formatted.
2015-12-06 18:41:13,290 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-12-06 18:41:13,290 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1841603009-127.0.1.1-1449456040486 directory /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current
2015-12-06 18:41:13,294 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-06 18:41:13,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1379726899;bpid=BP-1841603009-127.0.1.1-1449456040486;lv=-56;nsInfo=lv=-60;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0;bpid=BP-1841603009-127.0.1.1-1449456040486;dnuuid=null
2015-12-06 18:41:13,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-06 18:41:13,346 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /home/parallels/hadoop/tmp/dfs/data/current
2015-12-06 18:41:13,346 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/parallels/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-06 18:41:13,351 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-06 18:41:13,356 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1449468908356 with interval 21600000
2015-12-06 18:41:13,356 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-06 18:41:13,357 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-06 18:41:13,381 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1841603009-127.0.1.1-1449456040486 on /home/parallels/hadoop/tmp/dfs/data/current: 24ms
2015-12-06 18:41:13,381 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1841603009-127.0.1.1-1449456040486: 24ms
2015-12-06 18:41:13,385 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-06 18:41:13,385 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current: 0ms
2015-12-06 18:41:13,385 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-12-06 18:41:13,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-06 18:41:13,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-06 18:41:13,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-06 18:41:13,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-12-06 18:41:13,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000
2015-12-06 18:41:13,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 55 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@60c209ae
2015-12-06 18:41:13,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-06 18:41:13,575 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-12-06 18:41:13,575 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-06 18:41:13,576 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-12-06 18:41:13,576 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-12-06 18:41:13,576 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-06 18:41:13,583 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1841603009-127.0.1.1-1449456040486 to blockPoolScannerMap, new size=1
2015-12-06 18:44:45,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741825_1001 src: /127.0.0.1:55644 dest: /127.0.0.1:50010
2015-12-06 18:44:46,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55644, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1868091400_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741825_1001, duration: 68628019
2015-12-06 18:44:46,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-06 18:44:53,397 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741825_1001
2015-12-06 18:45:19,474 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2015-12-06 18:45:19,476 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741825_1001 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741825
2015-12-06 18:45:58,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741826_1002 src: /127.0.0.1:55655 dest: /127.0.0.1:50010
2015-12-06 18:45:58,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55655, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_914896958_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741826_1002, duration: 56923781
2015-12-06 18:45:58,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-06 18:46:08,423 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741826_1002
2015-12-06 18:46:11,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741827_1003 src: /127.0.0.1:55657 dest: /127.0.0.1:50010
2015-12-06 18:46:11,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55657, dest: /127.0.0.1:50010, bytes: 28, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-263158760_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741827_1003, duration: 69998569
2015-12-06 18:46:11,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-06 18:46:18,436 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741827_1003
2015-12-06 18:50:06,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741828_1004 src: /127.0.0.1:55675 dest: /127.0.0.1:50010
2015-12-06 18:50:06,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55675, dest: /127.0.0.1:50010, bytes: 3071, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_289117872_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741828_1004, duration: 77571592
2015-12-06 18:50:06,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-06 18:50:06,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741829_1005 src: /127.0.0.1:55676 dest: /127.0.0.1:50010
2015-12-06 18:50:06,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55676, dest: /127.0.0.1:50010, bytes: 235, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_289117872_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741829_1005, duration: 1972123
2015-12-06 18:50:06,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-06 18:50:07,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741830_1006 src: /127.0.0.1:55677 dest: /127.0.0.1:50010
2015-12-06 18:50:07,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55677, dest: /127.0.0.1:50010, bytes: 30, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_289117872_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741830_1006, duration: 5825643
2015-12-06 18:50:07,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-06 18:50:07,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741831_1007 src: /127.0.0.1:55678 dest: /127.0.0.1:50010
2015-12-06 18:50:07,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55678, dest: /127.0.0.1:50010, bytes: 87253, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_289117872_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741831_1007, duration: 30635885
2015-12-06 18:50:07,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-06 18:50:13,529 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741830_1006
2015-12-06 18:50:13,530 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741829_1005
2015-12-06 18:50:13,530 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741828_1004
2015-12-06 18:50:13,537 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741831_1007
2015-12-06 18:50:17,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741832_1008 src: /127.0.0.1:55686 dest: /127.0.0.1:50010
2015-12-06 18:50:17,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55686, dest: /127.0.0.1:50010, bytes: 103799, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1284525877_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741832_1008, duration: 47416660
2015-12-06 18:50:17,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741832_1008, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-06 18:50:23,564 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741832_1008
2015-12-06 18:50:28,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741833_1009 src: /127.0.0.1:55700 dest: /127.0.0.1:50010
2015-12-06 18:50:35,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741834_1010 src: /127.0.0.1:55706 dest: /127.0.0.1:50010
2015-12-06 18:50:35,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55706, dest: /127.0.0.1:50010, bytes: 41, op: HDFS_WRITE, cliID: DFSClient_attempt_1449456599213_0001_r_000000_0_830353187_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741834_1010, duration: 138867806
2015-12-06 18:50:35,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741834_1010, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-06 18:50:37,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55700, dest: /127.0.0.1:50010, bytes: 40127, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1284525877_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741833_1009, duration: 8192318404
2015-12-06 18:50:37,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741833_1009, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-06 18:50:37,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741835_1011 src: /127.0.0.1:55708 dest: /127.0.0.1:50010
2015-12-06 18:50:37,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55708, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1284525877_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741835_1011, duration: 1664723
2015-12-06 18:50:37,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741835_1011, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-06 18:50:37,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741836_1012 src: /127.0.0.1:55710 dest: /127.0.0.1:50010
2015-12-06 18:50:37,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55710, dest: /127.0.0.1:50010, bytes: 40127, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1284525877_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741836_1012, duration: 3113879
2015-12-06 18:50:37,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741836_1012, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-06 18:50:37,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741837_1013 src: /127.0.0.1:55711 dest: /127.0.0.1:50010
2015-12-06 18:50:37,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55711, dest: /127.0.0.1:50010, bytes: 103799, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1284525877_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741837_1013, duration: 1989590
2015-12-06 18:50:37,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741837_1013, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-06 18:50:40,504 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741828 for deletion
2015-12-06 18:50:40,505 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2015-12-06 18:50:40,505 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2015-12-06 18:50:40,505 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741831 for deletion
2015-12-06 18:50:40,505 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741832_1008 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741832 for deletion
2015-12-06 18:50:40,505 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741833_1009 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741833 for deletion
2015-12-06 18:50:40,506 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741828_1004 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741828
2015-12-06 18:50:40,506 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741829_1005 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741829
2015-12-06 18:50:40,506 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741830_1006 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741830
2015-12-06 18:50:40,506 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741831_1007 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741831
2015-12-06 18:50:40,506 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741832_1008 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741832
2015-12-06 18:50:40,506 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741833_1009 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741833
2015-12-06 18:50:43,582 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741835_1011
2015-12-06 18:50:43,583 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741834_1010
2015-12-06 18:50:43,585 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741836_1012
2015-12-06 18:50:43,586 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741837_1013
2015-12-06 19:29:08,784 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-06 19:29:08,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-10 17:08:46,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-10 17:08:46,654 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-10 17:08:47,461 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-10 17:08:47,556 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-10 17:08:47,556 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-10 17:08:47,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-10 17:08:47,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-10 17:08:47,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-10 17:08:47,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-10 17:08:47,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-10 17:08:47,720 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-10 17:08:47,723 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-10 17:08:47,734 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-10 17:08:47,736 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-10 17:08:47,736 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-10 17:08:47,736 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-10 17:08:47,758 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-10 17:08:47,762 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-10 17:08:47,763 INFO org.mortbay.log: jetty-6.1.26
2015-12-10 17:08:48,020 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-10 17:08:48,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-10 17:08:48,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-10 17:08:48,294 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-10 17:08:48,312 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-10 17:08:48,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-10 17:08:48,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-10 17:08:48,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-10 17:08:48,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-10 17:08:48,405 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-10 17:08:48,406 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-10 17:08:48,786 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-10 17:08:48,824 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 26920@ubuntu
2015-12-10 17:08:48,895 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1841603009-127.0.1.1-1449456040486
2015-12-10 17:08:48,896 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-12-10 17:08:48,908 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-10 17:08:48,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1379726899;bpid=BP-1841603009-127.0.1.1-1449456040486;lv=-56;nsInfo=lv=-60;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0;bpid=BP-1841603009-127.0.1.1-1449456040486;dnuuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-10 17:08:48,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /home/parallels/hadoop/tmp/dfs/data/current
2015-12-10 17:08:48,999 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/parallels/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-10 17:08:49,058 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-10 17:08:49,062 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1449786842062 with interval 21600000
2015-12-10 17:08:49,062 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-10 17:08:49,068 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-10 17:08:49,099 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1841603009-127.0.1.1-1449456040486 on /home/parallels/hadoop/tmp/dfs/data/current: 32ms
2015-12-10 17:08:49,099 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1841603009-127.0.1.1-1449456040486: 37ms
2015-12-10 17:08:49,100 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-10 17:08:49,103 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current: 3ms
2015-12-10 17:08:49,103 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2015-12-10 17:08:49,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-10 17:08:49,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-10 17:08:49,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-10 17:08:49,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=116
2015-12-10 17:08:49,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000
2015-12-10 17:08:49,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 6 blocks total. Took 2 msec to generate and 109 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@78daafc0
2015-12-10 17:08:49,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-10 17:08:49,402 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-12-10 17:08:49,402 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-10 17:08:49,404 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-12-10 17:08:49,404 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-12-10 17:08:49,407 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-10 17:08:49,411 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1841603009-127.0.1.1-1449456040486 to blockPoolScannerMap, new size=1
2015-12-10 17:11:34,274 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741826 for deletion
2015-12-10 17:11:34,317 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2015-12-10 17:11:34,323 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741826_1002 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741826
2015-12-10 17:11:34,325 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741827_1003 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741827
2015-12-10 17:11:46,201 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741834_1010 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741834 for deletion
2015-12-10 17:11:46,203 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741834_1010 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741834
2015-12-10 17:15:08,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741838_1014 src: /127.0.0.1:45941 dest: /127.0.0.1:50010
2015-12-10 17:15:09,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:45941, dest: /127.0.0.1:50010, bytes: 7560294, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1425481013_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741838_1014, duration: 200445720
2015-12-10 17:15:09,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741838_1014, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:15:09,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741839_1015 src: /127.0.0.1:45942 dest: /127.0.0.1:50010
2015-12-10 17:15:09,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:45942, dest: /127.0.0.1:50010, bytes: 6065547, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1425481013_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741839_1015, duration: 40403728
2015-12-10 17:15:09,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741839_1015, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:15:09,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741840_1016 src: /127.0.0.1:45943 dest: /127.0.0.1:50010
2015-12-10 17:15:09,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:45943, dest: /127.0.0.1:50010, bytes: 8144694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1425481013_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741840_1016, duration: 98906227
2015-12-10 17:15:09,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741840_1016, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:15:19,787 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741838_1014
2015-12-10 17:15:32,419 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741840_1016
2015-12-10 17:15:38,219 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741839_1015
2015-12-10 17:15:49,238 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741840_1016 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741840 for deletion
2015-12-10 17:15:49,239 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741838_1014 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741838 for deletion
2015-12-10 17:15:49,239 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741839_1015 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741839 for deletion
2015-12-10 17:15:49,242 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741840_1016 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741840
2015-12-10 17:15:49,242 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741838_1014 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741838
2015-12-10 17:15:49,243 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741839_1015 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741839
2015-12-10 17:16:19,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741841_1017 src: /127.0.0.1:45949 dest: /127.0.0.1:50010
2015-12-10 17:16:19,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:45949, dest: /127.0.0.1:50010, bytes: 7560294, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1770453338_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741841_1017, duration: 79385630
2015-12-10 17:16:19,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741841_1017, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:16:19,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741842_1018 src: /127.0.0.1:45950 dest: /127.0.0.1:50010
2015-12-10 17:16:19,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:45950, dest: /127.0.0.1:50010, bytes: 6065547, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1770453338_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741842_1018, duration: 43983985
2015-12-10 17:16:19,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741842_1018, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:16:19,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741843_1019 src: /127.0.0.1:45951 dest: /127.0.0.1:50010
2015-12-10 17:16:19,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:45951, dest: /127.0.0.1:50010, bytes: 8144694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1770453338_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741843_1019, duration: 87604433
2015-12-10 17:16:19,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741843_1019, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:16:35,851 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741843_1019
2015-12-10 17:16:41,849 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741842_1018
2015-12-10 17:16:49,050 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741841_1017
2015-12-10 17:20:38,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741844_1020 src: /127.0.0.1:45968 dest: /127.0.0.1:50010
2015-12-10 17:20:38,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:45968, dest: /127.0.0.1:50010, bytes: 3173, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1251220522_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741844_1020, duration: 66450727
2015-12-10 17:20:38,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741844_1020, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:20:38,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741845_1021 src: /127.0.0.1:45969 dest: /127.0.0.1:50010
2015-12-10 17:20:38,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:45969, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1251220522_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741845_1021, duration: 2025051
2015-12-10 17:20:38,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741845_1021, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:20:39,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741846_1022 src: /127.0.0.1:45970 dest: /127.0.0.1:50010
2015-12-10 17:20:39,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:45970, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1251220522_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741846_1022, duration: 2156282
2015-12-10 17:20:39,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741846_1022, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:20:39,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741847_1023 src: /127.0.0.1:45971 dest: /127.0.0.1:50010
2015-12-10 17:20:39,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:45971, dest: /127.0.0.1:50010, bytes: 87255, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1251220522_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741847_1023, duration: 9373563
2015-12-10 17:20:39,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741847_1023, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:20:44,128 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741845_1021
2015-12-10 17:20:44,132 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741844_1020
2015-12-10 17:20:44,133 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741846_1022
2015-12-10 17:20:49,140 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741847_1023
2015-12-10 17:20:50,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741848_1024 src: /127.0.0.1:45980 dest: /127.0.0.1:50010
2015-12-10 17:20:50,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:45980, dest: /127.0.0.1:50010, bytes: 103801, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-480449843_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741848_1024, duration: 48999307
2015-12-10 17:20:50,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741848_1024, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:20:59,181 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741848_1024
2015-12-10 17:21:03,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741849_1025 src: /127.0.0.1:45996 dest: /127.0.0.1:50010
2015-12-10 17:21:21,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741850_1026 src: /127.0.0.1:46017 dest: /127.0.0.1:50010
2015-12-10 17:21:21,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46017, dest: /127.0.0.1:50010, bytes: 33, op: HDFS_WRITE, cliID: DFSClient_attempt_1449785342086_0001_r_000000_0_-2036151038_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741850_1026, duration: 37857768
2015-12-10 17:21:21,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741850_1026, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:21:21,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:45996, dest: /127.0.0.1:50010, bytes: 53517, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-480449843_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741849_1025, duration: 18433712150
2015-12-10 17:21:21,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741849_1025, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:21:21,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741851_1027 src: /127.0.0.1:46019 dest: /127.0.0.1:50010
2015-12-10 17:21:21,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46019, dest: /127.0.0.1:50010, bytes: 354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-480449843_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741851_1027, duration: 1948233
2015-12-10 17:21:21,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741851_1027, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:21:21,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741852_1028 src: /127.0.0.1:46021 dest: /127.0.0.1:50010
2015-12-10 17:21:21,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46021, dest: /127.0.0.1:50010, bytes: 53517, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-480449843_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741852_1028, duration: 5717896
2015-12-10 17:21:21,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741852_1028, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:21:21,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741853_1029 src: /127.0.0.1:46022 dest: /127.0.0.1:50010
2015-12-10 17:21:21,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46022, dest: /127.0.0.1:50010, bytes: 103801, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-480449843_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741853_1029, duration: 12388880
2015-12-10 17:21:21,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741853_1029, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:21:25,302 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741844_1020 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741844 for deletion
2015-12-10 17:21:25,305 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741845_1021 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741845 for deletion
2015-12-10 17:21:25,305 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741846_1022 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741846 for deletion
2015-12-10 17:21:25,305 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741844_1020 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741844
2015-12-10 17:21:25,306 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741847_1023 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741847 for deletion
2015-12-10 17:21:25,306 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741848_1024 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741848 for deletion
2015-12-10 17:21:25,306 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741845_1021 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741845
2015-12-10 17:21:25,306 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741849_1025 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741849 for deletion
2015-12-10 17:21:25,307 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741846_1022 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741846
2015-12-10 17:21:25,307 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741847_1023 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741847
2015-12-10 17:21:25,308 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741848_1024 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741848
2015-12-10 17:21:25,308 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741849_1025 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741849
2015-12-10 17:21:29,240 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741852_1028
2015-12-10 17:21:29,241 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741851_1027
2015-12-10 17:21:29,243 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741850_1026
2015-12-10 17:21:29,246 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741853_1029
2015-12-10 17:24:31,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741850_1026 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741850 for deletion
2015-12-10 17:24:31,331 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741850_1026 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741850
2015-12-10 17:24:35,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741854_1030 src: /127.0.0.1:46044 dest: /127.0.0.1:50010
2015-12-10 17:24:35,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46044, dest: /127.0.0.1:50010, bytes: 3173, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-782895968_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741854_1030, duration: 64225743
2015-12-10 17:24:35,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741854_1030, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:24:36,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741855_1031 src: /127.0.0.1:46045 dest: /127.0.0.1:50010
2015-12-10 17:24:36,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46045, dest: /127.0.0.1:50010, bytes: 153, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-782895968_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741855_1031, duration: 6980529
2015-12-10 17:24:36,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741855_1031, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:24:36,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741856_1032 src: /127.0.0.1:46046 dest: /127.0.0.1:50010
2015-12-10 17:24:36,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46046, dest: /127.0.0.1:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-782895968_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741856_1032, duration: 2080523
2015-12-10 17:24:36,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741856_1032, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:24:36,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741857_1033 src: /127.0.0.1:46047 dest: /127.0.0.1:50010
2015-12-10 17:24:36,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46047, dest: /127.0.0.1:50010, bytes: 87284, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-782895968_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741857_1033, duration: 3450043
2015-12-10 17:24:36,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741857_1033, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:24:44,296 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741857_1033
2015-12-10 17:24:44,297 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741856_1032
2015-12-10 17:24:44,299 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741855_1031
2015-12-10 17:24:44,300 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741854_1030
2015-12-10 17:24:44,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741858_1034 src: /127.0.0.1:46056 dest: /127.0.0.1:50010
2015-12-10 17:24:44,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46056, dest: /127.0.0.1:50010, bytes: 103830, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1071761326_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741858_1034, duration: 84343680
2015-12-10 17:24:44,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741858_1034, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:24:54,384 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741858_1034
2015-12-10 17:24:56,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741859_1035 src: /127.0.0.1:46062 dest: /127.0.0.1:50010
2015-12-10 17:25:03,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741860_1036 src: /127.0.0.1:46068 dest: /127.0.0.1:50010
2015-12-10 17:25:03,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46068, dest: /127.0.0.1:50010, bytes: 32, op: HDFS_WRITE, cliID: DFSClient_attempt_1449785342086_0002_r_000000_0_-150447708_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741860_1036, duration: 92705732
2015-12-10 17:25:03,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741860_1036, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:25:03,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46062, dest: /127.0.0.1:50010, bytes: 33606, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1071761326_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741859_1035, duration: 7701869544
2015-12-10 17:25:03,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741859_1035, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:25:03,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741861_1037 src: /127.0.0.1:46070 dest: /127.0.0.1:50010
2015-12-10 17:25:03,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46070, dest: /127.0.0.1:50010, bytes: 352, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1071761326_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741861_1037, duration: 2042521
2015-12-10 17:25:03,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741861_1037, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:25:04,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741862_1038 src: /127.0.0.1:46072 dest: /127.0.0.1:50010
2015-12-10 17:25:04,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46072, dest: /127.0.0.1:50010, bytes: 33606, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1071761326_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741862_1038, duration: 3846129
2015-12-10 17:25:04,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741862_1038, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:25:04,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741863_1039 src: /127.0.0.1:46073 dest: /127.0.0.1:50010
2015-12-10 17:25:04,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46073, dest: /127.0.0.1:50010, bytes: 103830, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1071761326_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741863_1039, duration: 21010620
2015-12-10 17:25:04,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741863_1039, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:25:07,313 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741856_1032 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741856 for deletion
2015-12-10 17:25:07,314 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741857_1033 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741857 for deletion
2015-12-10 17:25:07,315 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741858_1034 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741858 for deletion
2015-12-10 17:25:07,315 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741859_1035 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741859 for deletion
2015-12-10 17:25:07,315 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741854_1030 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741854 for deletion
2015-12-10 17:25:07,315 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741855_1031 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741855 for deletion
2015-12-10 17:25:07,316 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741856_1032 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741856
2015-12-10 17:25:07,317 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741857_1033 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741857
2015-12-10 17:25:07,317 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741858_1034 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741858
2015-12-10 17:25:07,317 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741859_1035 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741859
2015-12-10 17:25:07,317 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741854_1030 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741854
2015-12-10 17:25:07,317 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741855_1031 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741855
2015-12-10 17:25:09,403 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741860_1036
2015-12-10 17:25:09,404 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741863_1039
2015-12-10 17:25:09,406 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741862_1038
2015-12-10 17:25:09,407 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741861_1037
2015-12-10 17:30:52,356 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741860_1036 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741860 for deletion
2015-12-10 17:30:52,357 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741860_1036 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741860
2015-12-10 17:33:23,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741864_1040 src: /127.0.0.1:46095 dest: /127.0.0.1:50010
2015-12-10 17:33:23,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46095, dest: /127.0.0.1:50010, bytes: 3222, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-309811808_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741864_1040, duration: 59972205
2015-12-10 17:33:23,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741864_1040, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:33:23,250 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741865_1041 src: /127.0.0.1:46096 dest: /127.0.0.1:50010
2015-12-10 17:33:23,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46096, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-309811808_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741865_1041, duration: 1963008
2015-12-10 17:33:23,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741865_1041, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:33:23,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741866_1042 src: /127.0.0.1:46097 dest: /127.0.0.1:50010
2015-12-10 17:33:23,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46097, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-309811808_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741866_1042, duration: 1953413
2015-12-10 17:33:23,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741866_1042, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:33:23,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741867_1043 src: /127.0.0.1:46098 dest: /127.0.0.1:50010
2015-12-10 17:33:23,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46098, dest: /127.0.0.1:50010, bytes: 87270, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-309811808_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741867_1043, duration: 17079877
2015-12-10 17:33:23,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741867_1043, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:33:29,468 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741867_1043
2015-12-10 17:33:29,469 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741866_1042
2015-12-10 17:33:29,469 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741865_1041
2015-12-10 17:33:29,471 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741864_1040
2015-12-10 17:33:29,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741868_1044 src: /127.0.0.1:46106 dest: /127.0.0.1:50010
2015-12-10 17:33:30,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46106, dest: /127.0.0.1:50010, bytes: 103816, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-204081518_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741868_1044, duration: 138644231
2015-12-10 17:33:30,116 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741868_1044, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:33:39,506 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741868_1044
2015-12-10 17:33:43,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741869_1045 src: /127.0.0.1:46124 dest: /127.0.0.1:50010
2015-12-10 17:33:52,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741870_1046 src: /127.0.0.1:46137 dest: /127.0.0.1:50010
2015-12-10 17:33:53,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46137, dest: /127.0.0.1:50010, bytes: 33, op: HDFS_WRITE, cliID: DFSClient_attempt_1449785342086_0003_r_000000_0_-1391354756_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741870_1046, duration: 128574916
2015-12-10 17:33:53,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741870_1046, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:33:54,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46124, dest: /127.0.0.1:50010, bytes: 48645, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-204081518_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741869_1045, duration: 10923252909
2015-12-10 17:33:54,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741869_1045, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:33:54,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741871_1047 src: /127.0.0.1:46140 dest: /127.0.0.1:50010
2015-12-10 17:33:54,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46140, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-204081518_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741871_1047, duration: 1484264
2015-12-10 17:33:54,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741871_1047, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:33:54,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741872_1048 src: /127.0.0.1:46142 dest: /127.0.0.1:50010
2015-12-10 17:33:54,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46142, dest: /127.0.0.1:50010, bytes: 48645, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-204081518_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741872_1048, duration: 8240117
2015-12-10 17:33:54,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741872_1048, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:33:54,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741873_1049 src: /127.0.0.1:46143 dest: /127.0.0.1:50010
2015-12-10 17:33:54,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46143, dest: /127.0.0.1:50010, bytes: 103816, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-204081518_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741873_1049, duration: 3075822
2015-12-10 17:33:54,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741873_1049, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 17:33:58,393 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741864_1040 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741864 for deletion
2015-12-10 17:33:58,396 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741865_1041 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741865 for deletion
2015-12-10 17:33:58,396 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741866_1042 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741866 for deletion
2015-12-10 17:33:58,396 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741867_1043 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741867 for deletion
2015-12-10 17:33:58,396 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741868_1044 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741868 for deletion
2015-12-10 17:33:58,396 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741869_1045 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741869 for deletion
2015-12-10 17:33:58,399 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741864_1040 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741864
2015-12-10 17:33:58,401 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741865_1041 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741865
2015-12-10 17:33:58,402 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741866_1042 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741866
2015-12-10 17:33:58,403 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741867_1043 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741867
2015-12-10 17:33:58,403 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741868_1044 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741868
2015-12-10 17:33:58,404 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741869_1045 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741869
2015-12-10 17:34:02,161 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1841603009-127.0.1.1-1449456040486 Total blocks: 16, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-12-10 17:34:04,538 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741871_1047
2015-12-10 17:34:04,540 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741873_1049
2015-12-10 17:34:04,542 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741872_1048
2015-12-10 17:34:04,543 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741870_1046
2015-12-10 17:34:43,405 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "ubuntu/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-12-10 17:34:46,223 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-10 17:34:46,237 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-10 21:55:40,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-10 21:55:40,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-10 21:55:41,077 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-10 21:55:41,162 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-10 21:55:41,162 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-10 21:55:41,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-10 21:55:41,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-10 21:55:41,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-10 21:55:41,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-10 21:55:41,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-10 21:55:41,323 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-10 21:55:41,326 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-10 21:55:41,335 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-10 21:55:41,337 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-10 21:55:41,337 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-10 21:55:41,337 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-10 21:55:41,362 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-10 21:55:41,370 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-10 21:55:41,370 INFO org.mortbay.log: jetty-6.1.26
2015-12-10 21:55:41,648 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-10 21:55:41,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-10 21:55:41,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-10 21:55:41,879 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-10 21:55:41,902 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-10 21:55:41,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-10 21:55:41,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-10 21:55:41,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-10 21:55:41,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-10 21:55:41,983 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-10 21:55:41,984 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-10 21:55:42,321 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-10 21:55:42,327 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 26434@ubuntu
2015-12-10 21:55:42,454 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1841603009-127.0.1.1-1449456040486
2015-12-10 21:55:42,454 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-12-10 21:55:42,455 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-10 21:55:42,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1379726899;bpid=BP-1841603009-127.0.1.1-1449456040486;lv=-56;nsInfo=lv=-60;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0;bpid=BP-1841603009-127.0.1.1-1449456040486;dnuuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-10 21:55:42,518 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /home/parallels/hadoop/tmp/dfs/data/current
2015-12-10 21:55:42,520 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/parallels/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-10 21:55:42,564 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-10 21:55:42,568 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1449813774568 with interval 21600000
2015-12-10 21:55:42,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-10 21:55:42,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-10 21:55:42,594 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1841603009-127.0.1.1-1449456040486 on /home/parallels/hadoop/tmp/dfs/data/current: 24ms
2015-12-10 21:55:42,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1841603009-127.0.1.1-1449456040486: 26ms
2015-12-10 21:55:42,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-10 21:55:42,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current: 10ms
2015-12-10 21:55:42,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 10ms
2015-12-10 21:55:42,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-10 21:55:42,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-10 21:55:42,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-10 21:55:42,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=404
2015-12-10 21:55:42,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000
2015-12-10 21:55:42,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 16 blocks total. Took 3 msec to generate and 69 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@53d4287
2015-12-10 21:55:42,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-10 21:55:42,920 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-12-10 21:55:42,920 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-10 21:55:42,921 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-12-10 21:55:42,921 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-12-10 21:55:42,923 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-10 21:55:42,928 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1841603009-127.0.1.1-1449456040486 to blockPoolScannerMap, new size=1
2015-12-10 21:56:30,765 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741870_1046 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741870 for deletion
2015-12-10 21:56:30,773 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741870_1046 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741870
2015-12-10 21:56:46,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741874_1050 src: /127.0.0.1:46183 dest: /127.0.0.1:50010
2015-12-10 21:56:46,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46183, dest: /127.0.0.1:50010, bytes: 4272, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2131924496_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741874_1050, duration: 94277598
2015-12-10 21:56:46,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741874_1050, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 21:56:46,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741875_1051 src: /127.0.0.1:46184 dest: /127.0.0.1:50010
2015-12-10 21:56:46,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46184, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2131924496_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741875_1051, duration: 2152184
2015-12-10 21:56:46,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741875_1051, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 21:56:46,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741876_1052 src: /127.0.0.1:46185 dest: /127.0.0.1:50010
2015-12-10 21:56:46,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46185, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2131924496_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741876_1052, duration: 2094006
2015-12-10 21:56:46,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741876_1052, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 21:56:47,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741877_1053 src: /127.0.0.1:46186 dest: /127.0.0.1:50010
2015-12-10 21:56:47,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46186, dest: /127.0.0.1:50010, bytes: 87270, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2131924496_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741877_1053, duration: 27818897
2015-12-10 21:56:47,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741877_1053, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 21:56:52,655 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741877_1053
2015-12-10 21:56:52,657 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741874_1050
2015-12-10 21:56:52,658 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741876_1052
2015-12-10 21:56:52,659 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741875_1051
2015-12-10 21:56:59,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741878_1054 src: /127.0.0.1:46197 dest: /127.0.0.1:50010
2015-12-10 21:56:59,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46197, dest: /127.0.0.1:50010, bytes: 103816, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_474703413_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741878_1054, duration: 55627720
2015-12-10 21:56:59,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741878_1054, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 21:57:07,854 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741878_1054
2015-12-10 21:57:15,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741879_1055 src: /127.0.0.1:46215 dest: /127.0.0.1:50010
2015-12-10 21:57:56,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46215, dest: /127.0.0.1:50010, bytes: 49452, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_474703413_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741879_1055, duration: 40794776495
2015-12-10 21:57:56,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741879_1055, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 21:57:56,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741880_1056 src: /127.0.0.1:46265 dest: /127.0.0.1:50010
2015-12-10 21:57:56,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46265, dest: /127.0.0.1:50010, bytes: 339, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_474703413_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741880_1056, duration: 2139080
2015-12-10 21:57:56,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741880_1056, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 21:57:56,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741881_1057 src: /127.0.0.1:46267 dest: /127.0.0.1:50010
2015-12-10 21:57:56,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46267, dest: /127.0.0.1:50010, bytes: 49452, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_474703413_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741881_1057, duration: 64744461
2015-12-10 21:57:56,662 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741881_1057, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 21:57:56,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741882_1058 src: /127.0.0.1:46268 dest: /127.0.0.1:50010
2015-12-10 21:57:56,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46268, dest: /127.0.0.1:50010, bytes: 103816, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_474703413_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741882_1058, duration: 14136863
2015-12-10 21:57:56,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741882_1058, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 21:58:00,843 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741874_1050 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741874 for deletion
2015-12-10 21:58:00,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741875_1051 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741875 for deletion
2015-12-10 21:58:00,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741874_1050 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741874
2015-12-10 21:58:00,848 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741876_1052 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741876 for deletion
2015-12-10 21:58:00,848 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741875_1051 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741875
2015-12-10 21:58:00,848 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741877_1053 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741877 for deletion
2015-12-10 21:58:00,848 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741876_1052 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741876
2015-12-10 21:58:00,848 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741878_1054 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741878 for deletion
2015-12-10 21:58:00,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741879_1055 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741879 for deletion
2015-12-10 21:58:00,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741877_1053 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741877
2015-12-10 21:58:00,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741878_1054 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741878
2015-12-10 21:58:00,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741879_1055 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741879
2015-12-10 21:58:02,902 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741880_1056
2015-12-10 21:58:02,903 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741882_1058
2015-12-10 21:58:02,906 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741881_1057
2015-12-10 22:19:39,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from localhost/127.0.0.1:9000 with active state
2015-12-10 22:19:39,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-10 22:19:39,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-10 22:19:39,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 18 blocks total. Took 1 msec to generate and 9 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@5b649d6f
2015-12-10 22:19:39,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-10 22:38:19,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741883_1059 src: /127.0.0.1:46301 dest: /127.0.0.1:50010
2015-12-10 22:38:19,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46301, dest: /127.0.0.1:50010, bytes: 4253, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-205624779_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741883_1059, duration: 54572824
2015-12-10 22:38:19,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741883_1059, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:38:19,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741884_1060 src: /127.0.0.1:46302 dest: /127.0.0.1:50010
2015-12-10 22:38:19,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46302, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-205624779_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741884_1060, duration: 1920807
2015-12-10 22:38:19,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741884_1060, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:38:19,587 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741885_1061 src: /127.0.0.1:46303 dest: /127.0.0.1:50010
2015-12-10 22:38:19,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46303, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-205624779_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741885_1061, duration: 1947996
2015-12-10 22:38:19,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741885_1061, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:38:19,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741886_1062 src: /127.0.0.1:46304 dest: /127.0.0.1:50010
2015-12-10 22:38:19,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46304, dest: /127.0.0.1:50010, bytes: 87263, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-205624779_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741886_1062, duration: 9049701
2015-12-10 22:38:19,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741886_1062, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:38:27,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741887_1063 src: /127.0.0.1:46312 dest: /127.0.0.1:50010
2015-12-10 22:38:27,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46312, dest: /127.0.0.1:50010, bytes: 103809, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_73091126_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741887_1063, duration: 51652231
2015-12-10 22:38:27,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741887_1063, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:38:29,066 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741885_1061
2015-12-10 22:38:29,072 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741883_1059
2015-12-10 22:38:29,073 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741886_1062
2015-12-10 22:38:29,074 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741884_1060
2015-12-10 22:38:34,173 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741887_1063
2015-12-10 22:38:41,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741888_1064 src: /127.0.0.1:46329 dest: /127.0.0.1:50010
2015-12-10 22:39:05,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46329, dest: /127.0.0.1:50010, bytes: 43315, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_73091126_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741888_1064, duration: 24204880607
2015-12-10 22:39:05,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741888_1064, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:39:05,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741889_1065 src: /127.0.0.1:46371 dest: /127.0.0.1:50010
2015-12-10 22:39:05,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46371, dest: /127.0.0.1:50010, bytes: 338, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_73091126_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741889_1065, duration: 3003335
2015-12-10 22:39:05,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741889_1065, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:39:05,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741890_1066 src: /127.0.0.1:46373 dest: /127.0.0.1:50010
2015-12-10 22:39:05,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46373, dest: /127.0.0.1:50010, bytes: 43315, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_73091126_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741890_1066, duration: 7063010
2015-12-10 22:39:05,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741890_1066, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:39:06,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741891_1067 src: /127.0.0.1:46374 dest: /127.0.0.1:50010
2015-12-10 22:39:06,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46374, dest: /127.0.0.1:50010, bytes: 103809, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_73091126_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741891_1067, duration: 6709482
2015-12-10 22:39:06,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741891_1067, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:39:10,117 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741888_1064 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741888 for deletion
2015-12-10 22:39:10,120 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741883_1059 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741883 for deletion
2015-12-10 22:39:10,120 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741888_1064 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741888
2015-12-10 22:39:10,120 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741884_1060 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741884 for deletion
2015-12-10 22:39:10,120 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741885_1061 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741885 for deletion
2015-12-10 22:39:10,121 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741886_1062 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741886 for deletion
2015-12-10 22:39:10,122 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741887_1063 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741887 for deletion
2015-12-10 22:39:10,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741883_1059 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741883
2015-12-10 22:39:10,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741884_1060 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741884
2015-12-10 22:39:10,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741885_1061 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741885
2015-12-10 22:39:10,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741886_1062 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741886
2015-12-10 22:39:10,124 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741887_1063 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741887
2015-12-10 22:39:14,318 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741891_1067
2015-12-10 22:39:14,320 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741890_1066
2015-12-10 22:39:14,320 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741889_1065
2015-12-10 22:50:35,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741892_1068 src: /127.0.0.1:46390 dest: /127.0.0.1:50010
2015-12-10 22:50:35,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46390, dest: /127.0.0.1:50010, bytes: 4234, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1365974731_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741892_1068, duration: 50746494
2015-12-10 22:50:35,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741892_1068, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:50:35,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741893_1069 src: /127.0.0.1:46391 dest: /127.0.0.1:50010
2015-12-10 22:50:35,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46391, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1365974731_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741893_1069, duration: 2322691
2015-12-10 22:50:35,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741893_1069, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:50:35,924 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741894_1070 src: /127.0.0.1:46392 dest: /127.0.0.1:50010
2015-12-10 22:50:35,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46392, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1365974731_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741894_1070, duration: 1919621
2015-12-10 22:50:35,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741894_1070, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:50:36,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741895_1071 src: /127.0.0.1:46393 dest: /127.0.0.1:50010
2015-12-10 22:50:36,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46393, dest: /127.0.0.1:50010, bytes: 87133, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1365974731_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741895_1071, duration: 10158536
2015-12-10 22:50:36,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741895_1071, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:50:42,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741896_1072 src: /127.0.0.1:46402 dest: /127.0.0.1:50010
2015-12-10 22:50:42,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46402, dest: /127.0.0.1:50010, bytes: 103655, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-427384879_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741896_1072, duration: 44425869
2015-12-10 22:50:42,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741896_1072, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:50:44,402 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741893_1069
2015-12-10 22:50:44,404 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741894_1070
2015-12-10 22:50:44,405 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741892_1068
2015-12-10 22:50:44,408 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741895_1071
2015-12-10 22:50:49,450 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741896_1072
2015-12-10 22:50:55,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741897_1073 src: /127.0.0.1:46421 dest: /127.0.0.1:50010
2015-12-10 22:51:32,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46421, dest: /127.0.0.1:50010, bytes: 44640, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-427384879_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741897_1073, duration: 36840914149
2015-12-10 22:51:32,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741897_1073, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:51:32,742 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741898_1074 src: /127.0.0.1:46469 dest: /127.0.0.1:50010
2015-12-10 22:51:32,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46469, dest: /127.0.0.1:50010, bytes: 339, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-427384879_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741898_1074, duration: 5122597
2015-12-10 22:51:32,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741898_1074, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:51:32,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741899_1075 src: /127.0.0.1:46471 dest: /127.0.0.1:50010
2015-12-10 22:51:32,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46471, dest: /127.0.0.1:50010, bytes: 44640, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-427384879_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741899_1075, duration: 6869263
2015-12-10 22:51:32,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741899_1075, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:51:33,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741900_1076 src: /127.0.0.1:46472 dest: /127.0.0.1:50010
2015-12-10 22:51:33,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46472, dest: /127.0.0.1:50010, bytes: 103655, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-427384879_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741900_1076, duration: 12463184
2015-12-10 22:51:33,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741900_1076, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:51:37,441 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741892_1068 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741892 for deletion
2015-12-10 22:51:37,446 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741893_1069 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741893 for deletion
2015-12-10 22:51:37,446 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741894_1070 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741894 for deletion
2015-12-10 22:51:37,446 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741895_1071 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741895 for deletion
2015-12-10 22:51:37,447 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741892_1068 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741892
2015-12-10 22:51:37,447 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741896_1072 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741896 for deletion
2015-12-10 22:51:37,447 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741897_1073 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741897 for deletion
2015-12-10 22:51:37,448 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741893_1069 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741893
2015-12-10 22:51:37,449 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741894_1070 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741894
2015-12-10 22:51:37,449 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741895_1071 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741895
2015-12-10 22:51:37,449 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741896_1072 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741896
2015-12-10 22:51:37,450 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741897_1073 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741897
2015-12-10 22:51:39,497 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741900_1076
2015-12-10 22:51:39,498 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741899_1075
2015-12-10 22:51:39,499 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741898_1074
2015-12-10 22:56:19,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741901_1077 src: /127.0.0.1:46486 dest: /127.0.0.1:50010
2015-12-10 22:56:19,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46486, dest: /127.0.0.1:50010, bytes: 4257, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1798564717_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741901_1077, duration: 54200227
2015-12-10 22:56:19,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741901_1077, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:56:19,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741902_1078 src: /127.0.0.1:46487 dest: /127.0.0.1:50010
2015-12-10 22:56:19,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46487, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1798564717_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741902_1078, duration: 2129124
2015-12-10 22:56:19,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741902_1078, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:56:19,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741903_1079 src: /127.0.0.1:46488 dest: /127.0.0.1:50010
2015-12-10 22:56:19,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46488, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1798564717_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741903_1079, duration: 1854195
2015-12-10 22:56:19,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741903_1079, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:56:19,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741904_1080 src: /127.0.0.1:46489 dest: /127.0.0.1:50010
2015-12-10 22:56:19,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46489, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1798564717_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741904_1080, duration: 26895471
2015-12-10 22:56:19,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741904_1080, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:56:24,578 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741901_1077
2015-12-10 22:56:24,579 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741902_1078
2015-12-10 22:56:24,581 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741904_1080
2015-12-10 22:56:24,582 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741903_1079
2015-12-10 22:56:25,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741905_1081 src: /127.0.0.1:46497 dest: /127.0.0.1:50010
2015-12-10 22:56:25,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46497, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1672126040_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741905_1081, duration: 106117763
2015-12-10 22:56:25,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741905_1081, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:56:34,648 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741905_1081
2015-12-10 22:56:40,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741906_1082 src: /127.0.0.1:46513 dest: /127.0.0.1:50010
2015-12-10 22:56:51,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741907_1083 src: /127.0.0.1:46527 dest: /127.0.0.1:50010
2015-12-10 22:56:51,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46527, dest: /127.0.0.1:50010, bytes: 60, op: HDFS_WRITE, cliID: DFSClient_attempt_1449802555556_0004_r_000000_0_1093380799_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741907_1083, duration: 41500764
2015-12-10 22:56:51,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741907_1083, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:56:51,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46513, dest: /127.0.0.1:50010, bytes: 49832, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1672126040_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741906_1082, duration: 11649843047
2015-12-10 22:56:51,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741906_1082, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:56:51,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741908_1084 src: /127.0.0.1:46530 dest: /127.0.0.1:50010
2015-12-10 22:56:51,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46530, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1672126040_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741908_1084, duration: 1995694
2015-12-10 22:56:51,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741908_1084, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:56:51,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741909_1085 src: /127.0.0.1:46532 dest: /127.0.0.1:50010
2015-12-10 22:56:51,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46532, dest: /127.0.0.1:50010, bytes: 49832, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1672126040_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741909_1085, duration: 6587046
2015-12-10 22:56:51,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741909_1085, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:56:51,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741910_1086 src: /127.0.0.1:46533 dest: /127.0.0.1:50010
2015-12-10 22:56:51,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46533, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1672126040_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741910_1086, duration: 6526438
2015-12-10 22:56:52,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741910_1086, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 22:56:55,541 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741904_1080 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741904 for deletion
2015-12-10 22:56:55,543 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741905_1081 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741905 for deletion
2015-12-10 22:56:55,544 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741906_1082 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741906 for deletion
2015-12-10 22:56:55,545 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741901_1077 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741901 for deletion
2015-12-10 22:56:55,545 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741902_1078 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741902 for deletion
2015-12-10 22:56:55,545 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741903_1079 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741903 for deletion
2015-12-10 22:56:55,545 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741904_1080 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741904
2015-12-10 22:56:55,546 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741905_1081 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741905
2015-12-10 22:56:55,546 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741906_1082 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741906
2015-12-10 22:56:55,547 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741901_1077 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741901
2015-12-10 22:56:55,548 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741902_1078 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741902
2015-12-10 22:56:55,548 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741903_1079 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741903
2015-12-10 22:56:59,763 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741907_1083
2015-12-10 22:56:59,765 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741910_1086
2015-12-10 22:56:59,767 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741909_1085
2015-12-10 22:56:59,768 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741908_1084
2015-12-10 23:03:52,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741907_1083 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741907 for deletion
2015-12-10 23:03:52,597 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741907_1083 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741907
2015-12-10 23:03:54,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741911_1087 src: /127.0.0.1:46550 dest: /127.0.0.1:50010
2015-12-10 23:03:54,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46550, dest: /127.0.0.1:50010, bytes: 4256, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_275638596_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741911_1087, duration: 45124050
2015-12-10 23:03:54,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741911_1087, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:03:54,979 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741912_1088 src: /127.0.0.1:46551 dest: /127.0.0.1:50010
2015-12-10 23:03:54,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46551, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_275638596_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741912_1088, duration: 6695429
2015-12-10 23:03:54,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741912_1088, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:03:55,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741913_1089 src: /127.0.0.1:46552 dest: /127.0.0.1:50010
2015-12-10 23:03:55,017 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46552, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_275638596_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741913_1089, duration: 1976075
2015-12-10 23:03:55,017 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741913_1089, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:03:55,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741914_1090 src: /127.0.0.1:46553 dest: /127.0.0.1:50010
2015-12-10 23:03:55,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46553, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_275638596_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741914_1090, duration: 19835240
2015-12-10 23:03:55,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741914_1090, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:04:00,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741915_1091 src: /127.0.0.1:46561 dest: /127.0.0.1:50010
2015-12-10 23:04:00,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46561, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_27097967_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741915_1091, duration: 30902165
2015-12-10 23:04:00,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741915_1091, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:04:04,841 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741914_1090
2015-12-10 23:04:04,843 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741913_1089
2015-12-10 23:04:04,844 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741912_1088
2015-12-10 23:04:04,850 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741911_1087
2015-12-10 23:04:09,857 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741915_1091
2015-12-10 23:04:14,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741916_1092 src: /127.0.0.1:46579 dest: /127.0.0.1:50010
2015-12-10 23:04:25,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741917_1093 src: /127.0.0.1:46591 dest: /127.0.0.1:50010
2015-12-10 23:04:25,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46591, dest: /127.0.0.1:50010, bytes: 60, op: HDFS_WRITE, cliID: DFSClient_attempt_1449802555556_0005_r_000000_0_-467405944_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741917_1093, duration: 99260714
2015-12-10 23:04:25,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741917_1093, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:04:25,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46579, dest: /127.0.0.1:50010, bytes: 49795, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_27097967_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741916_1092, duration: 11555744963
2015-12-10 23:04:25,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741916_1092, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:04:25,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741918_1094 src: /127.0.0.1:46594 dest: /127.0.0.1:50010
2015-12-10 23:04:25,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46594, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_27097967_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741918_1094, duration: 9983311
2015-12-10 23:04:25,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741918_1094, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:04:25,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741919_1095 src: /127.0.0.1:46596 dest: /127.0.0.1:50010
2015-12-10 23:04:25,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46596, dest: /127.0.0.1:50010, bytes: 49795, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_27097967_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741919_1095, duration: 3237207
2015-12-10 23:04:25,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741919_1095, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:04:25,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741920_1096 src: /127.0.0.1:46597 dest: /127.0.0.1:50010
2015-12-10 23:04:25,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46597, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_27097967_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741920_1096, duration: 4419211
2015-12-10 23:04:25,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741920_1096, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:04:28,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741911_1087 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741911 for deletion
2015-12-10 23:04:28,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741912_1088 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741912 for deletion
2015-12-10 23:04:28,619 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741913_1089 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741913 for deletion
2015-12-10 23:04:28,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741914_1090 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741914 for deletion
2015-12-10 23:04:28,622 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741915_1091 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741915 for deletion
2015-12-10 23:04:28,622 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741911_1087 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741911
2015-12-10 23:04:28,623 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741916_1092 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741916 for deletion
2015-12-10 23:04:28,623 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741912_1088 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741912
2015-12-10 23:04:28,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741913_1089 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741913
2015-12-10 23:04:28,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741914_1090 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741914
2015-12-10 23:04:28,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741915_1091 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741915
2015-12-10 23:04:28,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741916_1092 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741916
2015-12-10 23:04:34,882 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741917_1093
2015-12-10 23:04:34,884 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741919_1095
2015-12-10 23:04:34,885 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741918_1094
2015-12-10 23:04:34,886 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741920_1096
2015-12-10 23:10:04,647 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741917_1093 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741917 for deletion
2015-12-10 23:10:04,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741917_1093 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741917
2015-12-10 23:10:07,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741921_1097 src: /127.0.0.1:46613 dest: /127.0.0.1:50010
2015-12-10 23:10:07,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46613, dest: /127.0.0.1:50010, bytes: 4253, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-632804457_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741921_1097, duration: 45969398
2015-12-10 23:10:07,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741921_1097, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:10:07,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741922_1098 src: /127.0.0.1:46614 dest: /127.0.0.1:50010
2015-12-10 23:10:07,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46614, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-632804457_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741922_1098, duration: 3286290
2015-12-10 23:10:07,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741922_1098, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:10:07,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741923_1099 src: /127.0.0.1:46615 dest: /127.0.0.1:50010
2015-12-10 23:10:07,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46615, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-632804457_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741923_1099, duration: 2409106
2015-12-10 23:10:07,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741923_1099, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:10:08,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741924_1100 src: /127.0.0.1:46616 dest: /127.0.0.1:50010
2015-12-10 23:10:08,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46616, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-632804457_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741924_1100, duration: 11313375
2015-12-10 23:10:08,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741924_1100, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:10:13,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741925_1101 src: /127.0.0.1:46624 dest: /127.0.0.1:50010
2015-12-10 23:10:13,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46624, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1536706009_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741925_1101, duration: 40993428
2015-12-10 23:10:13,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741925_1101, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:10:14,945 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741921_1097
2015-12-10 23:10:14,947 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741924_1100
2015-12-10 23:10:14,948 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741923_1099
2015-12-10 23:10:14,948 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741922_1098
2015-12-10 23:10:20,014 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741925_1101
2015-12-10 23:10:29,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741926_1102 src: /127.0.0.1:46643 dest: /127.0.0.1:50010
2015-12-10 23:10:39,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741927_1103 src: /127.0.0.1:46655 dest: /127.0.0.1:50010
2015-12-10 23:10:39,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46655, dest: /127.0.0.1:50010, bytes: 60, op: HDFS_WRITE, cliID: DFSClient_attempt_1449802555556_0006_r_000000_0_1562035676_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741927_1103, duration: 95945732
2015-12-10 23:10:39,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741927_1103, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:10:40,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46643, dest: /127.0.0.1:50010, bytes: 49844, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1536706009_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741926_1102, duration: 11700594897
2015-12-10 23:10:40,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741926_1102, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:10:40,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741928_1104 src: /127.0.0.1:46658 dest: /127.0.0.1:50010
2015-12-10 23:10:40,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46658, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1536706009_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741928_1104, duration: 2868355
2015-12-10 23:10:40,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741928_1104, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:10:40,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741929_1105 src: /127.0.0.1:46660 dest: /127.0.0.1:50010
2015-12-10 23:10:40,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46660, dest: /127.0.0.1:50010, bytes: 49844, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1536706009_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741929_1105, duration: 5043946
2015-12-10 23:10:40,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741929_1105, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:10:40,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741930_1106 src: /127.0.0.1:46661 dest: /127.0.0.1:50010
2015-12-10 23:10:40,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46661, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1536706009_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741930_1106, duration: 6101219
2015-12-10 23:10:40,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741930_1106, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:10:43,655 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741921_1097 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741921 for deletion
2015-12-10 23:10:43,656 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741922_1098 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741922 for deletion
2015-12-10 23:10:43,658 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741923_1099 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741923 for deletion
2015-12-10 23:10:43,658 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741924_1100 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741924 for deletion
2015-12-10 23:10:43,659 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741925_1101 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741925 for deletion
2015-12-10 23:10:43,659 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741926_1102 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741926 for deletion
2015-12-10 23:10:43,659 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741921_1097 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741921
2015-12-10 23:10:43,660 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741922_1098 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741922
2015-12-10 23:10:43,660 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741923_1099 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741923
2015-12-10 23:10:43,660 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741924_1100 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741924
2015-12-10 23:10:43,660 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741925_1101 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741925
2015-12-10 23:10:43,661 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741926_1102 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741926
2015-12-10 23:10:45,046 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741927_1103
2015-12-10 23:10:50,139 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741928_1104
2015-12-10 23:10:50,141 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741929_1105
2015-12-10 23:10:50,143 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741930_1106
2015-12-10 23:15:25,692 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741927_1103 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741927 for deletion
2015-12-10 23:15:25,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741927_1103 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741927
2015-12-10 23:15:31,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741931_1107 src: /127.0.0.1:46674 dest: /127.0.0.1:50010
2015-12-10 23:15:31,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46674, dest: /127.0.0.1:50010, bytes: 4253, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1464273389_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741931_1107, duration: 48528542
2015-12-10 23:15:31,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741931_1107, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:15:31,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741932_1108 src: /127.0.0.1:46675 dest: /127.0.0.1:50010
2015-12-10 23:15:31,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46675, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1464273389_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741932_1108, duration: 5609809
2015-12-10 23:15:31,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741932_1108, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:15:31,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741933_1109 src: /127.0.0.1:46676 dest: /127.0.0.1:50010
2015-12-10 23:15:31,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46676, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1464273389_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741933_1109, duration: 1862960
2015-12-10 23:15:31,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741933_1109, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:15:31,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741934_1110 src: /127.0.0.1:46677 dest: /127.0.0.1:50010
2015-12-10 23:15:31,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46677, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1464273389_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741934_1110, duration: 12443232
2015-12-10 23:15:31,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741934_1110, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:15:37,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741935_1111 src: /127.0.0.1:46686 dest: /127.0.0.1:50010
2015-12-10 23:15:37,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46686, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1000575728_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741935_1111, duration: 43058243
2015-12-10 23:15:37,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741935_1111, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:15:40,185 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741932_1108
2015-12-10 23:15:40,187 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741933_1109
2015-12-10 23:15:40,190 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741934_1110
2015-12-10 23:15:40,191 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741931_1107
2015-12-10 23:15:45,201 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741935_1111
2015-12-10 23:15:53,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741936_1112 src: /127.0.0.1:46704 dest: /127.0.0.1:50010
2015-12-10 23:16:04,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741937_1113 src: /127.0.0.1:46715 dest: /127.0.0.1:50010
2015-12-10 23:16:04,321 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46715, dest: /127.0.0.1:50010, bytes: 60, op: HDFS_WRITE, cliID: DFSClient_attempt_1449802555556_0007_r_000000_0_-1999329940_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741937_1113, duration: 120775202
2015-12-10 23:16:04,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741937_1113, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:16:05,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46704, dest: /127.0.0.1:50010, bytes: 49818, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1000575728_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741936_1112, duration: 12049382643
2015-12-10 23:16:05,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741936_1112, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:16:06,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741938_1114 src: /127.0.0.1:46718 dest: /127.0.0.1:50010
2015-12-10 23:16:06,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46718, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1000575728_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741938_1114, duration: 1779748
2015-12-10 23:16:06,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741938_1114, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:16:06,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741939_1115 src: /127.0.0.1:46720 dest: /127.0.0.1:50010
2015-12-10 23:16:06,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46720, dest: /127.0.0.1:50010, bytes: 49818, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1000575728_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741939_1115, duration: 6955918
2015-12-10 23:16:06,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741939_1115, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:16:06,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741940_1116 src: /127.0.0.1:46721 dest: /127.0.0.1:50010
2015-12-10 23:16:06,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46721, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1000575728_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741940_1116, duration: 3977268
2015-12-10 23:16:06,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741940_1116, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:16:10,291 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741937_1113
2015-12-10 23:16:10,708 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741936_1112 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741936 for deletion
2015-12-10 23:16:10,709 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741931_1107 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741931 for deletion
2015-12-10 23:16:10,710 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741932_1108 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741932 for deletion
2015-12-10 23:16:10,712 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741933_1109 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741933 for deletion
2015-12-10 23:16:10,713 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741934_1110 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741934 for deletion
2015-12-10 23:16:10,714 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741935_1111 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741935 for deletion
2015-12-10 23:16:10,714 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741936_1112 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741936
2015-12-10 23:16:10,714 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741931_1107 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741931
2015-12-10 23:16:10,714 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741932_1108 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741932
2015-12-10 23:16:10,714 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741933_1109 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741933
2015-12-10 23:16:10,714 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741934_1110 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741934
2015-12-10 23:16:10,715 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741935_1111 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741935
2015-12-10 23:16:15,302 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741939_1115
2015-12-10 23:16:15,305 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741940_1116
2015-12-10 23:16:15,306 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741938_1114
2015-12-10 23:22:28,771 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741937_1113 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741937 for deletion
2015-12-10 23:22:28,772 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741937_1113 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741937
2015-12-10 23:22:31,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741941_1117 src: /127.0.0.1:46732 dest: /127.0.0.1:50010
2015-12-10 23:22:31,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46732, dest: /127.0.0.1:50010, bytes: 4253, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1930240578_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741941_1117, duration: 62800054
2015-12-10 23:22:31,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741941_1117, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:22:31,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741942_1118 src: /127.0.0.1:46733 dest: /127.0.0.1:50010
2015-12-10 23:22:31,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46733, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1930240578_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741942_1118, duration: 2028610
2015-12-10 23:22:31,960 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741942_1118, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:22:31,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741943_1119 src: /127.0.0.1:46734 dest: /127.0.0.1:50010
2015-12-10 23:22:31,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46734, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1930240578_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741943_1119, duration: 1771553
2015-12-10 23:22:31,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741943_1119, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:22:32,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741944_1120 src: /127.0.0.1:46735 dest: /127.0.0.1:50010
2015-12-10 23:22:32,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46735, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1930240578_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741944_1120, duration: 9303381
2015-12-10 23:22:32,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741944_1120, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:22:38,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741945_1121 src: /127.0.0.1:46744 dest: /127.0.0.1:50010
2015-12-10 23:22:38,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46744, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1068966971_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741945_1121, duration: 45426048
2015-12-10 23:22:38,385 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741945_1121, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:22:40,366 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741942_1118
2015-12-10 23:22:40,368 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741941_1117
2015-12-10 23:22:40,369 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741943_1119
2015-12-10 23:22:40,371 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741944_1120
2015-12-10 23:22:45,391 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741945_1121
2015-12-10 23:22:55,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741946_1122 src: /127.0.0.1:46760 dest: /127.0.0.1:50010
2015-12-10 23:23:06,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741947_1123 src: /127.0.0.1:46773 dest: /127.0.0.1:50010
2015-12-10 23:23:06,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46773, dest: /127.0.0.1:50010, bytes: 60, op: HDFS_WRITE, cliID: DFSClient_attempt_1449802555556_0008_r_000000_0_674795283_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741947_1123, duration: 92298194
2015-12-10 23:23:06,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741947_1123, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:23:08,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46760, dest: /127.0.0.1:50010, bytes: 49841, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1068966971_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741946_1122, duration: 12871165729
2015-12-10 23:23:08,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741946_1122, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:23:08,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741948_1124 src: /127.0.0.1:46776 dest: /127.0.0.1:50010
2015-12-10 23:23:08,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46776, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1068966971_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741948_1124, duration: 2024672
2015-12-10 23:23:08,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741948_1124, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:23:08,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741949_1125 src: /127.0.0.1:46778 dest: /127.0.0.1:50010
2015-12-10 23:23:08,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46778, dest: /127.0.0.1:50010, bytes: 49841, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1068966971_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741949_1125, duration: 2686327
2015-12-10 23:23:08,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741949_1125, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:23:08,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741950_1126 src: /127.0.0.1:46779 dest: /127.0.0.1:50010
2015-12-10 23:23:08,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46779, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1068966971_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741950_1126, duration: 4907447
2015-12-10 23:23:08,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741950_1126, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:23:10,781 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741941_1117 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741941 for deletion
2015-12-10 23:23:10,781 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741942_1118 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741942 for deletion
2015-12-10 23:23:10,781 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741943_1119 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741943 for deletion
2015-12-10 23:23:10,781 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741944_1120 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741944 for deletion
2015-12-10 23:23:10,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741945_1121 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741945 for deletion
2015-12-10 23:23:10,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741946_1122 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741946 for deletion
2015-12-10 23:23:10,784 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741941_1117 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741941
2015-12-10 23:23:10,784 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741942_1118 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741942
2015-12-10 23:23:10,784 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741943_1119 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741943
2015-12-10 23:23:10,784 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741944_1120 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741944
2015-12-10 23:23:10,784 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741945_1121 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741945
2015-12-10 23:23:10,784 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741946_1122 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741946
2015-12-10 23:23:15,419 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741950_1126
2015-12-10 23:23:15,420 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741949_1125
2015-12-10 23:23:15,420 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741948_1124
2015-12-10 23:23:15,420 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741947_1123
2015-12-10 23:23:22,778 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741947_1123 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741947 for deletion
2015-12-10 23:23:22,778 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741947_1123 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741947
2015-12-10 23:25:58,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741951_1127 src: /127.0.0.1:46791 dest: /127.0.0.1:50010
2015-12-10 23:25:58,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46791, dest: /127.0.0.1:50010, bytes: 4155, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1747542676_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741951_1127, duration: 56938867
2015-12-10 23:25:58,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741951_1127, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:25:58,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741952_1128 src: /127.0.0.1:46792 dest: /127.0.0.1:50010
2015-12-10 23:25:58,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46792, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1747542676_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741952_1128, duration: 1980990
2015-12-10 23:25:58,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741952_1128, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:25:58,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741953_1129 src: /127.0.0.1:46793 dest: /127.0.0.1:50010
2015-12-10 23:25:58,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46793, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1747542676_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741953_1129, duration: 1930024
2015-12-10 23:25:58,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741953_1129, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:25:58,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741954_1130 src: /127.0.0.1:46794 dest: /127.0.0.1:50010
2015-12-10 23:25:58,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46794, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1747542676_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741954_1130, duration: 21966780
2015-12-10 23:25:58,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741954_1130, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:26:04,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741955_1131 src: /127.0.0.1:46803 dest: /127.0.0.1:50010
2015-12-10 23:26:04,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46803, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1068241642_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741955_1131, duration: 46890273
2015-12-10 23:26:04,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741955_1131, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:26:05,452 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741952_1128
2015-12-10 23:26:05,453 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741953_1129
2015-12-10 23:26:05,454 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741954_1130
2015-12-10 23:26:05,454 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741951_1127
2015-12-10 23:26:10,459 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741955_1131
2015-12-10 23:26:18,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741956_1132 src: /127.0.0.1:46818 dest: /127.0.0.1:50010
2015-12-10 23:26:27,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741957_1133 src: /127.0.0.1:46831 dest: /127.0.0.1:50010
2015-12-10 23:26:27,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46831, dest: /127.0.0.1:50010, bytes: 75, op: HDFS_WRITE, cliID: DFSClient_attempt_1449802555556_0009_r_000000_0_538013692_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741957_1133, duration: 78720552
2015-12-10 23:26:27,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741957_1133, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:26:29,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46818, dest: /127.0.0.1:50010, bytes: 48759, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1068241642_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741956_1132, duration: 11118143743
2015-12-10 23:26:29,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741956_1132, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:26:29,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741958_1134 src: /127.0.0.1:46834 dest: /127.0.0.1:50010
2015-12-10 23:26:29,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46834, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1068241642_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741958_1134, duration: 4659126
2015-12-10 23:26:29,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741958_1134, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:26:29,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741959_1135 src: /127.0.0.1:46836 dest: /127.0.0.1:50010
2015-12-10 23:26:29,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46836, dest: /127.0.0.1:50010, bytes: 48759, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1068241642_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741959_1135, duration: 3557423
2015-12-10 23:26:29,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741959_1135, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:26:29,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741960_1136 src: /127.0.0.1:46837 dest: /127.0.0.1:50010
2015-12-10 23:26:29,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46837, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1068241642_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741960_1136, duration: 3126362
2015-12-10 23:26:29,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741960_1136, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:26:31,819 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741952_1128 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741952 for deletion
2015-12-10 23:26:31,828 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741953_1129 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741953 for deletion
2015-12-10 23:26:31,829 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741952_1128 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741952
2015-12-10 23:26:31,829 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741954_1130 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741954 for deletion
2015-12-10 23:26:31,829 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741953_1129 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741953
2015-12-10 23:26:31,829 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741954_1130 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741954
2015-12-10 23:26:31,829 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741955_1131 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741955 for deletion
2015-12-10 23:26:31,829 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741956_1132 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741956 for deletion
2015-12-10 23:26:31,829 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741955_1131 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741955
2015-12-10 23:26:31,829 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741951_1127 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741951 for deletion
2015-12-10 23:26:31,829 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741956_1132 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741956
2015-12-10 23:26:31,832 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741951_1127 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741951
2015-12-10 23:26:35,478 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741957_1133
2015-12-10 23:26:35,479 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741959_1135
2015-12-10 23:26:35,481 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741960_1136
2015-12-10 23:26:35,483 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741958_1134
2015-12-10 23:30:01,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 43 blocks total. Took 0 msec to generate and 21 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@7f26f1e4
2015-12-10 23:30:01,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-10 23:33:19,879 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741957_1133 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741957 for deletion
2015-12-10 23:33:19,880 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741957_1133 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741957
2015-12-10 23:33:26,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741961_1137 src: /127.0.0.1:46851 dest: /127.0.0.1:50010
2015-12-10 23:33:26,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46851, dest: /127.0.0.1:50010, bytes: 4253, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1669111964_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741961_1137, duration: 54321039
2015-12-10 23:33:26,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741961_1137, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:33:26,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741962_1138 src: /127.0.0.1:46852 dest: /127.0.0.1:50010
2015-12-10 23:33:26,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46852, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1669111964_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741962_1138, duration: 2296300
2015-12-10 23:33:26,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741962_1138, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:33:26,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741963_1139 src: /127.0.0.1:46853 dest: /127.0.0.1:50010
2015-12-10 23:33:26,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46853, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1669111964_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741963_1139, duration: 1875144
2015-12-10 23:33:26,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741963_1139, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:33:27,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741964_1140 src: /127.0.0.1:46854 dest: /127.0.0.1:50010
2015-12-10 23:33:27,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46854, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1669111964_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741964_1140, duration: 26126144
2015-12-10 23:33:27,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741964_1140, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:33:32,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741965_1141 src: /127.0.0.1:46862 dest: /127.0.0.1:50010
2015-12-10 23:33:32,979 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46862, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1899464295_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741965_1141, duration: 38412026
2015-12-10 23:33:32,979 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741965_1141, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:33:35,532 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741964_1140
2015-12-10 23:33:35,533 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741962_1138
2015-12-10 23:33:35,533 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741961_1137
2015-12-10 23:33:35,534 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741963_1139
2015-12-10 23:33:40,539 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741965_1141
2015-12-10 23:33:47,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741966_1142 src: /127.0.0.1:46879 dest: /127.0.0.1:50010
2015-12-10 23:34:04,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741967_1143 src: /127.0.0.1:46898 dest: /127.0.0.1:50010
2015-12-10 23:34:04,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46898, dest: /127.0.0.1:50010, bytes: 60, op: HDFS_WRITE, cliID: DFSClient_attempt_1449802555556_0010_r_000000_0_361573347_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741967_1143, duration: 156127773
2015-12-10 23:34:04,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741967_1143, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:34:05,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46879, dest: /127.0.0.1:50010, bytes: 51352, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1899464295_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741966_1142, duration: 18501327564
2015-12-10 23:34:05,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741966_1142, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:34:05,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741968_1144 src: /127.0.0.1:46900 dest: /127.0.0.1:50010
2015-12-10 23:34:05,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46900, dest: /127.0.0.1:50010, bytes: 354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1899464295_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741968_1144, duration: 1794993
2015-12-10 23:34:05,861 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741968_1144, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:34:05,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741969_1145 src: /127.0.0.1:46902 dest: /127.0.0.1:50010
2015-12-10 23:34:05,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46902, dest: /127.0.0.1:50010, bytes: 51352, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1899464295_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741969_1145, duration: 9517535
2015-12-10 23:34:05,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741969_1145, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:34:05,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741970_1146 src: /127.0.0.1:46903 dest: /127.0.0.1:50010
2015-12-10 23:34:05,979 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46903, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1899464295_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741970_1146, duration: 6295189
2015-12-10 23:34:05,979 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741970_1146, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:34:07,970 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741961_1137 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741961 for deletion
2015-12-10 23:34:07,971 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741962_1138 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741962 for deletion
2015-12-10 23:34:07,971 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741963_1139 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741963 for deletion
2015-12-10 23:34:07,971 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741964_1140 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741964 for deletion
2015-12-10 23:34:07,972 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741965_1141 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741965 for deletion
2015-12-10 23:34:07,972 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741966_1142 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741966 for deletion
2015-12-10 23:34:07,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741961_1137 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741961
2015-12-10 23:34:07,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741962_1138 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741962
2015-12-10 23:34:07,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741963_1139 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741963
2015-12-10 23:34:07,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741964_1140 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741964
2015-12-10 23:34:07,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741965_1141 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741965
2015-12-10 23:34:07,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741966_1142 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741966
2015-12-10 23:34:10,566 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741967_1143
2015-12-10 23:34:15,579 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741969_1145
2015-12-10 23:34:15,580 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741968_1144
2015-12-10 23:34:15,582 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741970_1146
2015-12-10 23:40:19,953 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741967_1143 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741967 for deletion
2015-12-10 23:40:19,955 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741967_1143 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741967
2015-12-10 23:40:24,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741971_1147 src: /127.0.0.1:46916 dest: /127.0.0.1:50010
2015-12-10 23:40:24,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46916, dest: /127.0.0.1:50010, bytes: 4253, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1257412237_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741971_1147, duration: 42429250
2015-12-10 23:40:24,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741971_1147, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:40:24,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741972_1148 src: /127.0.0.1:46917 dest: /127.0.0.1:50010
2015-12-10 23:40:24,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46917, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1257412237_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741972_1148, duration: 2023075
2015-12-10 23:40:24,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741972_1148, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:40:24,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741973_1149 src: /127.0.0.1:46918 dest: /127.0.0.1:50010
2015-12-10 23:40:24,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46918, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1257412237_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741973_1149, duration: 1953050
2015-12-10 23:40:24,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741973_1149, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:40:24,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741974_1150 src: /127.0.0.1:46919 dest: /127.0.0.1:50010
2015-12-10 23:40:24,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46919, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1257412237_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741974_1150, duration: 15522739
2015-12-10 23:40:24,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741974_1150, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:40:29,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741975_1151 src: /127.0.0.1:46927 dest: /127.0.0.1:50010
2015-12-10 23:40:29,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46927, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_536831377_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741975_1151, duration: 33847124
2015-12-10 23:40:29,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741975_1151, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:40:30,630 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741972_1148
2015-12-10 23:40:30,631 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741974_1150
2015-12-10 23:40:30,631 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741971_1147
2015-12-10 23:40:30,631 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741973_1149
2015-12-10 23:40:35,641 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741975_1151
2015-12-10 23:40:46,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741976_1152 src: /127.0.0.1:46944 dest: /127.0.0.1:50010
2015-12-10 23:40:55,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741977_1153 src: /127.0.0.1:46957 dest: /127.0.0.1:50010
2015-12-10 23:40:55,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46957, dest: /127.0.0.1:50010, bytes: 60, op: HDFS_WRITE, cliID: DFSClient_attempt_1449802555556_0011_r_000000_0_1728892863_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741977_1153, duration: 73516933
2015-12-10 23:40:55,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741977_1153, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:40:56,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46944, dest: /127.0.0.1:50010, bytes: 49809, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_536831377_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741976_1152, duration: 10214924831
2015-12-10 23:40:56,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741976_1152, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:40:56,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741978_1154 src: /127.0.0.1:46960 dest: /127.0.0.1:50010
2015-12-10 23:40:56,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46960, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_536831377_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741978_1154, duration: 6787132
2015-12-10 23:40:56,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741978_1154, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:40:56,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741979_1155 src: /127.0.0.1:46962 dest: /127.0.0.1:50010
2015-12-10 23:40:56,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46962, dest: /127.0.0.1:50010, bytes: 49809, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_536831377_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741979_1155, duration: 8096462
2015-12-10 23:40:56,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741979_1155, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:40:56,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741980_1156 src: /127.0.0.1:46963 dest: /127.0.0.1:50010
2015-12-10 23:40:56,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46963, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_536831377_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741980_1156, duration: 12909045
2015-12-10 23:40:56,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741980_1156, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:40:58,971 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741971_1147 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741971 for deletion
2015-12-10 23:40:58,971 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741972_1148 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741972 for deletion
2015-12-10 23:40:58,972 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741973_1149 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741973 for deletion
2015-12-10 23:40:58,974 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741971_1147 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741971
2015-12-10 23:40:58,975 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741972_1148 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741972
2015-12-10 23:40:58,974 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741974_1150 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741974 for deletion
2015-12-10 23:40:58,975 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741973_1149 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741973
2015-12-10 23:40:58,975 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741975_1151 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741975 for deletion
2015-12-10 23:40:58,976 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741974_1150 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741974
2015-12-10 23:40:58,976 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741975_1151 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741975
2015-12-10 23:40:58,976 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741976_1152 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741976 for deletion
2015-12-10 23:40:58,976 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741976_1152 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741976
2015-12-10 23:41:05,732 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741980_1156
2015-12-10 23:41:05,733 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741977_1153
2015-12-10 23:41:05,775 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741979_1155
2015-12-10 23:41:05,776 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741978_1154
2015-12-10 23:44:22,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741977_1153 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741977 for deletion
2015-12-10 23:44:22,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741977_1153 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741977
2015-12-10 23:47:32,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741981_1157 src: /127.0.0.1:46993 dest: /127.0.0.1:50010
2015-12-10 23:47:32,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46993, dest: /127.0.0.1:50010, bytes: 4239, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105500336_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741981_1157, duration: 44784894
2015-12-10 23:47:32,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741981_1157, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:47:32,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741982_1158 src: /127.0.0.1:46994 dest: /127.0.0.1:50010
2015-12-10 23:47:32,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46994, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105500336_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741982_1158, duration: 6505746
2015-12-10 23:47:32,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741982_1158, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:47:32,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741983_1159 src: /127.0.0.1:46995 dest: /127.0.0.1:50010
2015-12-10 23:47:32,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46995, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105500336_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741983_1159, duration: 2157132
2015-12-10 23:47:32,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741983_1159, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:47:33,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741984_1160 src: /127.0.0.1:46996 dest: /127.0.0.1:50010
2015-12-10 23:47:33,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:46996, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2105500336_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741984_1160, duration: 23676868
2015-12-10 23:47:33,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741984_1160, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:47:40,830 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741982_1158
2015-12-10 23:47:40,834 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741981_1157
2015-12-10 23:47:40,835 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741983_1159
2015-12-10 23:47:40,836 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741984_1160
2015-12-10 23:47:41,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741985_1161 src: /127.0.0.1:47005 dest: /127.0.0.1:50010
2015-12-10 23:47:41,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47005, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_631685779_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741985_1161, duration: 65883292
2015-12-10 23:47:41,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741985_1161, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:47:50,882 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741985_1161
2015-12-10 23:47:57,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741986_1162 src: /127.0.0.1:47029 dest: /127.0.0.1:50010
2015-12-10 23:48:07,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741987_1163 src: /127.0.0.1:47041 dest: /127.0.0.1:50010
2015-12-10 23:48:08,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47041, dest: /127.0.0.1:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_attempt_1449802555556_0012_r_000000_0_-564690695_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741987_1163, duration: 87001149
2015-12-10 23:48:08,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741987_1163, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:48:09,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47029, dest: /127.0.0.1:50010, bytes: 49818, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_631685779_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741986_1162, duration: 12161105807
2015-12-10 23:48:09,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741986_1162, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:48:09,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741988_1164 src: /127.0.0.1:47044 dest: /127.0.0.1:50010
2015-12-10 23:48:09,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47044, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_631685779_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741988_1164, duration: 1955341
2015-12-10 23:48:09,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741988_1164, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:48:09,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741989_1165 src: /127.0.0.1:47046 dest: /127.0.0.1:50010
2015-12-10 23:48:09,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47046, dest: /127.0.0.1:50010, bytes: 49818, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_631685779_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741989_1165, duration: 3306639
2015-12-10 23:48:09,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741989_1165, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:48:09,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741990_1166 src: /127.0.0.1:47047 dest: /127.0.0.1:50010
2015-12-10 23:48:09,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47047, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_631685779_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741990_1166, duration: 2800292
2015-12-10 23:48:09,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741990_1166, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:48:14,028 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741984_1160 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741984 for deletion
2015-12-10 23:48:14,030 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741985_1161 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741985 for deletion
2015-12-10 23:48:14,030 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741986_1162 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741986 for deletion
2015-12-10 23:48:14,030 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741981_1157 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741981 for deletion
2015-12-10 23:48:14,030 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741982_1158 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741982 for deletion
2015-12-10 23:48:14,030 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741983_1159 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741983 for deletion
2015-12-10 23:48:14,031 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741984_1160 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741984
2015-12-10 23:48:14,031 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741985_1161 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741985
2015-12-10 23:48:14,031 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741986_1162 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741986
2015-12-10 23:48:14,031 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741981_1157 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741981
2015-12-10 23:48:14,031 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741982_1158 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741982
2015-12-10 23:48:14,031 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741983_1159 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741983
2015-12-10 23:48:16,023 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741990_1166
2015-12-10 23:48:16,024 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741989_1165
2015-12-10 23:48:16,024 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741987_1163
2015-12-10 23:48:16,025 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741988_1164
2015-12-10 23:49:26,035 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741987_1163 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741987 for deletion
2015-12-10 23:49:26,036 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741987_1163 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741987
2015-12-10 23:50:45,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741991_1167 src: /127.0.0.1:47057 dest: /127.0.0.1:50010
2015-12-10 23:50:45,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47057, dest: /127.0.0.1:50010, bytes: 4239, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_384772119_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741991_1167, duration: 44951334
2015-12-10 23:50:45,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741991_1167, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:50:45,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741992_1168 src: /127.0.0.1:47058 dest: /127.0.0.1:50010
2015-12-10 23:50:45,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47058, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_384772119_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741992_1168, duration: 1807553
2015-12-10 23:50:45,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741992_1168, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:50:45,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741993_1169 src: /127.0.0.1:47059 dest: /127.0.0.1:50010
2015-12-10 23:50:45,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47059, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_384772119_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741993_1169, duration: 1626206
2015-12-10 23:50:45,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741993_1169, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:50:46,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741994_1170 src: /127.0.0.1:47060 dest: /127.0.0.1:50010
2015-12-10 23:50:46,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47060, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_384772119_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741994_1170, duration: 9511561
2015-12-10 23:50:46,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741994_1170, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:50:51,069 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741992_1168
2015-12-10 23:50:51,069 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741991_1167
2015-12-10 23:50:51,070 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741993_1169
2015-12-10 23:50:52,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741995_1171 src: /127.0.0.1:47070 dest: /127.0.0.1:50010
2015-12-10 23:50:52,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47070, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988252152_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741995_1171, duration: 51459630
2015-12-10 23:50:52,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741995_1171, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:50:56,077 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741994_1170
2015-12-10 23:51:01,098 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741995_1171
2015-12-10 23:51:06,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741996_1172 src: /127.0.0.1:47085 dest: /127.0.0.1:50010
2015-12-10 23:51:15,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741997_1173 src: /127.0.0.1:47099 dest: /127.0.0.1:50010
2015-12-10 23:51:16,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47099, dest: /127.0.0.1:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_attempt_1449802555556_0013_r_000000_0_1067719597_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741997_1173, duration: 140488589
2015-12-10 23:51:16,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741997_1173, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:51:17,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47085, dest: /127.0.0.1:50010, bytes: 49806, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988252152_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741996_1172, duration: 11398708431
2015-12-10 23:51:17,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741996_1172, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:51:17,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741998_1174 src: /127.0.0.1:47102 dest: /127.0.0.1:50010
2015-12-10 23:51:17,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47102, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988252152_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741998_1174, duration: 1885535
2015-12-10 23:51:17,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741998_1174, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:51:17,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073741999_1175 src: /127.0.0.1:47104 dest: /127.0.0.1:50010
2015-12-10 23:51:17,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47104, dest: /127.0.0.1:50010, bytes: 49806, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988252152_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073741999_1175, duration: 4285870
2015-12-10 23:51:17,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073741999_1175, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:51:17,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742000_1176 src: /127.0.0.1:47105 dest: /127.0.0.1:50010
2015-12-10 23:51:17,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47105, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988252152_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742000_1176, duration: 7009783
2015-12-10 23:51:17,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742000_1176, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-10 23:51:20,051 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741991_1167 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741991 for deletion
2015-12-10 23:51:20,053 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741992_1168 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741992 for deletion
2015-12-10 23:51:20,053 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741993_1169 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741993 for deletion
2015-12-10 23:51:20,053 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741991_1167 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741991
2015-12-10 23:51:20,053 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741994_1170 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741994 for deletion
2015-12-10 23:51:20,054 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741995_1171 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741995 for deletion
2015-12-10 23:51:20,054 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741996_1172 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741996 for deletion
2015-12-10 23:51:20,054 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741992_1168 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741992
2015-12-10 23:51:20,054 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741993_1169 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741993
2015-12-10 23:51:20,055 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741994_1170 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741994
2015-12-10 23:51:20,055 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741995_1171 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741995
2015-12-10 23:51:20,055 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741996_1172 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741996
2015-12-10 23:51:26,130 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741997_1173
2015-12-10 23:51:26,132 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741999_1175
2015-12-10 23:51:26,133 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073741998_1174
2015-12-10 23:51:26,135 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742000_1176
2015-12-10 23:51:32,050 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741997_1173 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741997 for deletion
2015-12-10 23:51:32,051 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073741997_1173 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073741997
2015-12-11 00:00:53,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742001_1177 src: /127.0.0.1:47123 dest: /127.0.0.1:50010
2015-12-11 00:00:53,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47123, dest: /127.0.0.1:50010, bytes: 4239, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1136204055_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742001_1177, duration: 37354622
2015-12-11 00:00:53,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742001_1177, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 00:00:53,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742002_1178 src: /127.0.0.1:47124 dest: /127.0.0.1:50010
2015-12-11 00:00:53,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47124, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1136204055_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742002_1178, duration: 3138064
2015-12-11 00:00:53,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742002_1178, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 00:00:53,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742003_1179 src: /127.0.0.1:47125 dest: /127.0.0.1:50010
2015-12-11 00:00:53,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47125, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1136204055_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742003_1179, duration: 1921167
2015-12-11 00:00:53,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742003_1179, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 00:00:53,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742004_1180 src: /127.0.0.1:47126 dest: /127.0.0.1:50010
2015-12-11 00:00:53,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47126, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1136204055_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742004_1180, duration: 9221820
2015-12-11 00:00:53,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742004_1180, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 00:00:59,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742005_1181 src: /127.0.0.1:47135 dest: /127.0.0.1:50010
2015-12-11 00:01:00,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47135, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_785148752_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742005_1181, duration: 43908859
2015-12-11 00:01:00,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742005_1181, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 00:01:01,221 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742001_1177
2015-12-11 00:01:01,221 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742002_1178
2015-12-11 00:01:01,222 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742004_1180
2015-12-11 00:01:01,223 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742003_1179
2015-12-11 00:01:06,248 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742005_1181
2015-12-11 00:01:17,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742006_1182 src: /127.0.0.1:47152 dest: /127.0.0.1:50010
2015-12-11 00:01:30,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742007_1183 src: /127.0.0.1:47164 dest: /127.0.0.1:50010
2015-12-11 00:01:30,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47164, dest: /127.0.0.1:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_attempt_1449802555556_0014_r_000000_0_41215363_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742007_1183, duration: 175983006
2015-12-11 00:01:30,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742007_1183, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 00:01:31,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47152, dest: /127.0.0.1:50010, bytes: 49846, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_785148752_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742006_1182, duration: 13290016277
2015-12-11 00:01:31,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742006_1182, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 00:01:31,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742008_1184 src: /127.0.0.1:47167 dest: /127.0.0.1:50010
2015-12-11 00:01:31,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47167, dest: /127.0.0.1:50010, bytes: 354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_785148752_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742008_1184, duration: 1879756
2015-12-11 00:01:31,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742008_1184, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 00:01:31,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742009_1185 src: /127.0.0.1:47169 dest: /127.0.0.1:50010
2015-12-11 00:01:31,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47169, dest: /127.0.0.1:50010, bytes: 49846, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_785148752_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742009_1185, duration: 2852575
2015-12-11 00:01:31,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742009_1185, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 00:01:31,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742010_1186 src: /127.0.0.1:47170 dest: /127.0.0.1:50010
2015-12-11 00:01:31,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47170, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_785148752_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742010_1186, duration: 2651971
2015-12-11 00:01:31,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742010_1186, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 00:01:35,173 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742001_1177 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742001 for deletion
2015-12-11 00:01:35,175 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742002_1178 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742002 for deletion
2015-12-11 00:01:35,176 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742003_1179 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742003 for deletion
2015-12-11 00:01:35,176 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742004_1180 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742004 for deletion
2015-12-11 00:01:35,176 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742005_1181 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742005 for deletion
2015-12-11 00:01:35,176 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742006_1182 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742006 for deletion
2015-12-11 00:01:35,178 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742001_1177 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742001
2015-12-11 00:01:35,178 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742002_1178 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742002
2015-12-11 00:01:35,179 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742003_1179 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742003
2015-12-11 00:01:35,179 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742004_1180 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742004
2015-12-11 00:01:35,179 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742005_1181 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742005
2015-12-11 00:01:35,179 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742006_1182 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742006
2015-12-11 00:01:36,344 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742009_1185
2015-12-11 00:01:36,347 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742008_1184
2015-12-11 00:01:36,357 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742010_1186
2015-12-11 00:01:36,363 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742007_1183
2015-12-11 00:04:38,253 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "ubuntu/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-12-11 00:04:42,071 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-11 00:04:42,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-11 10:06:40,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-11 10:06:40,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-11 10:06:41,635 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-11 10:06:41,687 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-11 10:06:41,687 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-11 10:06:41,690 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-11 10:06:41,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-11 10:06:41,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-11 10:06:41,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-11 10:06:41,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-11 10:06:41,823 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-11 10:06:41,826 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-11 10:06:41,835 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-11 10:06:41,837 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-11 10:06:41,837 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-11 10:06:41,837 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-11 10:06:41,854 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-11 10:06:41,858 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-11 10:06:41,858 INFO org.mortbay.log: jetty-6.1.26
2015-12-11 10:06:42,109 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-11 10:06:42,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-11 10:06:42,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-11 10:06:42,308 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-11 10:06:42,327 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-11 10:06:42,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-11 10:06:42,365 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-11 10:06:42,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-11 10:06:42,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-11 10:06:42,403 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-11 10:06:42,404 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-11 10:06:42,706 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-11 10:06:42,711 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 32572@ubuntu
2015-12-11 10:06:42,779 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1841603009-127.0.1.1-1449456040486
2015-12-11 10:06:42,779 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-12-11 10:06:42,780 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-11 10:06:42,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1379726899;bpid=BP-1841603009-127.0.1.1-1449456040486;lv=-56;nsInfo=lv=-60;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0;bpid=BP-1841603009-127.0.1.1-1449456040486;dnuuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-11 10:06:42,829 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /home/parallels/hadoop/tmp/dfs/data/current
2015-12-11 10:06:42,839 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/parallels/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-11 10:06:42,889 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-11 10:06:42,892 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1449848478892 with interval 21600000
2015-12-11 10:06:42,893 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-11 10:06:42,894 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-11 10:06:42,954 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1841603009-127.0.1.1-1449456040486 on /home/parallels/hadoop/tmp/dfs/data/current: 60ms
2015-12-11 10:06:42,954 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1841603009-127.0.1.1-1449456040486: 61ms
2015-12-11 10:06:42,955 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-11 10:06:42,966 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current: 11ms
2015-12-11 10:06:42,967 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 12ms
2015-12-11 10:06:42,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-11 10:06:43,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-11 10:06:43,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-11 10:06:43,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1507
2015-12-11 10:06:43,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000
2015-12-11 10:06:43,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 58 blocks total. Took 2 msec to generate and 52 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@4c168efc
2015-12-11 10:06:43,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-11 10:06:43,215 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-12-11 10:06:43,215 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-11 10:06:43,216 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-12-11 10:06:43,216 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-12-11 10:06:43,219 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-11 10:06:43,224 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1841603009-127.0.1.1-1449456040486 to blockPoolScannerMap, new size=1
2015-12-11 10:07:16,037 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742007_1183 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742007 for deletion
2015-12-11 10:07:16,047 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742007_1183 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742007
2015-12-11 10:07:26,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742011_1187 src: /127.0.0.1:47202 dest: /127.0.0.1:50010
2015-12-11 10:07:26,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47202, dest: /127.0.0.1:50010, bytes: 4301, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-349724858_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742011_1187, duration: 133182631
2015-12-11 10:07:26,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742011_1187, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:07:27,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742012_1188 src: /127.0.0.1:47203 dest: /127.0.0.1:50010
2015-12-11 10:07:27,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47203, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-349724858_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742012_1188, duration: 6815738
2015-12-11 10:07:27,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742012_1188, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:07:27,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742013_1189 src: /127.0.0.1:47204 dest: /127.0.0.1:50010
2015-12-11 10:07:27,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47204, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-349724858_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742013_1189, duration: 5785801
2015-12-11 10:07:27,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742013_1189, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:07:27,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742014_1190 src: /127.0.0.1:47205 dest: /127.0.0.1:50010
2015-12-11 10:07:27,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47205, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-349724858_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742014_1190, duration: 13431860
2015-12-11 10:07:27,329 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742014_1190, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:07:33,051 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742013_1189
2015-12-11 10:07:33,056 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742014_1190
2015-12-11 10:07:33,057 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742012_1188
2015-12-11 10:07:33,059 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742011_1187
2015-12-11 10:07:37,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742015_1191 src: /127.0.0.1:47213 dest: /127.0.0.1:50010
2015-12-11 10:07:37,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47213, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1043095702_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742015_1191, duration: 38079744
2015-12-11 10:07:37,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742015_1191, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:07:43,215 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742015_1191
2015-12-11 10:07:55,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742016_1192 src: /127.0.0.1:47234 dest: /127.0.0.1:50010
2015-12-11 10:08:06,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742017_1193 src: /127.0.0.1:47246 dest: /127.0.0.1:50010
2015-12-11 10:08:06,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47246, dest: /127.0.0.1:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_attempt_1449846414359_0001_r_000000_0_-1584321040_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742017_1193, duration: 87771126
2015-12-11 10:08:06,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742017_1193, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:08:07,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47234, dest: /127.0.0.1:50010, bytes: 49843, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1043095702_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742016_1192, duration: 12200355820
2015-12-11 10:08:07,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742016_1192, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:08:07,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742018_1194 src: /127.0.0.1:47249 dest: /127.0.0.1:50010
2015-12-11 10:08:07,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47249, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1043095702_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742018_1194, duration: 2988123
2015-12-11 10:08:07,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742018_1194, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:08:07,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742019_1195 src: /127.0.0.1:47251 dest: /127.0.0.1:50010
2015-12-11 10:08:07,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47251, dest: /127.0.0.1:50010, bytes: 49843, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1043095702_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742019_1195, duration: 2568351
2015-12-11 10:08:07,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742019_1195, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:08:07,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742020_1196 src: /127.0.0.1:47252 dest: /127.0.0.1:50010
2015-12-11 10:08:07,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47252, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1043095702_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742020_1196, duration: 2855443
2015-12-11 10:08:07,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742020_1196, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:08:10,029 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742016_1192 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742016 for deletion
2015-12-11 10:08:10,031 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742011_1187 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742011 for deletion
2015-12-11 10:08:10,032 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742012_1188 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742012 for deletion
2015-12-11 10:08:10,032 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742013_1189 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742013 for deletion
2015-12-11 10:08:10,032 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742016_1192 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742016
2015-12-11 10:08:10,032 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742011_1187 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742011
2015-12-11 10:08:10,033 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742014_1190 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742014 for deletion
2015-12-11 10:08:10,033 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742015_1191 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742015 for deletion
2015-12-11 10:08:10,033 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742012_1188 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742012
2015-12-11 10:08:10,033 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742013_1189 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742013
2015-12-11 10:08:10,034 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742014_1190 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742014
2015-12-11 10:08:10,035 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742015_1191 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742015
2015-12-11 10:08:13,265 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742019_1195
2015-12-11 10:08:13,266 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742018_1194
2015-12-11 10:08:13,267 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742017_1193
2015-12-11 10:08:13,269 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742020_1196
2015-12-11 10:08:22,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742017_1193 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742017 for deletion
2015-12-11 10:08:22,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742017_1193 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742017
2015-12-11 10:16:30,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742021_1197 src: /127.0.0.1:47270 dest: /127.0.0.1:50010
2015-12-11 10:16:30,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47270, dest: /127.0.0.1:50010, bytes: 4293, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_451392798_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742021_1197, duration: 62301940
2015-12-11 10:16:30,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742021_1197, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:16:30,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742022_1198 src: /127.0.0.1:47271 dest: /127.0.0.1:50010
2015-12-11 10:16:30,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47271, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_451392798_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742022_1198, duration: 2069217
2015-12-11 10:16:30,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742022_1198, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:16:30,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742023_1199 src: /127.0.0.1:47272 dest: /127.0.0.1:50010
2015-12-11 10:16:30,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47272, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_451392798_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742023_1199, duration: 2632064
2015-12-11 10:16:30,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742023_1199, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:16:30,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742024_1200 src: /127.0.0.1:47273 dest: /127.0.0.1:50010
2015-12-11 10:16:30,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47273, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_451392798_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742024_1200, duration: 12235068
2015-12-11 10:16:30,506 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742024_1200, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:16:37,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742025_1201 src: /127.0.0.1:47281 dest: /127.0.0.1:50010
2015-12-11 10:16:37,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47281, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-645430726_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742025_1201, duration: 33829845
2015-12-11 10:16:37,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742025_1201, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:16:38,371 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742022_1198
2015-12-11 10:16:38,373 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742024_1200
2015-12-11 10:16:38,375 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742023_1199
2015-12-11 10:16:38,376 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742021_1197
2015-12-11 10:16:43,468 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742025_1201
2015-12-11 10:16:53,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742026_1202 src: /127.0.0.1:47300 dest: /127.0.0.1:50010
2015-12-11 10:17:04,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742027_1203 src: /127.0.0.1:47313 dest: /127.0.0.1:50010
2015-12-11 10:17:04,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47313, dest: /127.0.0.1:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_attempt_1449846414359_0002_r_000000_0_145808178_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742027_1203, duration: 118704477
2015-12-11 10:17:04,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742027_1203, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:17:05,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47300, dest: /127.0.0.1:50010, bytes: 49585, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-645430726_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742026_1202, duration: 11382219399
2015-12-11 10:17:05,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742026_1202, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:17:05,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742028_1204 src: /127.0.0.1:47316 dest: /127.0.0.1:50010
2015-12-11 10:17:05,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47316, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-645430726_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742028_1204, duration: 954343
2015-12-11 10:17:05,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742028_1204, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:17:05,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742029_1205 src: /127.0.0.1:47318 dest: /127.0.0.1:50010
2015-12-11 10:17:05,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47318, dest: /127.0.0.1:50010, bytes: 49585, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-645430726_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742029_1205, duration: 3267387
2015-12-11 10:17:05,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742029_1205, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:17:05,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742030_1206 src: /127.0.0.1:47319 dest: /127.0.0.1:50010
2015-12-11 10:17:05,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47319, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-645430726_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742030_1206, duration: 5535842
2015-12-11 10:17:05,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742030_1206, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:17:07,109 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742021_1197 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742021 for deletion
2015-12-11 10:17:07,110 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742022_1198 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742022 for deletion
2015-12-11 10:17:07,112 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742023_1199 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742023 for deletion
2015-12-11 10:17:07,112 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742021_1197 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742021
2015-12-11 10:17:07,112 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742024_1200 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742024 for deletion
2015-12-11 10:17:07,112 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742022_1198 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742022
2015-12-11 10:17:07,112 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742025_1201 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742025 for deletion
2015-12-11 10:17:07,112 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742026_1202 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742026 for deletion
2015-12-11 10:17:07,113 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742023_1199 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742023
2015-12-11 10:17:07,113 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742024_1200 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742024
2015-12-11 10:17:07,113 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742025_1201 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742025
2015-12-11 10:17:07,114 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742026_1202 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742026
2015-12-11 10:17:13,516 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742029_1205
2015-12-11 10:17:13,518 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742028_1204
2015-12-11 10:17:13,521 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742027_1203
2015-12-11 10:17:13,524 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742030_1206
2015-12-11 10:21:49,142 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742027_1203 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742027 for deletion
2015-12-11 10:21:49,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742027_1203 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742027
2015-12-11 10:21:52,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742031_1207 src: /127.0.0.1:47330 dest: /127.0.0.1:50010
2015-12-11 10:21:52,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47330, dest: /127.0.0.1:50010, bytes: 4293, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1989202495_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742031_1207, duration: 51088959
2015-12-11 10:21:52,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742031_1207, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:21:52,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742032_1208 src: /127.0.0.1:47331 dest: /127.0.0.1:50010
2015-12-11 10:21:52,993 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47331, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1989202495_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742032_1208, duration: 2549265
2015-12-11 10:21:52,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742032_1208, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:21:53,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742033_1209 src: /127.0.0.1:47332 dest: /127.0.0.1:50010
2015-12-11 10:21:53,017 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47332, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1989202495_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742033_1209, duration: 1929905
2015-12-11 10:21:53,017 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742033_1209, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:21:53,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742034_1210 src: /127.0.0.1:47333 dest: /127.0.0.1:50010
2015-12-11 10:21:53,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47333, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1989202495_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742034_1210, duration: 7918973
2015-12-11 10:21:53,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742034_1210, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:21:58,561 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742033_1209
2015-12-11 10:21:58,563 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742034_1210
2015-12-11 10:21:58,564 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742031_1207
2015-12-11 10:21:58,565 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742032_1208
2015-12-11 10:21:59,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742035_1211 src: /127.0.0.1:47343 dest: /127.0.0.1:50010
2015-12-11 10:21:59,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47343, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1868850257_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742035_1211, duration: 27458901
2015-12-11 10:21:59,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742035_1211, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:22:08,591 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742035_1211
2015-12-11 10:22:15,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742036_1212 src: /127.0.0.1:47359 dest: /127.0.0.1:50010
2015-12-11 10:22:25,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742037_1213 src: /127.0.0.1:47374 dest: /127.0.0.1:50010
2015-12-11 10:22:25,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47374, dest: /127.0.0.1:50010, bytes: 23, op: HDFS_WRITE, cliID: DFSClient_attempt_1449846414359_0003_r_000000_0_-1257080540_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742037_1213, duration: 39442955
2015-12-11 10:22:25,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742037_1213, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:22:25,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47359, dest: /127.0.0.1:50010, bytes: 49590, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1868850257_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742036_1212, duration: 10329183368
2015-12-11 10:22:25,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742036_1212, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:22:25,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742038_1214 src: /127.0.0.1:47377 dest: /127.0.0.1:50010
2015-12-11 10:22:25,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47377, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1868850257_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742038_1214, duration: 1854435
2015-12-11 10:22:25,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742038_1214, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:22:25,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742039_1215 src: /127.0.0.1:47379 dest: /127.0.0.1:50010
2015-12-11 10:22:25,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47379, dest: /127.0.0.1:50010, bytes: 49590, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1868850257_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742039_1215, duration: 5927924
2015-12-11 10:22:25,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742039_1215, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:22:25,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742040_1216 src: /127.0.0.1:47380 dest: /127.0.0.1:50010
2015-12-11 10:22:25,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47380, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1868850257_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742040_1216, duration: 3549907
2015-12-11 10:22:25,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742040_1216, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:22:31,404 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742032_1208 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742032 for deletion
2015-12-11 10:22:31,404 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742033_1209 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742033 for deletion
2015-12-11 10:22:31,404 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742034_1210 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742034 for deletion
2015-12-11 10:22:31,404 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742035_1211 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742035 for deletion
2015-12-11 10:22:31,404 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742036_1212 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742036 for deletion
2015-12-11 10:22:31,405 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742031_1207 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742031 for deletion
2015-12-11 10:22:31,407 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742032_1208 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742032
2015-12-11 10:22:31,407 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742033_1209 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742033
2015-12-11 10:22:31,408 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742034_1210 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742034
2015-12-11 10:22:31,408 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742035_1211 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742035
2015-12-11 10:22:31,408 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742036_1212 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742036
2015-12-11 10:22:31,408 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742031_1207 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742031
2015-12-11 10:22:33,674 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742040_1216
2015-12-11 10:22:33,675 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742037_1213
2015-12-11 10:22:33,676 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742039_1215
2015-12-11 10:22:33,676 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742038_1214
2015-12-11 10:24:46,409 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742037_1213 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742037 for deletion
2015-12-11 10:24:46,410 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742037_1213 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742037
2015-12-11 10:24:47,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742041_1217 src: /127.0.0.1:47391 dest: /127.0.0.1:50010
2015-12-11 10:24:47,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47391, dest: /127.0.0.1:50010, bytes: 4323, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_712849395_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742041_1217, duration: 44110051
2015-12-11 10:24:47,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742041_1217, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:24:47,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742042_1218 src: /127.0.0.1:47392 dest: /127.0.0.1:50010
2015-12-11 10:24:47,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47392, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_712849395_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742042_1218, duration: 1820190
2015-12-11 10:24:47,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742042_1218, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:24:47,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742043_1219 src: /127.0.0.1:47393 dest: /127.0.0.1:50010
2015-12-11 10:24:47,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47393, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_712849395_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742043_1219, duration: 2563047
2015-12-11 10:24:47,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742043_1219, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:24:47,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742044_1220 src: /127.0.0.1:47394 dest: /127.0.0.1:50010
2015-12-11 10:24:47,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47394, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_712849395_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742044_1220, duration: 27992021
2015-12-11 10:24:47,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742044_1220, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:24:52,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742045_1221 src: /127.0.0.1:47402 dest: /127.0.0.1:50010
2015-12-11 10:24:53,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47402, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_640598033_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742045_1221, duration: 61117144
2015-12-11 10:24:53,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742045_1221, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:24:53,696 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742041_1217
2015-12-11 10:24:53,697 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742043_1219
2015-12-11 10:24:53,697 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742042_1218
2015-12-11 10:24:53,698 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742044_1220
2015-12-11 10:24:58,744 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742045_1221
2015-12-11 10:25:10,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742046_1222 src: /127.0.0.1:47421 dest: /127.0.0.1:50010
2015-12-11 10:25:20,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742047_1223 src: /127.0.0.1:47433 dest: /127.0.0.1:50010
2015-12-11 10:25:20,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47433, dest: /127.0.0.1:50010, bytes: 43, op: HDFS_WRITE, cliID: DFSClient_attempt_1449846414359_0004_r_000000_0_1434525805_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742047_1223, duration: 102682530
2015-12-11 10:25:20,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742047_1223, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:25:21,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47421, dest: /127.0.0.1:50010, bytes: 49640, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_640598033_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742046_1222, duration: 11615223872
2015-12-11 10:25:21,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742046_1222, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:25:21,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742048_1224 src: /127.0.0.1:47436 dest: /127.0.0.1:50010
2015-12-11 10:25:21,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47436, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_640598033_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742048_1224, duration: 1860426
2015-12-11 10:25:21,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742048_1224, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:25:21,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742049_1225 src: /127.0.0.1:47438 dest: /127.0.0.1:50010
2015-12-11 10:25:21,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47438, dest: /127.0.0.1:50010, bytes: 49640, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_640598033_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742049_1225, duration: 3236755
2015-12-11 10:25:21,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742049_1225, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:25:21,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742050_1226 src: /127.0.0.1:47439 dest: /127.0.0.1:50010
2015-12-11 10:25:21,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47439, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_640598033_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742050_1226, duration: 4406061
2015-12-11 10:25:21,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742050_1226, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:25:28,429 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742041_1217 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742041 for deletion
2015-12-11 10:25:28,430 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742042_1218 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742042 for deletion
2015-12-11 10:25:28,430 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742043_1219 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742043 for deletion
2015-12-11 10:25:28,430 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742044_1220 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742044 for deletion
2015-12-11 10:25:28,430 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742045_1221 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742045 for deletion
2015-12-11 10:25:28,430 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742046_1222 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742046 for deletion
2015-12-11 10:25:28,431 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742041_1217 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742041
2015-12-11 10:25:28,431 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742042_1218 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742042
2015-12-11 10:25:28,431 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742043_1219 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742043
2015-12-11 10:25:28,431 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742044_1220 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742044
2015-12-11 10:25:28,431 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742045_1221 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742045
2015-12-11 10:25:28,431 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742046_1222 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742046
2015-12-11 10:25:28,760 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742050_1226
2015-12-11 10:25:28,761 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742047_1223
2015-12-11 10:25:28,761 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742048_1224
2015-12-11 10:25:28,762 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742049_1225
2015-12-11 10:40:37,578 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742047_1223 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742047 for deletion
2015-12-11 10:40:37,579 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742047_1223 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742047
2015-12-11 10:41:18,974 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1841603009-127.0.1.1-1449456040486 Total blocks: 69, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-12-11 10:41:43,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742051_1227 src: /127.0.0.1:47467 dest: /127.0.0.1:50010
2015-12-11 10:41:43,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47467, dest: /127.0.0.1:50010, bytes: 4087, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-483856251_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742051_1227, duration: 64564674
2015-12-11 10:41:43,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742051_1227, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:41:43,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742052_1228 src: /127.0.0.1:47468 dest: /127.0.0.1:50010
2015-12-11 10:41:43,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47468, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-483856251_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742052_1228, duration: 2587030
2015-12-11 10:41:43,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742052_1228, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:41:43,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742053_1229 src: /127.0.0.1:47469 dest: /127.0.0.1:50010
2015-12-11 10:41:43,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47469, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-483856251_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742053_1229, duration: 2108193
2015-12-11 10:41:43,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742053_1229, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:41:44,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742054_1230 src: /127.0.0.1:47470 dest: /127.0.0.1:50010
2015-12-11 10:41:44,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47470, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-483856251_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742054_1230, duration: 14456583
2015-12-11 10:41:44,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742054_1230, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:41:48,879 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742051_1227
2015-12-11 10:41:50,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742055_1231 src: /127.0.0.1:47479 dest: /127.0.0.1:50010
2015-12-11 10:41:50,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47479, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2066648004_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742055_1231, duration: 33740624
2015-12-11 10:41:50,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742055_1231, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:41:53,895 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742052_1228
2015-12-11 10:41:53,902 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742054_1230
2015-12-11 10:41:53,904 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742053_1229
2015-12-11 10:41:58,960 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742055_1231
2015-12-11 10:42:04,447 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1560ms
No GCs detected
2015-12-11 10:42:07,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742056_1232 src: /127.0.0.1:47499 dest: /127.0.0.1:50010
2015-12-11 10:42:17,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742057_1233 src: /127.0.0.1:47510 dest: /127.0.0.1:50010
2015-12-11 10:42:17,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47510, dest: /127.0.0.1:50010, bytes: 92, op: HDFS_WRITE, cliID: DFSClient_attempt_1449846414359_0006_r_000000_0_-2002347730_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742057_1233, duration: 79549979
2015-12-11 10:42:17,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742057_1233, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:42:19,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47499, dest: /127.0.0.1:50010, bytes: 48174, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2066648004_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742056_1232, duration: 11967242740
2015-12-11 10:42:19,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742056_1232, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:42:19,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742058_1234 src: /127.0.0.1:47513 dest: /127.0.0.1:50010
2015-12-11 10:42:19,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47513, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2066648004_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742058_1234, duration: 4442856
2015-12-11 10:42:19,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742058_1234, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:42:19,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742059_1235 src: /127.0.0.1:47515 dest: /127.0.0.1:50010
2015-12-11 10:42:19,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47515, dest: /127.0.0.1:50010, bytes: 48174, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2066648004_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742059_1235, duration: 5557260
2015-12-11 10:42:19,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742059_1235, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:42:19,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742060_1236 src: /127.0.0.1:47516 dest: /127.0.0.1:50010
2015-12-11 10:42:19,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47516, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2066648004_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742060_1236, duration: 12496322
2015-12-11 10:42:19,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742060_1236, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 10:42:24,259 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742056_1232
2015-12-11 10:42:24,260 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742057_1233
2015-12-11 10:42:25,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742051_1227 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742051 for deletion
2015-12-11 10:42:25,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742052_1228 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742052 for deletion
2015-12-11 10:42:25,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742053_1229 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742053 for deletion
2015-12-11 10:42:25,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742054_1230 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742054 for deletion
2015-12-11 10:42:25,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742055_1231 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742055 for deletion
2015-12-11 10:42:25,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742056_1232 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742056 for deletion
2015-12-11 10:42:25,599 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742051_1227 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742051
2015-12-11 10:42:25,599 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742052_1228 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742052
2015-12-11 10:42:25,599 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742053_1229 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742053
2015-12-11 10:42:25,599 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742054_1230 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742054
2015-12-11 10:42:25,599 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742055_1231 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742055
2015-12-11 10:42:25,599 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742056_1232 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742056
2015-12-11 10:42:29,266 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742058_1234
2015-12-11 10:42:29,267 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742059_1235
2015-12-11 10:42:29,268 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742060_1236
2015-12-11 10:42:31,585 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742057_1233 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742057 for deletion
2015-12-11 10:42:31,586 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742057_1233 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742057
2015-12-11 11:22:31,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742061_1237 src: /127.0.0.1:47576 dest: /127.0.0.1:50010
2015-12-11 11:22:31,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47576, dest: /127.0.0.1:50010, bytes: 4088, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1152410462_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742061_1237, duration: 47912699
2015-12-11 11:22:31,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742061_1237, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:22:31,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742062_1238 src: /127.0.0.1:47577 dest: /127.0.0.1:50010
2015-12-11 11:22:31,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47577, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1152410462_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742062_1238, duration: 1907296
2015-12-11 11:22:31,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742062_1238, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:22:31,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742063_1239 src: /127.0.0.1:47578 dest: /127.0.0.1:50010
2015-12-11 11:22:31,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47578, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1152410462_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742063_1239, duration: 1972455
2015-12-11 11:22:31,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742063_1239, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:22:31,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742064_1240 src: /127.0.0.1:47579 dest: /127.0.0.1:50010
2015-12-11 11:22:31,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47579, dest: /127.0.0.1:50010, bytes: 87142, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1152410462_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742064_1240, duration: 9093616
2015-12-11 11:22:31,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742064_1240, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:22:38,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742065_1241 src: /127.0.0.1:47588 dest: /127.0.0.1:50010
2015-12-11 11:22:38,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47588, dest: /127.0.0.1:50010, bytes: 103664, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1297431993_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742065_1241, duration: 38017107
2015-12-11 11:22:38,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742065_1241, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:22:39,544 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742064_1240
2015-12-11 11:22:39,545 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742061_1237
2015-12-11 11:22:39,546 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742062_1238
2015-12-11 11:22:39,548 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742063_1239
2015-12-11 11:22:44,587 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742065_1241
2015-12-11 11:22:53,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742066_1242 src: /127.0.0.1:47603 dest: /127.0.0.1:50010
2015-12-11 11:23:03,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742067_1243 src: /127.0.0.1:47617 dest: /127.0.0.1:50010
2015-12-11 11:23:03,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47617, dest: /127.0.0.1:50010, bytes: 92, op: HDFS_WRITE, cliID: DFSClient_attempt_1449846414359_0007_r_000000_0_1254106634_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742067_1243, duration: 88574929
2015-12-11 11:23:03,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742067_1243, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:23:05,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47603, dest: /127.0.0.1:50010, bytes: 49095, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1297431993_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742066_1242, duration: 11698540221
2015-12-11 11:23:05,365 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742066_1242, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:23:05,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742068_1244 src: /127.0.0.1:47620 dest: /127.0.0.1:50010
2015-12-11 11:23:05,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47620, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1297431993_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742068_1244, duration: 1979655
2015-12-11 11:23:05,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742068_1244, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:23:05,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742069_1245 src: /127.0.0.1:47622 dest: /127.0.0.1:50010
2015-12-11 11:23:05,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47622, dest: /127.0.0.1:50010, bytes: 49095, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1297431993_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742069_1245, duration: 3250522
2015-12-11 11:23:05,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742069_1245, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:23:05,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742070_1246 src: /127.0.0.1:47623 dest: /127.0.0.1:50010
2015-12-11 11:23:05,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47623, dest: /127.0.0.1:50010, bytes: 103664, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1297431993_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742070_1246, duration: 9264684
2015-12-11 11:23:05,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742070_1246, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:23:09,608 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742067_1243
2015-12-11 11:23:11,014 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742064_1240 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742064 for deletion
2015-12-11 11:23:11,015 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742065_1241 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742065 for deletion
2015-12-11 11:23:11,015 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742066_1242 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742066 for deletion
2015-12-11 11:23:11,015 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742064_1240 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742064
2015-12-11 11:23:11,015 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742061_1237 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742061 for deletion
2015-12-11 11:23:11,015 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742062_1238 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742062 for deletion
2015-12-11 11:23:11,015 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742065_1241 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742065
2015-12-11 11:23:11,015 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742063_1239 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742063 for deletion
2015-12-11 11:23:11,016 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742066_1242 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742066
2015-12-11 11:23:11,017 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742061_1237 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742061
2015-12-11 11:23:11,017 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742062_1238 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742062
2015-12-11 11:23:11,017 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742063_1239 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742063
2015-12-11 11:23:14,616 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742070_1246
2015-12-11 11:23:14,617 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742068_1244
2015-12-11 11:23:14,618 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742069_1245
2015-12-11 11:41:02,142 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742067_1243 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742067 for deletion
2015-12-11 11:41:02,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742067_1243 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742067
2015-12-11 11:41:08,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742071_1247 src: /127.0.0.1:47653 dest: /127.0.0.1:50010
2015-12-11 11:41:08,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47653, dest: /127.0.0.1:50010, bytes: 5019, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1380328326_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742071_1247, duration: 47537956
2015-12-11 11:41:08,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742071_1247, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:41:08,275 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742072_1248 src: /127.0.0.1:47654 dest: /127.0.0.1:50010
2015-12-11 11:41:08,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47654, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1380328326_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742072_1248, duration: 1891631
2015-12-11 11:41:08,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742072_1248, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:41:08,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742073_1249 src: /127.0.0.1:47655 dest: /127.0.0.1:50010
2015-12-11 11:41:08,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47655, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1380328326_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742073_1249, duration: 2031926
2015-12-11 11:41:08,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742073_1249, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:41:08,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742074_1250 src: /127.0.0.1:47656 dest: /127.0.0.1:50010
2015-12-11 11:41:08,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47656, dest: /127.0.0.1:50010, bytes: 87273, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1380328326_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742074_1250, duration: 7884441
2015-12-11 11:41:08,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742074_1250, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:41:14,734 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742071_1247
2015-12-11 11:41:14,735 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742074_1250
2015-12-11 11:41:14,737 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742073_1249
2015-12-11 11:41:14,737 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742072_1248
2015-12-11 11:41:14,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742075_1251 src: /127.0.0.1:47664 dest: /127.0.0.1:50010
2015-12-11 11:41:14,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47664, dest: /127.0.0.1:50010, bytes: 103819, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1994650406_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742075_1251, duration: 36924045
2015-12-11 11:41:14,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742075_1251, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:41:24,797 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742075_1251
2015-12-11 11:41:30,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742076_1252 src: /127.0.0.1:47682 dest: /127.0.0.1:50010
2015-12-11 11:41:41,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742077_1253 src: /127.0.0.1:47693 dest: /127.0.0.1:50010
2015-12-11 11:41:41,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47693, dest: /127.0.0.1:50010, bytes: 92, op: HDFS_WRITE, cliID: DFSClient_attempt_1449846414359_0008_r_000000_0_-1916204941_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742077_1253, duration: 89849207
2015-12-11 11:41:41,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742077_1253, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:41:42,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47682, dest: /127.0.0.1:50010, bytes: 48671, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1994650406_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742076_1252, duration: 11148457238
2015-12-11 11:41:42,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742076_1252, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:41:42,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742078_1254 src: /127.0.0.1:47696 dest: /127.0.0.1:50010
2015-12-11 11:41:42,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47696, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1994650406_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742078_1254, duration: 2861978
2015-12-11 11:41:42,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742078_1254, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:41:42,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742079_1255 src: /127.0.0.1:47698 dest: /127.0.0.1:50010
2015-12-11 11:41:42,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47698, dest: /127.0.0.1:50010, bytes: 48671, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1994650406_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742079_1255, duration: 7898572
2015-12-11 11:41:42,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742079_1255, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:41:42,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742080_1256 src: /127.0.0.1:47699 dest: /127.0.0.1:50010
2015-12-11 11:41:42,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47699, dest: /127.0.0.1:50010, bytes: 103819, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1994650406_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742080_1256, duration: 4295110
2015-12-11 11:41:42,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742080_1256, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 11:41:47,147 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742071_1247 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742071 for deletion
2015-12-11 11:41:47,148 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742072_1248 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742072 for deletion
2015-12-11 11:41:47,149 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742073_1249 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742073 for deletion
2015-12-11 11:41:47,149 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742074_1250 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742074 for deletion
2015-12-11 11:41:47,149 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742075_1251 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742075 for deletion
2015-12-11 11:41:47,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742076_1252 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742076 for deletion
2015-12-11 11:41:47,151 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742071_1247 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742071
2015-12-11 11:41:47,151 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742072_1248 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742072
2015-12-11 11:41:47,152 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742073_1249 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742073
2015-12-11 11:41:47,152 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742074_1250 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742074
2015-12-11 11:41:47,152 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742075_1251 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742075
2015-12-11 11:41:47,152 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742076_1252 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742076
2015-12-11 11:41:49,881 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742079_1255
2015-12-11 11:41:49,883 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742080_1256
2015-12-11 11:41:49,884 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742078_1254
2015-12-11 11:41:49,885 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742077_1253
2015-12-11 14:46:44,145 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742077_1253 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742077 for deletion
2015-12-11 14:46:44,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742077_1253 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir0/blk_1073742077
2015-12-11 17:15:44,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 78 blocks total. Took 0 msec to generate and 59 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@4fda105f
2015-12-11 17:15:44,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-11 22:07:55,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from localhost/127.0.0.1:9000 with active state
2015-12-11 22:07:55,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-11 22:07:55,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-11 22:07:55,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 78 blocks total. Took 1 msec to generate and 4 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@d689844
2015-12-11 22:07:55,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-11 23:26:36,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from localhost/127.0.0.1:9000 with active state
2015-12-11 23:26:36,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-11 23:26:36,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-11 23:26:36,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 78 blocks total. Took 0 msec to generate and 4 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@3b577220
2015-12-11 23:26:36,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-11 23:32:12,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742081_1257 src: /127.0.0.1:47963 dest: /127.0.0.1:50010
2015-12-11 23:32:12,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47963, dest: /127.0.0.1:50010, bytes: 5335, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_526466774_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742081_1257, duration: 36955348
2015-12-11 23:32:12,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742081_1257, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:32:12,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742082_1258 src: /127.0.0.1:47964 dest: /127.0.0.1:50010
2015-12-11 23:32:12,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47964, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_526466774_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742082_1258, duration: 4550054
2015-12-11 23:32:12,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742082_1258, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:32:12,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742083_1259 src: /127.0.0.1:47965 dest: /127.0.0.1:50010
2015-12-11 23:32:12,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47965, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_526466774_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742083_1259, duration: 1741554
2015-12-11 23:32:12,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742083_1259, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:32:13,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742084_1260 src: /127.0.0.1:47966 dest: /127.0.0.1:50010
2015-12-11 23:32:13,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47966, dest: /127.0.0.1:50010, bytes: 87274, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_526466774_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742084_1260, duration: 12149117
2015-12-11 23:32:13,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742084_1260, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:32:19,862 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742081_1257
2015-12-11 23:32:19,864 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742083_1259
2015-12-11 23:32:19,866 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742084_1260
2015-12-11 23:32:19,867 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742082_1258
2015-12-11 23:32:21,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742085_1261 src: /127.0.0.1:47974 dest: /127.0.0.1:50010
2015-12-11 23:32:21,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47974, dest: /127.0.0.1:50010, bytes: 103820, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1082452810_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742085_1261, duration: 64302506
2015-12-11 23:32:21,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742085_1261, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:32:29,986 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742085_1261
2015-12-11 23:32:35,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742086_1262 src: /127.0.0.1:47993 dest: /127.0.0.1:50010
2015-12-11 23:33:05,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:47993, dest: /127.0.0.1:50010, bytes: 43428, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1082452810_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742086_1262, duration: 30755633206
2015-12-11 23:33:05,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742086_1262, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:33:05,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742087_1263 src: /127.0.0.1:48041 dest: /127.0.0.1:50010
2015-12-11 23:33:05,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48041, dest: /127.0.0.1:50010, bytes: 340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1082452810_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742087_1263, duration: 6569024
2015-12-11 23:33:05,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742087_1263, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:33:06,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742088_1264 src: /127.0.0.1:48043 dest: /127.0.0.1:50010
2015-12-11 23:33:06,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48043, dest: /127.0.0.1:50010, bytes: 43428, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1082452810_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742088_1264, duration: 12029242
2015-12-11 23:33:06,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742088_1264, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:33:06,442 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742089_1265 src: /127.0.0.1:48044 dest: /127.0.0.1:50010
2015-12-11 23:33:06,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48044, dest: /127.0.0.1:50010, bytes: 103820, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1082452810_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742089_1265, duration: 8977100
2015-12-11 23:33:06,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742089_1265, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:33:10,153 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742081_1257 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742081 for deletion
2015-12-11 23:33:10,154 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742082_1258 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742082 for deletion
2015-12-11 23:33:10,155 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742081_1257 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742081
2015-12-11 23:33:10,155 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742083_1259 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742083 for deletion
2015-12-11 23:33:10,156 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742084_1260 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742084 for deletion
2015-12-11 23:33:10,156 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742082_1258 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742082
2015-12-11 23:33:10,156 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742085_1261 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742085 for deletion
2015-12-11 23:33:10,156 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742083_1259 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742083
2015-12-11 23:33:10,156 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742086_1262 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742086 for deletion
2015-12-11 23:33:10,156 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742084_1260 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742084
2015-12-11 23:33:10,156 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742085_1261 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742085
2015-12-11 23:33:10,156 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742086_1262 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742086
2015-12-11 23:33:15,020 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742089_1265
2015-12-11 23:33:15,021 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742088_1264
2015-12-11 23:33:15,022 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742087_1263
2015-12-11 23:34:10,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742090_1266 src: /127.0.0.1:48051 dest: /127.0.0.1:50010
2015-12-11 23:34:10,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48051, dest: /127.0.0.1:50010, bytes: 5338, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1170460364_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742090_1266, duration: 43449829
2015-12-11 23:34:10,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742090_1266, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:34:10,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742091_1267 src: /127.0.0.1:48052 dest: /127.0.0.1:50010
2015-12-11 23:34:10,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48052, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1170460364_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742091_1267, duration: 2274084
2015-12-11 23:34:10,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742091_1267, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:34:10,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742092_1268 src: /127.0.0.1:48053 dest: /127.0.0.1:50010
2015-12-11 23:34:10,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48053, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1170460364_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742092_1268, duration: 2008152
2015-12-11 23:34:10,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742092_1268, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:34:10,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742093_1269 src: /127.0.0.1:48054 dest: /127.0.0.1:50010
2015-12-11 23:34:10,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48054, dest: /127.0.0.1:50010, bytes: 87276, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1170460364_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742093_1269, duration: 17776461
2015-12-11 23:34:10,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742093_1269, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:34:15,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742094_1270 src: /127.0.0.1:48062 dest: /127.0.0.1:50010
2015-12-11 23:34:15,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48062, dest: /127.0.0.1:50010, bytes: 103822, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1317231136_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742094_1270, duration: 32182950
2015-12-11 23:34:15,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742094_1270, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:34:20,038 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742090_1266
2015-12-11 23:34:20,039 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742091_1267
2015-12-11 23:34:20,046 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742093_1269
2015-12-11 23:34:20,048 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742092_1268
2015-12-11 23:34:25,057 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742094_1270
2015-12-11 23:34:33,303 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4566ms
No GCs detected
2015-12-11 23:34:34,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742095_1271 src: /127.0.0.1:48083 dest: /127.0.0.1:50010
2015-12-11 23:35:07,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48083, dest: /127.0.0.1:50010, bytes: 45444, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1317231136_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742095_1271, duration: 33439046547
2015-12-11 23:35:07,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742095_1271, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:35:07,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742096_1272 src: /127.0.0.1:48131 dest: /127.0.0.1:50010
2015-12-11 23:35:07,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48131, dest: /127.0.0.1:50010, bytes: 340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1317231136_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742096_1272, duration: 4739698
2015-12-11 23:35:07,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742096_1272, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:35:07,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742097_1273 src: /127.0.0.1:48133 dest: /127.0.0.1:50010
2015-12-11 23:35:07,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48133, dest: /127.0.0.1:50010, bytes: 45444, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1317231136_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742097_1273, duration: 7670230
2015-12-11 23:35:07,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742097_1273, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:35:07,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742098_1274 src: /127.0.0.1:48134 dest: /127.0.0.1:50010
2015-12-11 23:35:07,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48134, dest: /127.0.0.1:50010, bytes: 103822, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1317231136_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742098_1274, duration: 15815645
2015-12-11 23:35:07,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742098_1274, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:35:13,244 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742096_1272
2015-12-11 23:35:13,245 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742098_1274
2015-12-11 23:35:13,246 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742097_1273
2015-12-11 23:35:13,246 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742095_1271
2015-12-11 23:35:15,085 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742090_1266 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742090 for deletion
2015-12-11 23:35:15,086 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742091_1267 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742091 for deletion
2015-12-11 23:35:15,086 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742092_1268 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742092 for deletion
2015-12-11 23:35:15,086 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742090_1266 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742090
2015-12-11 23:35:15,086 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742093_1269 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742093 for deletion
2015-12-11 23:35:15,086 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742091_1267 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742091
2015-12-11 23:35:15,086 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742094_1270 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742094 for deletion
2015-12-11 23:35:15,086 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742092_1268 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742092
2015-12-11 23:35:15,086 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742095_1271 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742095 for deletion
2015-12-11 23:35:15,086 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742093_1269 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742093
2015-12-11 23:35:15,086 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742094_1270 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742094
2015-12-11 23:35:15,086 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742095_1271 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742095
2015-12-11 23:50:08,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742099_1275 src: /127.0.0.1:48164 dest: /127.0.0.1:50010
2015-12-11 23:50:08,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48164, dest: /127.0.0.1:50010, bytes: 5319, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1942254671_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742099_1275, duration: 41202933
2015-12-11 23:50:08,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742099_1275, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:50:09,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742100_1276 src: /127.0.0.1:48165 dest: /127.0.0.1:50010
2015-12-11 23:50:09,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48165, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1942254671_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742100_1276, duration: 2512009
2015-12-11 23:50:09,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742100_1276, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:50:09,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742101_1277 src: /127.0.0.1:48166 dest: /127.0.0.1:50010
2015-12-11 23:50:09,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48166, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1942254671_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742101_1277, duration: 1852908
2015-12-11 23:50:09,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742101_1277, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:50:09,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742102_1278 src: /127.0.0.1:48167 dest: /127.0.0.1:50010
2015-12-11 23:50:09,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48167, dest: /127.0.0.1:50010, bytes: 87145, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1942254671_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742102_1278, duration: 20147825
2015-12-11 23:50:09,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742102_1278, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:50:16,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742103_1279 src: /127.0.0.1:48177 dest: /127.0.0.1:50010
2015-12-11 23:50:16,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48177, dest: /127.0.0.1:50010, bytes: 103667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_519660369_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742103_1279, duration: 62589858
2015-12-11 23:50:16,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742103_1279, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:50:18,372 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742101_1277
2015-12-11 23:50:18,375 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742100_1276
2015-12-11 23:50:18,379 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742102_1278
2015-12-11 23:50:18,381 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742099_1275
2015-12-11 23:50:23,591 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742103_1279
2015-12-11 23:50:29,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742104_1280 src: /127.0.0.1:48194 dest: /127.0.0.1:50010
2015-12-11 23:51:04,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48194, dest: /127.0.0.1:50010, bytes: 48401, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_519660369_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742104_1280, duration: 35233511818
2015-12-11 23:51:04,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742104_1280, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:51:04,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742105_1281 src: /127.0.0.1:48243 dest: /127.0.0.1:50010
2015-12-11 23:51:04,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48243, dest: /127.0.0.1:50010, bytes: 340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_519660369_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742105_1281, duration: 1662326
2015-12-11 23:51:04,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742105_1281, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:51:04,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742106_1282 src: /127.0.0.1:48245 dest: /127.0.0.1:50010
2015-12-11 23:51:04,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48245, dest: /127.0.0.1:50010, bytes: 48401, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_519660369_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742106_1282, duration: 6315822
2015-12-11 23:51:04,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742106_1282, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:51:04,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742107_1283 src: /127.0.0.1:48246 dest: /127.0.0.1:50010
2015-12-11 23:51:04,972 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48246, dest: /127.0.0.1:50010, bytes: 103667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_519660369_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742107_1283, duration: 11428026
2015-12-11 23:51:04,973 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742107_1283, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:51:09,239 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742099_1275 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742099 for deletion
2015-12-11 23:51:09,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742100_1276 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742100 for deletion
2015-12-11 23:51:09,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742101_1277 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742101 for deletion
2015-12-11 23:51:09,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742099_1275 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742099
2015-12-11 23:51:09,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742102_1278 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742102 for deletion
2015-12-11 23:51:09,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742100_1276 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742100
2015-12-11 23:51:09,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742103_1279 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742103 for deletion
2015-12-11 23:51:09,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742101_1277 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742101
2015-12-11 23:51:09,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742104_1280 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742104 for deletion
2015-12-11 23:51:09,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742102_1278 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742102
2015-12-11 23:51:09,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742103_1279 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742103
2015-12-11 23:51:09,242 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742104_1280 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742104
2015-12-11 23:51:13,616 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742106_1282
2015-12-11 23:51:13,618 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742107_1283
2015-12-11 23:51:13,618 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742105_1281
2015-12-11 23:53:08,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742108_1284 src: /127.0.0.1:48255 dest: /127.0.0.1:50010
2015-12-11 23:53:08,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48255, dest: /127.0.0.1:50010, bytes: 5364, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-501274724_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742108_1284, duration: 39124832
2015-12-11 23:53:08,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742108_1284, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:53:09,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742109_1285 src: /127.0.0.1:48256 dest: /127.0.0.1:50010
2015-12-11 23:53:09,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48256, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-501274724_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742109_1285, duration: 1939362
2015-12-11 23:53:09,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742109_1285, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:53:09,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742110_1286 src: /127.0.0.1:48257 dest: /127.0.0.1:50010
2015-12-11 23:53:09,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48257, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-501274724_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742110_1286, duration: 1974516
2015-12-11 23:53:09,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742110_1286, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:53:09,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742111_1287 src: /127.0.0.1:48258 dest: /127.0.0.1:50010
2015-12-11 23:53:09,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48258, dest: /127.0.0.1:50010, bytes: 87276, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-501274724_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742111_1287, duration: 17626554
2015-12-11 23:53:09,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742111_1287, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:53:14,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742112_1288 src: /127.0.0.1:48267 dest: /127.0.0.1:50010
2015-12-11 23:53:14,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48267, dest: /127.0.0.1:50010, bytes: 103822, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1443065067_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742112_1288, duration: 34543226
2015-12-11 23:53:14,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742112_1288, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:53:18,656 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742108_1284
2015-12-11 23:53:18,704 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742109_1285
2015-12-11 23:53:18,705 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742111_1287
2015-12-11 23:53:18,710 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742110_1286
2015-12-11 23:53:23,718 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742112_1288
2015-12-11 23:53:28,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742113_1289 src: /127.0.0.1:48287 dest: /127.0.0.1:50010
2015-12-11 23:54:02,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48287, dest: /127.0.0.1:50010, bytes: 45077, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1443065067_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742113_1289, duration: 33684838759
2015-12-11 23:54:02,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742113_1289, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:54:02,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742114_1290 src: /127.0.0.1:48334 dest: /127.0.0.1:50010
2015-12-11 23:54:02,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48334, dest: /127.0.0.1:50010, bytes: 340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1443065067_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742114_1290, duration: 2647162
2015-12-11 23:54:02,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742114_1290, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:54:02,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742115_1291 src: /127.0.0.1:48336 dest: /127.0.0.1:50010
2015-12-11 23:54:02,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48336, dest: /127.0.0.1:50010, bytes: 45077, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1443065067_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742115_1291, duration: 9131692
2015-12-11 23:54:02,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742115_1291, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:54:02,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742116_1292 src: /127.0.0.1:48337 dest: /127.0.0.1:50010
2015-12-11 23:54:02,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48337, dest: /127.0.0.1:50010, bytes: 103822, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1443065067_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742116_1292, duration: 6058353
2015-12-11 23:54:02,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742116_1292, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-11 23:54:08,774 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742115_1291
2015-12-11 23:54:08,851 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742116_1292
2015-12-11 23:54:08,853 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742113_1289
2015-12-11 23:54:08,854 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742114_1290
2015-12-11 23:54:09,274 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742112_1288 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742112 for deletion
2015-12-11 23:54:09,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742113_1289 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742113 for deletion
2015-12-11 23:54:09,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742108_1284 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742108 for deletion
2015-12-11 23:54:09,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742112_1288 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742112
2015-12-11 23:54:09,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742109_1285 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742109 for deletion
2015-12-11 23:54:09,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742110_1286 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742110 for deletion
2015-12-11 23:54:09,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742113_1289 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742113
2015-12-11 23:54:09,276 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742111_1287 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742111 for deletion
2015-12-11 23:54:09,276 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742108_1284 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742108
2015-12-11 23:54:09,276 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742109_1285 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742109
2015-12-11 23:54:09,276 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742110_1286 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742110
2015-12-11 23:54:09,276 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742111_1287 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742111
2015-12-12 00:24:09,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742117_1293 src: /127.0.0.1:48393 dest: /127.0.0.1:50010
2015-12-12 00:24:09,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48393, dest: /127.0.0.1:50010, bytes: 5825, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1171860838_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742117_1293, duration: 57785276
2015-12-12 00:24:09,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742117_1293, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:24:09,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742118_1294 src: /127.0.0.1:48394 dest: /127.0.0.1:50010
2015-12-12 00:24:09,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48394, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1171860838_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742118_1294, duration: 6472075
2015-12-12 00:24:09,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742118_1294, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:24:09,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742119_1295 src: /127.0.0.1:48395 dest: /127.0.0.1:50010
2015-12-12 00:24:09,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48395, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1171860838_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742119_1295, duration: 1824503
2015-12-12 00:24:09,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742119_1295, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:24:09,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742120_1296 src: /127.0.0.1:48396 dest: /127.0.0.1:50010
2015-12-12 00:24:09,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48396, dest: /127.0.0.1:50010, bytes: 87145, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1171860838_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742120_1296, duration: 4590596
2015-12-12 00:24:09,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742120_1296, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:24:15,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742121_1297 src: /127.0.0.1:48405 dest: /127.0.0.1:50010
2015-12-12 00:24:15,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48405, dest: /127.0.0.1:50010, bytes: 103667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2015500094_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742121_1297, duration: 60592026
2015-12-12 00:24:15,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742121_1297, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:24:19,057 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742117_1293
2015-12-12 00:24:19,065 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742120_1296
2015-12-12 00:24:19,066 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742119_1295
2015-12-12 00:24:19,069 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742118_1294
2015-12-12 00:24:24,078 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742121_1297
2015-12-12 00:24:29,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742122_1298 src: /127.0.0.1:48424 dest: /127.0.0.1:50010
2015-12-12 00:25:01,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48424, dest: /127.0.0.1:50010, bytes: 44683, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2015500094_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742122_1298, duration: 31889014806
2015-12-12 00:25:01,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742122_1298, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:25:01,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742123_1299 src: /127.0.0.1:48471 dest: /127.0.0.1:50010
2015-12-12 00:25:01,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48471, dest: /127.0.0.1:50010, bytes: 340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2015500094_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742123_1299, duration: 18544075
2015-12-12 00:25:01,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742123_1299, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:25:01,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742124_1300 src: /127.0.0.1:48473 dest: /127.0.0.1:50010
2015-12-12 00:25:01,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48473, dest: /127.0.0.1:50010, bytes: 44683, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2015500094_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742124_1300, duration: 7807592
2015-12-12 00:25:01,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742124_1300, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:25:01,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742125_1301 src: /127.0.0.1:48474 dest: /127.0.0.1:50010
2015-12-12 00:25:01,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48474, dest: /127.0.0.1:50010, bytes: 103667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2015500094_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742125_1301, duration: 8744841
2015-12-12 00:25:01,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742125_1301, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:25:06,525 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742117_1293 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742117 for deletion
2015-12-12 00:25:06,525 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742118_1294 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742118 for deletion
2015-12-12 00:25:06,525 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742119_1295 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742119 for deletion
2015-12-12 00:25:06,525 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742120_1296 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742120 for deletion
2015-12-12 00:25:06,526 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742121_1297 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742121 for deletion
2015-12-12 00:25:06,526 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742122_1298 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742122 for deletion
2015-12-12 00:25:06,526 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742117_1293 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742117
2015-12-12 00:25:06,526 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742118_1294 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742118
2015-12-12 00:25:06,526 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742119_1295 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742119
2015-12-12 00:25:06,526 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742120_1296 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742120
2015-12-12 00:25:06,526 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742121_1297 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742121
2015-12-12 00:25:06,526 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742122_1298 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742122
2015-12-12 00:25:09,110 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742124_1300
2015-12-12 00:25:09,111 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742123_1299
2015-12-12 00:25:09,112 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742125_1301
2015-12-12 00:25:27,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742126_1302 src: /127.0.0.1:48482 dest: /127.0.0.1:50010
2015-12-12 00:25:27,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48482, dest: /127.0.0.1:50010, bytes: 5819, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-905324509_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742126_1302, duration: 56930712
2015-12-12 00:25:27,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742126_1302, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:25:27,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742127_1303 src: /127.0.0.1:48483 dest: /127.0.0.1:50010
2015-12-12 00:25:27,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48483, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-905324509_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742127_1303, duration: 1917416
2015-12-12 00:25:27,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742127_1303, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:25:27,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742128_1304 src: /127.0.0.1:48484 dest: /127.0.0.1:50010
2015-12-12 00:25:27,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48484, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-905324509_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742128_1304, duration: 1756179
2015-12-12 00:25:27,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742128_1304, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:25:28,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742129_1305 src: /127.0.0.1:48485 dest: /127.0.0.1:50010
2015-12-12 00:25:28,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48485, dest: /127.0.0.1:50010, bytes: 87138, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-905324509_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742129_1305, duration: 11137096
2015-12-12 00:25:28,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742129_1305, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:25:33,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742130_1306 src: /127.0.0.1:48493 dest: /127.0.0.1:50010
2015-12-12 00:25:33,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48493, dest: /127.0.0.1:50010, bytes: 103660, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-654609605_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742130_1306, duration: 45550360
2015-12-12 00:25:33,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742130_1306, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:25:34,141 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742128_1304
2015-12-12 00:25:34,141 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742126_1302
2015-12-12 00:25:34,141 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742127_1303
2015-12-12 00:25:34,142 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742129_1305
2015-12-12 00:25:39,166 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742130_1306
2015-12-12 00:25:46,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742131_1307 src: /127.0.0.1:48509 dest: /127.0.0.1:50010
2015-12-12 00:26:01,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742132_1308 src: /127.0.0.1:48527 dest: /127.0.0.1:50010
2015-12-12 00:26:02,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48527, dest: /127.0.0.1:50010, bytes: 1082267, op: HDFS_WRITE, cliID: DFSClient_attempt_1449846414359_0014_r_000000_0_1769165486_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742132_1308, duration: 353744529
2015-12-12 00:26:02,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742132_1308, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:26:02,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48509, dest: /127.0.0.1:50010, bytes: 50641, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-654609605_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742131_1307, duration: 16335681677
2015-12-12 00:26:02,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742131_1307, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:26:02,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742133_1309 src: /127.0.0.1:48530 dest: /127.0.0.1:50010
2015-12-12 00:26:02,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48530, dest: /127.0.0.1:50010, bytes: 355, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-654609605_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742133_1309, duration: 3328872
2015-12-12 00:26:02,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742133_1309, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:26:02,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742134_1310 src: /127.0.0.1:48532 dest: /127.0.0.1:50010
2015-12-12 00:26:02,973 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48532, dest: /127.0.0.1:50010, bytes: 50641, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-654609605_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742134_1310, duration: 12565601
2015-12-12 00:26:02,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742134_1310, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:26:03,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742135_1311 src: /127.0.0.1:48533 dest: /127.0.0.1:50010
2015-12-12 00:26:03,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48533, dest: /127.0.0.1:50010, bytes: 103660, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-654609605_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742135_1311, duration: 10760076
2015-12-12 00:26:03,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742135_1311, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:26:09,233 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742134_1310
2015-12-12 00:26:09,236 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742135_1311
2015-12-12 00:26:09,240 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742133_1309
2015-12-12 00:26:09,241 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742131_1307
2015-12-12 00:26:09,549 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742128_1304 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742128 for deletion
2015-12-12 00:26:09,551 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742129_1305 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742129 for deletion
2015-12-12 00:26:09,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742130_1306 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742130 for deletion
2015-12-12 00:26:09,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742128_1304 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742128
2015-12-12 00:26:09,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742131_1307 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742131 for deletion
2015-12-12 00:26:09,556 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742129_1305 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742129
2015-12-12 00:26:09,556 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742126_1302 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742126 for deletion
2015-12-12 00:26:09,556 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742130_1306 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742130
2015-12-12 00:26:09,556 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742127_1303 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742127 for deletion
2015-12-12 00:26:09,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742131_1307 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742131
2015-12-12 00:26:09,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742126_1302 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742126
2015-12-12 00:26:09,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742127_1303 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742127
2015-12-12 00:30:36,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742132_1308 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742132 for deletion
2015-12-12 00:30:36,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742132_1308 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742132
2015-12-12 00:30:36,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742136_1312 src: /127.0.0.1:48545 dest: /127.0.0.1:50010
2015-12-12 00:30:37,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48545, dest: /127.0.0.1:50010, bytes: 5819, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1326460505_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742136_1312, duration: 40277158
2015-12-12 00:30:37,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742136_1312, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:30:37,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742137_1313 src: /127.0.0.1:48546 dest: /127.0.0.1:50010
2015-12-12 00:30:37,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48546, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1326460505_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742137_1313, duration: 1991054
2015-12-12 00:30:37,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742137_1313, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:30:37,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742138_1314 src: /127.0.0.1:48547 dest: /127.0.0.1:50010
2015-12-12 00:30:37,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48547, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1326460505_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742138_1314, duration: 2040611
2015-12-12 00:30:37,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742138_1314, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:30:44,734 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742137_1313
2015-12-12 00:30:44,734 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742138_1314
2015-12-12 00:30:44,735 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742136_1312
2015-12-12 00:31:13,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742139_1315 src: /127.0.0.1:48551 dest: /127.0.0.1:50010
2015-12-12 00:31:13,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48551, dest: /127.0.0.1:50010, bytes: 5843, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-867980133_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742139_1315, duration: 37927859
2015-12-12 00:31:13,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742139_1315, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:31:13,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742140_1316 src: /127.0.0.1:48552 dest: /127.0.0.1:50010
2015-12-12 00:31:13,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48552, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-867980133_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742140_1316, duration: 6749875
2015-12-12 00:31:13,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742140_1316, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:31:13,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742141_1317 src: /127.0.0.1:48553 dest: /127.0.0.1:50010
2015-12-12 00:31:13,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48553, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-867980133_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742141_1317, duration: 2047299
2015-12-12 00:31:13,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742141_1317, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:31:13,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742142_1318 src: /127.0.0.1:48554 dest: /127.0.0.1:50010
2015-12-12 00:31:13,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48554, dest: /127.0.0.1:50010, bytes: 87138, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-867980133_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742142_1318, duration: 6446777
2015-12-12 00:31:13,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742142_1318, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:31:19,746 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742139_1315
2015-12-12 00:31:19,747 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742142_1318
2015-12-12 00:31:19,748 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742141_1317
2015-12-12 00:31:19,748 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742140_1316
2015-12-12 00:31:20,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742143_1319 src: /127.0.0.1:48563 dest: /127.0.0.1:50010
2015-12-12 00:31:20,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48563, dest: /127.0.0.1:50010, bytes: 103660, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-237712542_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742143_1319, duration: 43895065
2015-12-12 00:31:20,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742143_1319, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:31:29,786 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742143_1319
2015-12-12 00:31:34,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742144_1320 src: /127.0.0.1:48580 dest: /127.0.0.1:50010
2015-12-12 00:31:43,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742145_1321 src: /127.0.0.1:48591 dest: /127.0.0.1:50010
2015-12-12 00:31:44,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48591, dest: /127.0.0.1:50010, bytes: 1082267, op: HDFS_WRITE, cliID: DFSClient_attempt_1449846414359_0016_r_000000_0_-1503813752_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742145_1321, duration: 268058276
2015-12-12 00:31:44,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742145_1321, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:31:44,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48580, dest: /127.0.0.1:50010, bytes: 48200, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-237712542_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742144_1320, duration: 9743207569
2015-12-12 00:31:44,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742144_1320, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:31:44,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742146_1322 src: /127.0.0.1:48593 dest: /127.0.0.1:50010
2015-12-12 00:31:44,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48593, dest: /127.0.0.1:50010, bytes: 354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-237712542_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742146_1322, duration: 1867117
2015-12-12 00:31:44,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742146_1322, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:31:44,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742147_1323 src: /127.0.0.1:48595 dest: /127.0.0.1:50010
2015-12-12 00:31:44,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48595, dest: /127.0.0.1:50010, bytes: 48200, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-237712542_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742147_1323, duration: 2989046
2015-12-12 00:31:44,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742147_1323, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:31:44,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742148_1324 src: /127.0.0.1:48596 dest: /127.0.0.1:50010
2015-12-12 00:31:44,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48596, dest: /127.0.0.1:50010, bytes: 103660, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-237712542_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742148_1324, duration: 4606266
2015-12-12 00:31:44,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742148_1324, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:31:48,566 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742144_1320 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742144 for deletion
2015-12-12 00:31:48,566 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742139_1315 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742139 for deletion
2015-12-12 00:31:48,566 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742140_1316 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742140 for deletion
2015-12-12 00:31:48,567 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742141_1317 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742141 for deletion
2015-12-12 00:31:48,567 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742142_1318 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742142 for deletion
2015-12-12 00:31:48,567 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742143_1319 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742143 for deletion
2015-12-12 00:31:48,567 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742144_1320 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742144
2015-12-12 00:31:48,567 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742139_1315 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742139
2015-12-12 00:31:48,567 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742140_1316 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742140
2015-12-12 00:31:48,567 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742141_1317 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742141
2015-12-12 00:31:48,567 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742142_1318 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742142
2015-12-12 00:31:48,567 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742143_1319 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742143
2015-12-12 00:31:49,881 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742146_1322
2015-12-12 00:31:49,895 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742147_1323
2015-12-12 00:31:49,900 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742148_1324
2015-12-12 00:31:50,893 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742145_1321
2015-12-12 00:34:42,580 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742145_1321 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742145 for deletion
2015-12-12 00:34:42,581 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742145_1321 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742145
2015-12-12 00:34:44,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742149_1325 src: /127.0.0.1:48606 dest: /127.0.0.1:50010
2015-12-12 00:34:44,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48606, dest: /127.0.0.1:50010, bytes: 5859, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1535496488_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742149_1325, duration: 39582993
2015-12-12 00:34:44,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742149_1325, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:34:44,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742150_1326 src: /127.0.0.1:48607 dest: /127.0.0.1:50010
2015-12-12 00:34:44,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48607, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1535496488_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742150_1326, duration: 2067974
2015-12-12 00:34:44,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742150_1326, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:34:44,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742151_1327 src: /127.0.0.1:48608 dest: /127.0.0.1:50010
2015-12-12 00:34:44,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48608, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1535496488_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742151_1327, duration: 2047991
2015-12-12 00:34:44,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742151_1327, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:34:44,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742152_1328 src: /127.0.0.1:48609 dest: /127.0.0.1:50010
2015-12-12 00:34:44,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48609, dest: /127.0.0.1:50010, bytes: 87269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1535496488_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742152_1328, duration: 9790304
2015-12-12 00:34:44,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742152_1328, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:34:49,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742153_1329 src: /127.0.0.1:48617 dest: /127.0.0.1:50010
2015-12-12 00:34:49,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48617, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1729789510_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742153_1329, duration: 43098055
2015-12-12 00:34:49,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742153_1329, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:34:50,922 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742152_1328
2015-12-12 00:34:50,922 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742149_1325
2015-12-12 00:34:50,923 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742151_1327
2015-12-12 00:34:50,923 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742150_1326
2015-12-12 00:34:55,934 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742153_1329
2015-12-12 00:35:02,239 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1102ms
No GCs detected
2015-12-12 00:35:04,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742154_1330 src: /127.0.0.1:48635 dest: /127.0.0.1:50010
2015-12-12 00:35:36,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48635, dest: /127.0.0.1:50010, bytes: 43189, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1729789510_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742154_1330, duration: 32804111075
2015-12-12 00:35:36,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742154_1330, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:35:37,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742155_1331 src: /127.0.0.1:48684 dest: /127.0.0.1:50010
2015-12-12 00:35:37,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48684, dest: /127.0.0.1:50010, bytes: 340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1729789510_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742155_1331, duration: 1077245
2015-12-12 00:35:37,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742155_1331, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:35:37,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742156_1332 src: /127.0.0.1:48686 dest: /127.0.0.1:50010
2015-12-12 00:35:37,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48686, dest: /127.0.0.1:50010, bytes: 43189, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1729789510_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742156_1332, duration: 13508018
2015-12-12 00:35:37,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742156_1332, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:35:37,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742157_1333 src: /127.0.0.1:48687 dest: /127.0.0.1:50010
2015-12-12 00:35:37,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48687, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1729789510_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742157_1333, duration: 5818323
2015-12-12 00:35:37,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742157_1333, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:35:42,171 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742154_1330
2015-12-12 00:35:42,812 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742149_1325 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742149 for deletion
2015-12-12 00:35:42,813 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742150_1326 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742150 for deletion
2015-12-12 00:35:42,813 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742151_1327 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742151 for deletion
2015-12-12 00:35:42,813 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742149_1325 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742149
2015-12-12 00:35:42,813 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742152_1328 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742152 for deletion
2015-12-12 00:35:42,813 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742150_1326 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742150
2015-12-12 00:35:42,813 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742153_1329 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742153 for deletion
2015-12-12 00:35:42,813 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742151_1327 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742151
2015-12-12 00:35:42,813 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742154_1330 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742154 for deletion
2015-12-12 00:35:42,813 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742152_1328 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742152
2015-12-12 00:35:42,813 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742153_1329 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742153
2015-12-12 00:35:42,814 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742154_1330 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742154
2015-12-12 00:35:47,183 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742156_1332
2015-12-12 00:35:47,185 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742155_1331
2015-12-12 00:35:47,186 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742157_1333
2015-12-12 00:38:51,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742158_1334 src: /127.0.0.1:48696 dest: /127.0.0.1:50010
2015-12-12 00:38:51,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48696, dest: /127.0.0.1:50010, bytes: 5836, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_832554930_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742158_1334, duration: 48962908
2015-12-12 00:38:51,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742158_1334, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:38:51,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742159_1335 src: /127.0.0.1:48697 dest: /127.0.0.1:50010
2015-12-12 00:38:51,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48697, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_832554930_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742159_1335, duration: 2062253
2015-12-12 00:38:51,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742159_1335, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:38:51,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742160_1336 src: /127.0.0.1:48698 dest: /127.0.0.1:50010
2015-12-12 00:38:51,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48698, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_832554930_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742160_1336, duration: 1744755
2015-12-12 00:38:51,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742160_1336, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:38:51,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742161_1337 src: /127.0.0.1:48699 dest: /127.0.0.1:50010
2015-12-12 00:38:51,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48699, dest: /127.0.0.1:50010, bytes: 87269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_832554930_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742161_1337, duration: 11855730
2015-12-12 00:38:51,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742161_1337, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:38:57,211 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742159_1335
2015-12-12 00:38:57,214 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742161_1337
2015-12-12 00:38:57,217 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742160_1336
2015-12-12 00:38:57,218 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742158_1334
2015-12-12 00:38:58,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742162_1338 src: /127.0.0.1:48707 dest: /127.0.0.1:50010
2015-12-12 00:38:58,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48707, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1720607980_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742162_1338, duration: 98062170
2015-12-12 00:38:58,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742162_1338, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:39:07,247 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742162_1338
2015-12-12 00:39:11,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742163_1339 src: /127.0.0.1:48723 dest: /127.0.0.1:50010
2015-12-12 00:39:22,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742164_1340 src: /127.0.0.1:48738 dest: /127.0.0.1:50010
2015-12-12 00:39:22,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48738, dest: /127.0.0.1:50010, bytes: 129, op: HDFS_WRITE, cliID: DFSClient_attempt_1449846414359_0018_r_000000_0_165233177_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742164_1340, duration: 104299586
2015-12-12 00:39:22,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742164_1340, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:39:22,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48723, dest: /127.0.0.1:50010, bytes: 48635, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1720607980_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742163_1339, duration: 10632398611
2015-12-12 00:39:22,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742163_1339, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:39:22,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742165_1341 src: /127.0.0.1:48741 dest: /127.0.0.1:50010
2015-12-12 00:39:22,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48741, dest: /127.0.0.1:50010, bytes: 354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1720607980_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742165_1341, duration: 8599086
2015-12-12 00:39:22,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742165_1341, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:39:22,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742166_1342 src: /127.0.0.1:48743 dest: /127.0.0.1:50010
2015-12-12 00:39:22,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48743, dest: /127.0.0.1:50010, bytes: 48635, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1720607980_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742166_1342, duration: 11086605
2015-12-12 00:39:22,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742166_1342, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:39:22,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742167_1343 src: /127.0.0.1:48744 dest: /127.0.0.1:50010
2015-12-12 00:39:22,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48744, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1720607980_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742167_1343, duration: 9069706
2015-12-12 00:39:22,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742167_1343, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:39:27,347 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742164_1340
2015-12-12 00:39:27,872 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742160_1336 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742160 for deletion
2015-12-12 00:39:27,873 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742161_1337 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742161 for deletion
2015-12-12 00:39:27,873 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742162_1338 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742162 for deletion
2015-12-12 00:39:27,873 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742163_1339 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742163 for deletion
2015-12-12 00:39:27,873 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742158_1334 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742158 for deletion
2015-12-12 00:39:27,873 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742159_1335 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742159 for deletion
2015-12-12 00:39:27,873 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742160_1336 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742160
2015-12-12 00:39:27,873 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742161_1337 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742161
2015-12-12 00:39:27,874 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742162_1338 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742162
2015-12-12 00:39:27,874 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742163_1339 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742163
2015-12-12 00:39:27,874 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742158_1334 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742158
2015-12-12 00:39:27,874 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742159_1335 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742159
2015-12-12 00:39:32,353 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742166_1342
2015-12-12 00:39:32,354 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742165_1341
2015-12-12 00:39:32,355 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742167_1343
2015-12-12 00:41:42,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742164_1340 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742164 for deletion
2015-12-12 00:41:42,882 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742164_1340 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742164
2015-12-12 00:41:47,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742168_1344 src: /127.0.0.1:48754 dest: /127.0.0.1:50010
2015-12-12 00:41:47,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48754, dest: /127.0.0.1:50010, bytes: 5835, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1472110699_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742168_1344, duration: 40177806
2015-12-12 00:41:47,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742168_1344, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:41:47,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742169_1345 src: /127.0.0.1:48755 dest: /127.0.0.1:50010
2015-12-12 00:41:47,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48755, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1472110699_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742169_1345, duration: 2047726
2015-12-12 00:41:47,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742169_1345, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:41:47,538 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742170_1346 src: /127.0.0.1:48756 dest: /127.0.0.1:50010
2015-12-12 00:41:47,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48756, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1472110699_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742170_1346, duration: 1756603
2015-12-12 00:41:47,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742170_1346, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:41:47,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742171_1347 src: /127.0.0.1:48757 dest: /127.0.0.1:50010
2015-12-12 00:41:47,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48757, dest: /127.0.0.1:50010, bytes: 87267, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1472110699_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742171_1347, duration: 11669337
2015-12-12 00:41:47,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742171_1347, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:41:53,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742172_1348 src: /127.0.0.1:48766 dest: /127.0.0.1:50010
2015-12-12 00:41:53,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48766, dest: /127.0.0.1:50010, bytes: 103813, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_567194823_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742172_1348, duration: 33888420
2015-12-12 00:41:53,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742172_1348, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:41:57,375 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742169_1345
2015-12-12 00:41:57,381 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742170_1346
2015-12-12 00:41:57,384 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742168_1344
2015-12-12 00:41:57,386 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742171_1347
2015-12-12 00:42:02,408 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742172_1348
2015-12-12 00:42:10,573 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4724ms
No GCs detected
2015-12-12 00:42:11,960 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742173_1349 src: /127.0.0.1:48789 dest: /127.0.0.1:50010
2015-12-12 00:42:44,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48789, dest: /127.0.0.1:50010, bytes: 44341, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_567194823_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742173_1349, duration: 32862086825
2015-12-12 00:42:44,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742173_1349, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:42:44,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742174_1350 src: /127.0.0.1:48835 dest: /127.0.0.1:50010
2015-12-12 00:42:44,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48835, dest: /127.0.0.1:50010, bytes: 340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_567194823_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742174_1350, duration: 2960058
2015-12-12 00:42:44,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742174_1350, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:42:45,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742175_1351 src: /127.0.0.1:48837 dest: /127.0.0.1:50010
2015-12-12 00:42:45,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48837, dest: /127.0.0.1:50010, bytes: 44341, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_567194823_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742175_1351, duration: 8650544
2015-12-12 00:42:45,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742175_1351, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:42:45,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742176_1352 src: /127.0.0.1:48838 dest: /127.0.0.1:50010
2015-12-12 00:42:45,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48838, dest: /127.0.0.1:50010, bytes: 103813, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_567194823_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742176_1352, duration: 3689557
2015-12-12 00:42:45,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742176_1352, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:42:49,366 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742168_1344 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742168 for deletion
2015-12-12 00:42:49,367 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742169_1345 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742169 for deletion
2015-12-12 00:42:49,368 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742168_1344 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742168
2015-12-12 00:42:49,368 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742170_1346 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742170 for deletion
2015-12-12 00:42:49,369 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742169_1345 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742169
2015-12-12 00:42:49,369 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742171_1347 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742171 for deletion
2015-12-12 00:42:49,369 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742170_1346 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742170
2015-12-12 00:42:49,370 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742172_1348 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742172 for deletion
2015-12-12 00:42:49,370 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742171_1347 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742171
2015-12-12 00:42:49,370 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742173_1349 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742173 for deletion
2015-12-12 00:42:49,371 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742172_1348 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742172
2015-12-12 00:42:49,371 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742173_1349 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742173
2015-12-12 00:42:50,394 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742175_1351
2015-12-12 00:42:50,395 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742174_1350
2015-12-12 00:42:50,396 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742176_1352
2015-12-12 00:45:27,364 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742177_1353 src: /127.0.0.1:48848 dest: /127.0.0.1:50010
2015-12-12 00:45:27,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48848, dest: /127.0.0.1:50010, bytes: 5835, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1570665407_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742177_1353, duration: 62602227
2015-12-12 00:45:27,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742177_1353, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:45:27,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742178_1354 src: /127.0.0.1:48849 dest: /127.0.0.1:50010
2015-12-12 00:45:27,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48849, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1570665407_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742178_1354, duration: 2049550
2015-12-12 00:45:27,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742178_1354, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:45:27,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742179_1355 src: /127.0.0.1:48850 dest: /127.0.0.1:50010
2015-12-12 00:45:27,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48850, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1570665407_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742179_1355, duration: 2055493
2015-12-12 00:45:27,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742179_1355, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:45:27,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742180_1356 src: /127.0.0.1:48851 dest: /127.0.0.1:50010
2015-12-12 00:45:27,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48851, dest: /127.0.0.1:50010, bytes: 87269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1570665407_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742180_1356, duration: 9503762
2015-12-12 00:45:27,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742180_1356, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:45:34,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742181_1357 src: /127.0.0.1:48859 dest: /127.0.0.1:50010
2015-12-12 00:45:34,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48859, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1372391359_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742181_1357, duration: 42962994
2015-12-12 00:45:34,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742181_1357, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:45:35,414 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742177_1353
2015-12-12 00:45:35,414 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742179_1355
2015-12-12 00:45:35,414 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742178_1354
2015-12-12 00:45:35,416 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742180_1356
2015-12-12 00:45:40,430 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742181_1357
2015-12-12 00:45:48,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742182_1358 src: /127.0.0.1:48876 dest: /127.0.0.1:50010
2015-12-12 00:46:01,498 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "ubuntu/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-12-12 00:46:05,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:06,440 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:07,440 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:08,441 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:09,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:10,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:11,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:12,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:13,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:14,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:14,474 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-12 00:46:14,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:48876, dest: /127.0.0.1:50010, bytes: 58574, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1372391359_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742182_1358, duration: 26560126610
2015-12-12 00:46:14,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742182_1358, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:46:15,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:16,504 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:17,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:18,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:19,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:20,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:20,520 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742182_1358
2015-12-12 00:46:21,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:22,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:23,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:24,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:24,523 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-12 00:46:25,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:26,525 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:27,526 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:28,528 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:29,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:30,530 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:31,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:32,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:33,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:34,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:34,534 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-12 00:46:35,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:36,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:37,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:38,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:39,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:40,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:41,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:42,543 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:43,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:44,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:44,545 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-12 00:46:45,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:46,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:47,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:48,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:49,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:50,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:51,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:52,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:53,554 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:54,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:54,556 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-12 00:46:55,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:56,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:57,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:58,560 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:46:59,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:00,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:01,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:02,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:03,566 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:04,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:04,568 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-12 00:47:05,570 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:06,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:07,572 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:08,573 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:09,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:10,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:11,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:12,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:13,578 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:14,578 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:14,579 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-12 00:47:15,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:16,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:17,582 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:18,583 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:19,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:20,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:21,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:22,587 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:23,588 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:24,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:24,591 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-12 00:47:25,594 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:26,594 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:27,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:28,596 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:29,597 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:30,597 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:31,598 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:32,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:33,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:34,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:34,602 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From ubuntu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 8 more
2015-12-12 00:47:35,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:36,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:37,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:38,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:47:38,635 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-12 00:47:38,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-12 00:48:30,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-12 00:48:30,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-12 00:48:31,440 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-12 00:48:31,502 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-12 00:48:31,503 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-12 00:48:31,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-12 00:48:31,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-12 00:48:31,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-12 00:48:31,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-12 00:48:31,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-12 00:48:31,642 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-12 00:48:31,645 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-12 00:48:31,654 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-12 00:48:31,656 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-12 00:48:31,656 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-12 00:48:31,656 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-12 00:48:31,677 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-12 00:48:31,690 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-12 00:48:31,690 INFO org.mortbay.log: jetty-6.1.26
2015-12-12 00:48:31,979 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-12 00:48:32,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-12 00:48:32,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-12 00:48:32,215 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-12 00:48:32,237 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-12 00:48:32,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-12 00:48:32,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-12 00:48:32,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-12 00:48:32,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-12 00:48:32,318 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-12 00:48:32,319 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-12 00:48:32,635 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-12 00:48:32,646 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 24131@ubuntu
2015-12-12 00:48:32,714 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1841603009-127.0.1.1-1449456040486
2015-12-12 00:48:32,714 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-12-12 00:48:32,715 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-12 00:48:32,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1379726899;bpid=BP-1841603009-127.0.1.1-1449456040486;lv=-56;nsInfo=lv=-60;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0;bpid=BP-1841603009-127.0.1.1-1449456040486;dnuuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-12 00:48:32,776 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /home/parallels/hadoop/tmp/dfs/data/current
2015-12-12 00:48:32,786 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/parallels/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-12 00:48:32,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-12 00:48:32,851 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1449917177851 with interval 21600000
2015-12-12 00:48:32,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-12 00:48:32,852 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-12 00:48:32,860 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current: 28348412
2015-12-12 00:48:32,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1841603009-127.0.1.1-1449456040486 on /home/parallels/hadoop/tmp/dfs/data/current: 13ms
2015-12-12 00:48:32,866 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1841603009-127.0.1.1-1449456040486: 14ms
2015-12-12 00:48:32,866 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-12 00:48:32,901 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current: 34ms
2015-12-12 00:48:32,901 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 36ms
2015-12-12 00:48:32,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-12 00:48:32,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-12 00:48:32,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-12 00:48:33,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=2878
2015-12-12 00:48:33,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000
2015-12-12 00:48:33,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 117 blocks total. Took 1 msec to generate and 115 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@5d48a43c
2015-12-12 00:48:33,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-12 00:48:33,160 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-12-12 00:48:33,160 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-12 00:48:33,161 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-12-12 00:48:33,161 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-12-12 00:48:33,162 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-12 00:48:33,167 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1841603009-127.0.1.1-1449456040486 to blockPoolScannerMap, new size=1
2015-12-12 00:49:24,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742183_1359 src: /127.0.0.1:49212 dest: /127.0.0.1:50010
2015-12-12 00:49:24,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49212, dest: /127.0.0.1:50010, bytes: 5835, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1006313544_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742183_1359, duration: 81757545
2015-12-12 00:49:24,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742183_1359, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:49:25,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742184_1360 src: /127.0.0.1:49213 dest: /127.0.0.1:50010
2015-12-12 00:49:25,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49213, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1006313544_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742184_1360, duration: 3241540
2015-12-12 00:49:25,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742184_1360, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:49:25,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742185_1361 src: /127.0.0.1:49214 dest: /127.0.0.1:50010
2015-12-12 00:49:25,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49214, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1006313544_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742185_1361, duration: 2134259
2015-12-12 00:49:25,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742185_1361, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:49:25,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742186_1362 src: /127.0.0.1:49215 dest: /127.0.0.1:50010
2015-12-12 00:49:25,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49215, dest: /127.0.0.1:50010, bytes: 87269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1006313544_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742186_1362, duration: 10815781
2015-12-12 00:49:25,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742186_1362, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:49:33,063 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742184_1360
2015-12-12 00:49:33,064 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742185_1361
2015-12-12 00:49:33,065 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742183_1359
2015-12-12 00:49:33,068 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742186_1362
2015-12-12 00:49:35,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742187_1363 src: /127.0.0.1:49224 dest: /127.0.0.1:50010
2015-12-12 00:49:35,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49224, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_125393537_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742187_1363, duration: 48978066
2015-12-12 00:49:35,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742187_1363, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:49:43,287 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742187_1363
2015-12-12 00:49:53,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742188_1364 src: /127.0.0.1:49243 dest: /127.0.0.1:50010
2015-12-12 00:50:03,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742189_1365 src: /127.0.0.1:49254 dest: /127.0.0.1:50010
2015-12-12 00:50:03,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49254, dest: /127.0.0.1:50010, bytes: 92, op: HDFS_WRITE, cliID: DFSClient_attempt_1449899324019_0001_r_000000_0_-1335832294_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742189_1365, duration: 63735335
2015-12-12 00:50:03,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742189_1365, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:50:04,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49243, dest: /127.0.0.1:50010, bytes: 48787, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_125393537_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742188_1364, duration: 10617753747
2015-12-12 00:50:04,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742188_1364, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:50:04,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742190_1366 src: /127.0.0.1:49257 dest: /127.0.0.1:50010
2015-12-12 00:50:04,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49257, dest: /127.0.0.1:50010, bytes: 354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_125393537_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742190_1366, duration: 1802378
2015-12-12 00:50:04,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742190_1366, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:50:04,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742191_1367 src: /127.0.0.1:49259 dest: /127.0.0.1:50010
2015-12-12 00:50:04,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49259, dest: /127.0.0.1:50010, bytes: 48787, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_125393537_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742191_1367, duration: 55174793
2015-12-12 00:50:04,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742191_1367, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:50:04,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742192_1368 src: /127.0.0.1:49260 dest: /127.0.0.1:50010
2015-12-12 00:50:04,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49260, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_125393537_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742192_1368, duration: 3167803
2015-12-12 00:50:04,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742192_1368, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 00:50:09,024 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742183_1359 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742183 for deletion
2015-12-12 00:50:09,035 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742184_1360 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742184 for deletion
2015-12-12 00:50:09,035 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742185_1361 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742185 for deletion
2015-12-12 00:50:09,036 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742186_1362 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742186 for deletion
2015-12-12 00:50:09,036 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742187_1363 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742187 for deletion
2015-12-12 00:50:09,036 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742188_1364 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742188 for deletion
2015-12-12 00:50:09,042 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742183_1359 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742183
2015-12-12 00:50:09,043 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742184_1360 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742184
2015-12-12 00:50:09,044 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742185_1361 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742185
2015-12-12 00:50:09,044 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742186_1362 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742186
2015-12-12 00:50:09,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742187_1363 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742187
2015-12-12 00:50:09,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742188_1364 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742188
2015-12-12 00:50:13,327 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742192_1368
2015-12-12 00:50:13,328 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742189_1365
2015-12-12 00:50:13,328 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742190_1366
2015-12-12 00:50:13,329 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742191_1367
2015-12-12 00:55:33,014 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "ubuntu/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-12-12 00:55:37,014 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-12 00:55:37,568 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-12 00:55:37,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-12 11:55:47,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-12 11:55:47,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-12 11:55:48,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-12 11:55:48,570 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-12 11:55:48,570 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-12 11:55:48,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-12 11:55:48,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-12 11:55:48,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-12 11:55:48,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-12 11:55:48,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-12 11:55:48,726 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-12 11:55:48,729 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-12 11:55:48,739 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-12 11:55:48,741 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-12 11:55:48,741 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-12 11:55:48,741 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-12 11:55:48,764 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-12 11:55:48,777 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-12 11:55:48,777 INFO org.mortbay.log: jetty-6.1.26
2015-12-12 11:55:49,059 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-12 11:55:49,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-12 11:55:49,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-12 11:55:49,631 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-12 11:55:49,648 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-12 11:55:49,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-12 11:55:49,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-12 11:55:49,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-12 11:55:49,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-12 11:55:49,744 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-12 11:55:49,744 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-12 11:55:50,072 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-12 11:55:50,108 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 27986@ubuntu
2015-12-12 11:55:50,223 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1841603009-127.0.1.1-1449456040486
2015-12-12 11:55:50,224 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-12-12 11:55:50,225 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-12 11:55:50,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1379726899;bpid=BP-1841603009-127.0.1.1-1449456040486;lv=-56;nsInfo=lv=-60;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0;bpid=BP-1841603009-127.0.1.1-1449456040486;dnuuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-12 11:55:50,320 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /home/parallels/hadoop/tmp/dfs/data/current
2015-12-12 11:55:50,322 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/parallels/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-12 11:55:50,390 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-12 11:55:50,393 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1449953191393 with interval 21600000
2015-12-12 11:55:50,394 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-12 11:55:50,395 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-12 11:55:50,433 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1841603009-127.0.1.1-1449456040486 on /home/parallels/hadoop/tmp/dfs/data/current: 38ms
2015-12-12 11:55:50,434 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1841603009-127.0.1.1-1449456040486: 40ms
2015-12-12 11:55:50,434 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-12 11:55:50,449 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current: 15ms
2015-12-12 11:55:50,450 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 16ms
2015-12-12 11:55:50,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-12 11:55:50,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-12 11:55:50,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-12 11:55:50,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=2962
2015-12-12 11:55:50,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000
2015-12-12 11:55:50,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 121 blocks total. Took 2 msec to generate and 76 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@78daafc0
2015-12-12 11:55:50,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-12 11:55:50,797 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-12-12 11:55:50,797 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-12 11:55:50,800 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-12-12 11:55:50,800 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-12-12 11:55:50,803 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-12 11:55:50,813 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1841603009-127.0.1.1-1449456040486 to blockPoolScannerMap, new size=1
2015-12-12 12:03:04,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742193_1369 src: /127.0.0.1:56023 dest: /127.0.0.1:50010
2015-12-12 12:03:04,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56023, dest: /127.0.0.1:50010, bytes: 5395, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988214434_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742193_1369, duration: 137220133
2015-12-12 12:03:04,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742193_1369, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:03:05,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742194_1370 src: /127.0.0.1:56024 dest: /127.0.0.1:50010
2015-12-12 12:03:05,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56024, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988214434_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742194_1370, duration: 3917030
2015-12-12 12:03:05,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742194_1370, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:03:05,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742195_1371 src: /127.0.0.1:56025 dest: /127.0.0.1:50010
2015-12-12 12:03:05,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56025, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988214434_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742195_1371, duration: 2157659
2015-12-12 12:03:05,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742195_1371, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:03:05,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742196_1372 src: /127.0.0.1:56026 dest: /127.0.0.1:50010
2015-12-12 12:03:05,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56026, dest: /127.0.0.1:50010, bytes: 87269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988214434_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742196_1372, duration: 14147882
2015-12-12 12:03:05,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742196_1372, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:03:10,604 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742194_1370
2015-12-12 12:03:10,605 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742195_1371
2015-12-12 12:03:10,606 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742193_1369
2015-12-12 12:03:15,676 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742196_1372
2015-12-12 12:03:15,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742197_1373 src: /127.0.0.1:56035 dest: /127.0.0.1:50010
2015-12-12 12:03:15,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56035, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-819830546_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742197_1373, duration: 42124577
2015-12-12 12:03:15,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742197_1373, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:03:26,049 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742197_1373
2015-12-12 12:03:31,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742198_1374 src: /127.0.0.1:56052 dest: /127.0.0.1:50010
2015-12-12 12:04:11,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56052, dest: /127.0.0.1:50010, bytes: 43276, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-819830546_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742198_1374, duration: 39295789644
2015-12-12 12:04:11,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742198_1374, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:04:11,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742199_1375 src: /127.0.0.1:56100 dest: /127.0.0.1:50010
2015-12-12 12:04:11,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56100, dest: /127.0.0.1:50010, bytes: 340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-819830546_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742199_1375, duration: 2025457
2015-12-12 12:04:11,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742199_1375, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:04:11,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742200_1376 src: /127.0.0.1:56102 dest: /127.0.0.1:50010
2015-12-12 12:04:11,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56102, dest: /127.0.0.1:50010, bytes: 43276, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-819830546_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742200_1376, duration: 3016383
2015-12-12 12:04:11,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742200_1376, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:04:11,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742201_1377 src: /127.0.0.1:56103 dest: /127.0.0.1:50010
2015-12-12 12:04:11,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56103, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-819830546_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742201_1377, duration: 4455231
2015-12-12 12:04:11,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742201_1377, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:04:14,693 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742193_1369 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742193 for deletion
2015-12-12 12:04:14,701 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742194_1370 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742194 for deletion
2015-12-12 12:04:14,702 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742195_1371 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742195 for deletion
2015-12-12 12:04:14,702 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742196_1372 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742196 for deletion
2015-12-12 12:04:14,702 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742193_1369 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742193
2015-12-12 12:04:14,702 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742197_1373 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742197 for deletion
2015-12-12 12:04:14,703 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742198_1374 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742198 for deletion
2015-12-12 12:04:14,703 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742194_1370 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742194
2015-12-12 12:04:14,703 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742195_1371 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742195
2015-12-12 12:04:14,703 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742196_1372 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742196
2015-12-12 12:04:14,704 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742197_1373 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742197
2015-12-12 12:04:14,705 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742198_1374 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742198
2015-12-12 12:04:21,170 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742200_1376
2015-12-12 12:04:21,173 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742201_1377
2015-12-12 12:04:21,174 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742199_1375
2015-12-12 12:12:28,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742202_1378 src: /127.0.0.1:56123 dest: /127.0.0.1:50010
2015-12-12 12:12:28,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56123, dest: /127.0.0.1:50010, bytes: 5395, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_82792457_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742202_1378, duration: 69510446
2015-12-12 12:12:28,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742202_1378, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:12:28,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742203_1379 src: /127.0.0.1:56124 dest: /127.0.0.1:50010
2015-12-12 12:12:28,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56124, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_82792457_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742203_1379, duration: 2192044
2015-12-12 12:12:28,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742203_1379, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:12:28,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742204_1380 src: /127.0.0.1:56125 dest: /127.0.0.1:50010
2015-12-12 12:12:28,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56125, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_82792457_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742204_1380, duration: 1932231
2015-12-12 12:12:28,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742204_1380, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:12:28,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742205_1381 src: /127.0.0.1:56126 dest: /127.0.0.1:50010
2015-12-12 12:12:28,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56126, dest: /127.0.0.1:50010, bytes: 87269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_82792457_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742205_1381, duration: 23760598
2015-12-12 12:12:28,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742205_1381, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:12:36,413 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742202_1378
2015-12-12 12:12:36,436 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742203_1379
2015-12-12 12:12:36,440 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742205_1381
2015-12-12 12:12:36,442 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742204_1380
2015-12-12 12:12:37,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742206_1382 src: /127.0.0.1:56134 dest: /127.0.0.1:50010
2015-12-12 12:12:37,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56134, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2030259423_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742206_1382, duration: 33907328
2015-12-12 12:12:37,182 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742206_1382, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:12:46,854 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742206_1382
2015-12-12 12:12:55,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742207_1383 src: /127.0.0.1:56153 dest: /127.0.0.1:50010
2015-12-12 12:13:33,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56153, dest: /127.0.0.1:50010, bytes: 42118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2030259423_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742207_1383, duration: 37771737612
2015-12-12 12:13:33,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742207_1383, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:13:33,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742208_1384 src: /127.0.0.1:56202 dest: /127.0.0.1:50010
2015-12-12 12:13:33,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56202, dest: /127.0.0.1:50010, bytes: 340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2030259423_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742208_1384, duration: 1947441
2015-12-12 12:13:33,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742208_1384, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:13:33,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742209_1385 src: /127.0.0.1:56204 dest: /127.0.0.1:50010
2015-12-12 12:13:33,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56204, dest: /127.0.0.1:50010, bytes: 42118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2030259423_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742209_1385, duration: 2027500
2015-12-12 12:13:33,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742209_1385, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:13:33,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742210_1386 src: /127.0.0.1:56205 dest: /127.0.0.1:50010
2015-12-12 12:13:33,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56205, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2030259423_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742210_1386, duration: 3333489
2015-12-12 12:13:33,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742210_1386, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:13:35,690 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742202_1378 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742202 for deletion
2015-12-12 12:13:35,693 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742203_1379 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742203 for deletion
2015-12-12 12:13:35,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742204_1380 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742204 for deletion
2015-12-12 12:13:35,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742202_1378 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742202
2015-12-12 12:13:35,694 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742205_1381 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742205 for deletion
2015-12-12 12:13:35,695 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742206_1382 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742206 for deletion
2015-12-12 12:13:35,695 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742207_1383 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742207 for deletion
2015-12-12 12:13:35,696 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742203_1379 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742203
2015-12-12 12:13:35,697 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742204_1380 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742204
2015-12-12 12:13:35,698 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742205_1381 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742205
2015-12-12 12:13:35,699 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742206_1382 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742206
2015-12-12 12:13:35,699 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742207_1383 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742207
2015-12-12 12:13:41,911 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742210_1386
2015-12-12 12:13:41,911 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742208_1384
2015-12-12 12:13:41,913 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742209_1385
2015-12-12 12:20:06,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742211_1387 src: /127.0.0.1:56223 dest: /127.0.0.1:50010
2015-12-12 12:20:06,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56223, dest: /127.0.0.1:50010, bytes: 5398, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_695849014_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742211_1387, duration: 48788093
2015-12-12 12:20:06,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742211_1387, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:20:06,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742212_1388 src: /127.0.0.1:56224 dest: /127.0.0.1:50010
2015-12-12 12:20:06,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56224, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_695849014_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742212_1388, duration: 2547612
2015-12-12 12:20:06,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742212_1388, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:20:06,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742213_1389 src: /127.0.0.1:56225 dest: /127.0.0.1:50010
2015-12-12 12:20:06,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56225, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_695849014_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742213_1389, duration: 2130247
2015-12-12 12:20:06,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742213_1389, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:20:06,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742214_1390 src: /127.0.0.1:56226 dest: /127.0.0.1:50010
2015-12-12 12:20:06,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56226, dest: /127.0.0.1:50010, bytes: 87269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_695849014_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742214_1390, duration: 11917250
2015-12-12 12:20:06,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742214_1390, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:20:12,008 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742213_1389
2015-12-12 12:20:12,009 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742212_1388
2015-12-12 12:20:12,009 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742211_1387
2015-12-12 12:20:12,012 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742214_1390
2015-12-12 12:20:13,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742215_1391 src: /127.0.0.1:56234 dest: /127.0.0.1:50010
2015-12-12 12:20:13,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56234, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-731835732_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742215_1391, duration: 45789648
2015-12-12 12:20:13,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742215_1391, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:20:22,088 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742215_1391
2015-12-12 12:20:32,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742216_1392 src: /127.0.0.1:56253 dest: /127.0.0.1:50010
2015-12-12 12:21:27,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56253, dest: /127.0.0.1:50010, bytes: 42255, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-731835732_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742216_1392, duration: 55095682003
2015-12-12 12:21:27,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742216_1392, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:21:27,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742217_1393 src: /127.0.0.1:56309 dest: /127.0.0.1:50010
2015-12-12 12:21:27,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56309, dest: /127.0.0.1:50010, bytes: 340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-731835732_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742217_1393, duration: 4733115
2015-12-12 12:21:27,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742217_1393, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:21:28,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742218_1394 src: /127.0.0.1:56311 dest: /127.0.0.1:50010
2015-12-12 12:21:28,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56311, dest: /127.0.0.1:50010, bytes: 42255, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-731835732_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742218_1394, duration: 5631037
2015-12-12 12:21:28,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742218_1394, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:21:28,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742219_1395 src: /127.0.0.1:56312 dest: /127.0.0.1:50010
2015-12-12 12:21:28,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56312, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-731835732_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742219_1395, duration: 12144996
2015-12-12 12:21:28,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742219_1395, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:21:32,761 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742211_1387 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742211 for deletion
2015-12-12 12:21:32,765 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742212_1388 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742212 for deletion
2015-12-12 12:21:32,765 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742213_1389 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742213 for deletion
2015-12-12 12:21:32,765 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742214_1390 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742214 for deletion
2015-12-12 12:21:32,765 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742215_1391 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742215 for deletion
2015-12-12 12:21:32,765 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742216_1392 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742216 for deletion
2015-12-12 12:21:32,766 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742211_1387 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742211
2015-12-12 12:21:32,766 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742212_1388 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742212
2015-12-12 12:21:32,766 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742213_1389 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742213
2015-12-12 12:21:32,767 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742214_1390 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742214
2015-12-12 12:21:32,767 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742215_1391 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742215
2015-12-12 12:21:32,768 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742216_1392 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742216
2015-12-12 12:21:37,514 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742218_1394
2015-12-12 12:21:37,521 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742219_1395
2015-12-12 12:21:37,523 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742217_1393
2015-12-12 12:23:00,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742220_1396 src: /127.0.0.1:56336 dest: /127.0.0.1:50010
2015-12-12 12:23:00,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56336, dest: /127.0.0.1:50010, bytes: 5398, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_169392428_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742220_1396, duration: 58090509
2015-12-12 12:23:00,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742220_1396, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:23:00,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742221_1397 src: /127.0.0.1:56337 dest: /127.0.0.1:50010
2015-12-12 12:23:00,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56337, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_169392428_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742221_1397, duration: 5535008
2015-12-12 12:23:00,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742221_1397, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:23:00,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742222_1398 src: /127.0.0.1:56338 dest: /127.0.0.1:50010
2015-12-12 12:23:00,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56338, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_169392428_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742222_1398, duration: 1711062
2015-12-12 12:23:00,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742222_1398, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:23:01,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742223_1399 src: /127.0.0.1:56339 dest: /127.0.0.1:50010
2015-12-12 12:23:01,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56339, dest: /127.0.0.1:50010, bytes: 87269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_169392428_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742223_1399, duration: 16227634
2015-12-12 12:23:01,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742223_1399, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:23:07,635 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742220_1396
2015-12-12 12:23:07,640 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742221_1397
2015-12-12 12:23:07,641 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742222_1398
2015-12-12 12:23:07,642 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742223_1399
2015-12-12 12:23:07,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742224_1400 src: /127.0.0.1:56348 dest: /127.0.0.1:50010
2015-12-12 12:23:07,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56348, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_630512287_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742224_1400, duration: 77125804
2015-12-12 12:23:07,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742224_1400, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:23:17,767 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742224_1400
2015-12-12 12:23:23,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742225_1401 src: /127.0.0.1:56367 dest: /127.0.0.1:50010
2015-12-12 12:23:59,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56367, dest: /127.0.0.1:50010, bytes: 42269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_630512287_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742225_1401, duration: 35957071980
2015-12-12 12:23:59,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742225_1401, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:23:59,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742226_1402 src: /127.0.0.1:56416 dest: /127.0.0.1:50010
2015-12-12 12:23:59,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56416, dest: /127.0.0.1:50010, bytes: 340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_630512287_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742226_1402, duration: 5822434
2015-12-12 12:23:59,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742226_1402, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:23:59,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742227_1403 src: /127.0.0.1:56418 dest: /127.0.0.1:50010
2015-12-12 12:23:59,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56418, dest: /127.0.0.1:50010, bytes: 42269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_630512287_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742227_1403, duration: 3843032
2015-12-12 12:23:59,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742227_1403, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:23:59,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742228_1404 src: /127.0.0.1:56419 dest: /127.0.0.1:50010
2015-12-12 12:23:59,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56419, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_630512287_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742228_1404, duration: 7901342
2015-12-12 12:23:59,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742228_1404, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:24:02,784 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742224_1400 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742224 for deletion
2015-12-12 12:24:02,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742225_1401 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742225 for deletion
2015-12-12 12:24:02,787 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742220_1396 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742220 for deletion
2015-12-12 12:24:02,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742221_1397 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742221 for deletion
2015-12-12 12:24:02,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742222_1398 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742222 for deletion
2015-12-12 12:24:02,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742223_1399 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742223 for deletion
2015-12-12 12:24:02,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742224_1400 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742224
2015-12-12 12:24:02,791 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742225_1401 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742225
2015-12-12 12:24:02,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742220_1396 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742220
2015-12-12 12:24:02,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742221_1397 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742221
2015-12-12 12:24:02,792 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742222_1398 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742222
2015-12-12 12:24:02,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742223_1399 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742223
2015-12-12 12:24:07,868 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742226_1402
2015-12-12 12:24:07,870 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742227_1403
2015-12-12 12:24:07,873 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742228_1404
2015-12-12 12:25:57,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742229_1405 src: /127.0.0.1:56432 dest: /127.0.0.1:50010
2015-12-12 12:25:57,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56432, dest: /127.0.0.1:50010, bytes: 5409, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1764368169_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742229_1405, duration: 55435130
2015-12-12 12:25:57,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742229_1405, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:25:57,973 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742230_1406 src: /127.0.0.1:56433 dest: /127.0.0.1:50010
2015-12-12 12:25:57,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56433, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1764368169_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742230_1406, duration: 5948250
2015-12-12 12:25:57,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742230_1406, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:25:58,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742231_1407 src: /127.0.0.1:56434 dest: /127.0.0.1:50010
2015-12-12 12:25:58,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56434, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1764368169_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742231_1407, duration: 2365565
2015-12-12 12:25:58,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742231_1407, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:25:58,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742232_1408 src: /127.0.0.1:56435 dest: /127.0.0.1:50010
2015-12-12 12:25:58,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56435, dest: /127.0.0.1:50010, bytes: 87269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1764368169_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742232_1408, duration: 7755595
2015-12-12 12:25:58,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742232_1408, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:26:02,903 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742229_1405
2015-12-12 12:26:04,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742233_1409 src: /127.0.0.1:56444 dest: /127.0.0.1:50010
2015-12-12 12:26:04,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56444, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_102225082_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742233_1409, duration: 104782395
2015-12-12 12:26:04,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742233_1409, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:26:07,921 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742231_1407
2015-12-12 12:26:07,923 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742232_1408
2015-12-12 12:26:07,923 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742230_1406
2015-12-12 12:26:12,939 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742233_1409
2015-12-12 12:26:18,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742234_1410 src: /127.0.0.1:56462 dest: /127.0.0.1:50010
2015-12-12 12:26:49,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56462, dest: /127.0.0.1:50010, bytes: 39847, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_102225082_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742234_1410, duration: 31186751980
2015-12-12 12:26:49,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742234_1410, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:26:49,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742235_1411 src: /127.0.0.1:56506 dest: /127.0.0.1:50010
2015-12-12 12:26:49,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56506, dest: /127.0.0.1:50010, bytes: 340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_102225082_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742235_1411, duration: 2055373
2015-12-12 12:26:49,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742235_1411, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:26:49,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742236_1412 src: /127.0.0.1:56508 dest: /127.0.0.1:50010
2015-12-12 12:26:49,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56508, dest: /127.0.0.1:50010, bytes: 39847, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_102225082_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742236_1412, duration: 6748130
2015-12-12 12:26:49,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742236_1412, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:26:49,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742237_1413 src: /127.0.0.1:56509 dest: /127.0.0.1:50010
2015-12-12 12:26:49,870 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56509, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_102225082_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742237_1413, duration: 5276507
2015-12-12 12:26:49,870 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742237_1413, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 12:26:53,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742229_1405 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742229 for deletion
2015-12-12 12:26:53,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742230_1406 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742230 for deletion
2015-12-12 12:26:53,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742229_1405 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742229
2015-12-12 12:26:53,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742231_1407 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742231 for deletion
2015-12-12 12:26:53,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742230_1406 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742230
2015-12-12 12:26:53,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742232_1408 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742232 for deletion
2015-12-12 12:26:53,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742231_1407 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742231
2015-12-12 12:26:53,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742233_1409 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742233 for deletion
2015-12-12 12:26:53,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742232_1408 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742232
2015-12-12 12:26:53,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742234_1410 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742234 for deletion
2015-12-12 12:26:53,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742233_1409 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742233
2015-12-12 12:26:53,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742234_1410 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742234
2015-12-12 12:26:58,004 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742236_1412
2015-12-12 12:26:58,007 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742237_1413
2015-12-12 12:26:58,007 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742235_1411
2015-12-12 14:43:50,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: NameNode at localhost/127.0.0.1:9000 calls recoverBlock(BP-1841603009-127.0.1.1-1449456040486:blk_1073742182_1358, targets=[127.0.0.1:50010], newGenerationStamp=1414)
2015-12-12 14:43:50,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: initReplicaRecovery: blk_1073742182_1358, recoveryId=1414, replica=FinalizedReplica, blk_1073742182_1358, FINALIZED
  getNumBytes()     = 58574
  getBytesOnDisk()  = 58574
  getVisibleLength()= 58574
  getVolume()       = /home/parallels/hadoop/tmp/dfs/data/current
  getBlockFile()    = /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742182
  unlinked          =false
2015-12-12 14:43:50,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: initReplicaRecovery: changing replica state for blk_1073742182_1358 from FINALIZED to RUR
2015-12-12 14:43:50,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: updateReplica: BP-1841603009-127.0.1.1-1449456040486:blk_1073742182_1358, recoveryId=1414, length=58574, replica=ReplicaUnderRecovery, blk_1073742182_1358, RUR
  getNumBytes()     = 58574
  getBytesOnDisk()  = 58574
  getVisibleLength()= 58574
  getVolume()       = /home/parallels/hadoop/tmp/dfs/data/current
  getBlockFile()    = /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742182
  recoveryId=1414
  original=FinalizedReplica, blk_1073742182_1358, FINALIZED
  getNumBytes()     = 58574
  getBytesOnDisk()  = 58574
  getVisibleLength()= 58574
  getVolume()       = /home/parallels/hadoop/tmp/dfs/data/current
  getBlockFile()    = /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742182
  unlinked          =false
2015-12-12 14:43:50,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 136 blocks total. Took 0 msec to generate and 20 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@900d6e4
2015-12-12 14:43:50,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-12 15:53:47,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742238_1415 src: /127.0.0.1:56564 dest: /127.0.0.1:50010
2015-12-12 15:53:47,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56564, dest: /127.0.0.1:50010, bytes: 5406, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1859945875_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742238_1415, duration: 36502627
2015-12-12 15:53:47,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742238_1415, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 15:53:47,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742239_1416 src: /127.0.0.1:56565 dest: /127.0.0.1:50010
2015-12-12 15:53:47,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56565, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1859945875_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742239_1416, duration: 1937510
2015-12-12 15:53:47,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742239_1416, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 15:53:47,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742240_1417 src: /127.0.0.1:56566 dest: /127.0.0.1:50010
2015-12-12 15:53:47,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56566, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1859945875_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742240_1417, duration: 1711777
2015-12-12 15:53:47,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742240_1417, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 15:53:47,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742241_1418 src: /127.0.0.1:56567 dest: /127.0.0.1:50010
2015-12-12 15:53:47,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56567, dest: /127.0.0.1:50010, bytes: 87269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1859945875_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742241_1418, duration: 18004278
2015-12-12 15:53:47,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742241_1418, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 15:53:52,724 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742240_1417
2015-12-12 15:53:52,725 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742239_1416
2015-12-12 15:53:52,726 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742238_1415
2015-12-12 15:53:54,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742242_1419 src: /127.0.0.1:56576 dest: /127.0.0.1:50010
2015-12-12 15:53:55,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56576, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_517446540_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742242_1419, duration: 72225145
2015-12-12 15:53:55,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742242_1419, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 15:53:57,789 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742241_1418
2015-12-12 15:54:02,798 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742242_1419
2015-12-12 15:54:09,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742243_1420 src: /127.0.0.1:56593 dest: /127.0.0.1:50010
2015-12-12 15:54:41,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56593, dest: /127.0.0.1:50010, bytes: 40201, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_517446540_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742243_1420, duration: 32197515756
2015-12-12 15:54:41,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742243_1420, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 15:54:41,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742244_1421 src: /127.0.0.1:56638 dest: /127.0.0.1:50010
2015-12-12 15:54:41,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56638, dest: /127.0.0.1:50010, bytes: 340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_517446540_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742244_1421, duration: 6320378
2015-12-12 15:54:41,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742244_1421, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 15:54:41,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742245_1422 src: /127.0.0.1:56640 dest: /127.0.0.1:50010
2015-12-12 15:54:41,690 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56640, dest: /127.0.0.1:50010, bytes: 40201, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_517446540_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742245_1422, duration: 4180290
2015-12-12 15:54:41,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742245_1422, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 15:54:41,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742246_1423 src: /127.0.0.1:56641 dest: /127.0.0.1:50010
2015-12-12 15:54:41,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56641, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_517446540_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742246_1423, duration: 6377433
2015-12-12 15:54:41,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742246_1423, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 15:54:45,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742240_1417 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742240 for deletion
2015-12-12 15:54:45,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742241_1418 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742241 for deletion
2015-12-12 15:54:45,677 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742242_1419 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742242 for deletion
2015-12-12 15:54:45,677 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742243_1420 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742243 for deletion
2015-12-12 15:54:45,678 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742238_1415 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742238 for deletion
2015-12-12 15:54:45,678 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742239_1416 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742239 for deletion
2015-12-12 15:54:45,678 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742240_1417 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742240
2015-12-12 15:54:45,678 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742241_1418 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742241
2015-12-12 15:54:45,678 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742242_1419 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742242
2015-12-12 15:54:45,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742243_1420 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742243
2015-12-12 15:54:45,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742238_1415 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742238
2015-12-12 15:54:45,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742239_1416 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742239
2015-12-12 15:54:47,997 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742244_1421
2015-12-12 15:54:47,998 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742245_1422
2015-12-12 15:54:47,999 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742246_1423
2015-12-12 15:59:40,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742247_1424 src: /127.0.0.1:56654 dest: /127.0.0.1:50010
2015-12-12 15:59:40,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56654, dest: /127.0.0.1:50010, bytes: 5432, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1412005870_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742247_1424, duration: 48582742
2015-12-12 15:59:40,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742247_1424, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 15:59:40,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742248_1425 src: /127.0.0.1:56655 dest: /127.0.0.1:50010
2015-12-12 15:59:40,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56655, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1412005870_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742248_1425, duration: 2282817
2015-12-12 15:59:40,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742248_1425, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 15:59:40,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742249_1426 src: /127.0.0.1:56656 dest: /127.0.0.1:50010
2015-12-12 15:59:40,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56656, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1412005870_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742249_1426, duration: 1596177
2015-12-12 15:59:40,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742249_1426, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 15:59:40,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742250_1427 src: /127.0.0.1:56657 dest: /127.0.0.1:50010
2015-12-12 15:59:40,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56657, dest: /127.0.0.1:50010, bytes: 87269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1412005870_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742250_1427, duration: 18818770
2015-12-12 15:59:40,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742250_1427, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 15:59:46,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742251_1428 src: /127.0.0.1:56666 dest: /127.0.0.1:50010
2015-12-12 15:59:46,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56666, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-402241059_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742251_1428, duration: 58540403
2015-12-12 15:59:46,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742251_1428, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 15:59:48,040 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742247_1424
2015-12-12 15:59:48,041 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742249_1426
2015-12-12 15:59:48,041 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742248_1425
2015-12-12 15:59:48,042 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742250_1427
2015-12-12 15:59:53,069 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742251_1428
2015-12-12 16:00:03,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742252_1429 src: /127.0.0.1:56686 dest: /127.0.0.1:50010
2015-12-12 16:00:13,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56686, dest: /127.0.0.1:50010, bytes: 48502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-402241059_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742252_1429, duration: 10205747737
2015-12-12 16:00:13,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742252_1429, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 16:00:13,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742253_1430 src: /127.0.0.1:56699 dest: /127.0.0.1:50010
2015-12-12 16:00:13,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56699, dest: /127.0.0.1:50010, bytes: 354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-402241059_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742253_1430, duration: 1839328
2015-12-12 16:00:13,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742253_1430, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 16:00:13,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742254_1431 src: /127.0.0.1:56701 dest: /127.0.0.1:50010
2015-12-12 16:00:13,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56701, dest: /127.0.0.1:50010, bytes: 48502, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-402241059_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742254_1431, duration: 2918230
2015-12-12 16:00:13,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742254_1431, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 16:00:13,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742255_1432 src: /127.0.0.1:56702 dest: /127.0.0.1:50010
2015-12-12 16:00:13,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56702, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-402241059_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742255_1432, duration: 5844436
2015-12-12 16:00:13,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742255_1432, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 16:00:15,731 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742247_1424 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742247 for deletion
2015-12-12 16:00:15,735 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742248_1425 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742248 for deletion
2015-12-12 16:00:15,736 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742249_1426 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742249 for deletion
2015-12-12 16:00:15,736 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742250_1427 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742250 for deletion
2015-12-12 16:00:15,736 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742247_1424 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742247
2015-12-12 16:00:15,736 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742251_1428 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742251 for deletion
2015-12-12 16:00:15,736 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742248_1425 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742248
2015-12-12 16:00:15,736 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742252_1429 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742252 for deletion
2015-12-12 16:00:15,738 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742249_1426 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742249
2015-12-12 16:00:15,738 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742250_1427 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742250
2015-12-12 16:00:15,739 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742251_1428 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742251
2015-12-12 16:00:15,739 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742252_1429 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742252
2015-12-12 16:00:23,199 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742253_1430
2015-12-12 16:00:23,200 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742254_1431
2015-12-12 16:00:23,202 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742255_1432
2015-12-12 16:38:56,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742256_1433 src: /127.0.0.1:56752 dest: /127.0.0.1:50010
2015-12-12 16:38:56,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56752, dest: /127.0.0.1:50010, bytes: 5455, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_899392274_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742256_1433, duration: 45887343
2015-12-12 16:38:56,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742256_1433, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 16:38:56,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742257_1434 src: /127.0.0.1:56753 dest: /127.0.0.1:50010
2015-12-12 16:38:56,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56753, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_899392274_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742257_1434, duration: 1913283
2015-12-12 16:38:56,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742257_1434, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 16:38:56,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742258_1435 src: /127.0.0.1:56754 dest: /127.0.0.1:50010
2015-12-12 16:38:56,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56754, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_899392274_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742258_1435, duration: 1877137
2015-12-12 16:38:56,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742258_1435, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 16:38:56,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742259_1436 src: /127.0.0.1:56755 dest: /127.0.0.1:50010
2015-12-12 16:38:56,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56755, dest: /127.0.0.1:50010, bytes: 87269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_899392274_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742259_1436, duration: 9887926
2015-12-12 16:38:56,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742259_1436, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 16:39:03,503 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742258_1435
2015-12-12 16:39:03,504 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742257_1434
2015-12-12 16:39:03,505 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742259_1436
2015-12-12 16:39:03,506 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742256_1433
2015-12-12 16:39:03,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742260_1437 src: /127.0.0.1:56764 dest: /127.0.0.1:50010
2015-12-12 16:39:03,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56764, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1937711158_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742260_1437, duration: 42266694
2015-12-12 16:39:03,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742260_1437, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 16:39:13,720 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742260_1437
2015-12-12 16:39:19,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742261_1438 src: /127.0.0.1:56782 dest: /127.0.0.1:50010
2015-12-12 16:39:27,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56782, dest: /127.0.0.1:50010, bytes: 46835, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1937711158_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742261_1438, duration: 7528312924
2015-12-12 16:39:27,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742261_1438, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 16:39:27,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742262_1439 src: /127.0.0.1:56788 dest: /127.0.0.1:50010
2015-12-12 16:39:27,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56788, dest: /127.0.0.1:50010, bytes: 354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1937711158_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742262_1439, duration: 1951606
2015-12-12 16:39:27,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742262_1439, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 16:39:27,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742263_1440 src: /127.0.0.1:56790 dest: /127.0.0.1:50010
2015-12-12 16:39:27,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56790, dest: /127.0.0.1:50010, bytes: 46835, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1937711158_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742263_1440, duration: 5439914
2015-12-12 16:39:27,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742263_1440, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 16:39:27,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742264_1441 src: /127.0.0.1:56791 dest: /127.0.0.1:50010
2015-12-12 16:39:27,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56791, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1937711158_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742264_1441, duration: 10658820
2015-12-12 16:39:27,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742264_1441, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 16:39:33,838 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742264_1441
2015-12-12 16:39:33,839 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742261_1438
2015-12-12 16:39:33,842 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742263_1440
2015-12-12 16:39:33,843 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742262_1439
2015-12-12 16:39:34,068 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742256_1433 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742256 for deletion
2015-12-12 16:39:34,070 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742257_1434 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742257 for deletion
2015-12-12 16:39:34,071 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742258_1435 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742258 for deletion
2015-12-12 16:39:34,071 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742259_1436 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742259 for deletion
2015-12-12 16:39:34,071 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742260_1437 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742260 for deletion
2015-12-12 16:39:34,072 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742261_1438 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742261 for deletion
2015-12-12 16:39:34,073 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742256_1433 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742256
2015-12-12 16:39:34,074 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742257_1434 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742257
2015-12-12 16:39:34,075 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742258_1435 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742258
2015-12-12 16:39:34,075 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742259_1436 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742259
2015-12-12 16:39:34,075 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742260_1437 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742260
2015-12-12 16:39:34,075 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742261_1438 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742261
2015-12-12 17:03:19,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742265_1442 src: /127.0.0.1:56827 dest: /127.0.0.1:50010
2015-12-12 17:03:19,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56827, dest: /127.0.0.1:50010, bytes: 5416, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_259439563_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742265_1442, duration: 63451851
2015-12-12 17:03:19,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742265_1442, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:03:19,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742266_1443 src: /127.0.0.1:56828 dest: /127.0.0.1:50010
2015-12-12 17:03:19,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56828, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_259439563_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742266_1443, duration: 3835652
2015-12-12 17:03:19,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742266_1443, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:03:19,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742267_1444 src: /127.0.0.1:56829 dest: /127.0.0.1:50010
2015-12-12 17:03:19,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56829, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_259439563_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742267_1444, duration: 2044422
2015-12-12 17:03:19,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742267_1444, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:03:20,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742268_1445 src: /127.0.0.1:56830 dest: /127.0.0.1:50010
2015-12-12 17:03:20,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56830, dest: /127.0.0.1:50010, bytes: 87138, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_259439563_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742268_1445, duration: 15722538
2015-12-12 17:03:20,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742268_1445, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:03:27,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742269_1446 src: /127.0.0.1:56839 dest: /127.0.0.1:50010
2015-12-12 17:03:27,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56839, dest: /127.0.0.1:50010, bytes: 103660, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_308101612_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742269_1446, duration: 35513448
2015-12-12 17:03:27,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742269_1446, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:03:29,010 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742268_1445
2015-12-12 17:03:29,011 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742266_1443
2015-12-12 17:03:29,011 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742265_1442
2015-12-12 17:03:29,012 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742267_1444
2015-12-12 17:03:34,036 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742269_1446
2015-12-12 17:03:44,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742270_1447 src: /127.0.0.1:56857 dest: /127.0.0.1:50010
2015-12-12 17:03:50,973 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56857, dest: /127.0.0.1:50010, bytes: 46252, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_308101612_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742270_1447, duration: 6935192723
2015-12-12 17:03:50,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742270_1447, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:03:50,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742271_1448 src: /127.0.0.1:56865 dest: /127.0.0.1:50010
2015-12-12 17:03:51,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56865, dest: /127.0.0.1:50010, bytes: 354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_308101612_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742271_1448, duration: 9413489
2015-12-12 17:03:51,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742271_1448, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:03:51,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742272_1449 src: /127.0.0.1:56867 dest: /127.0.0.1:50010
2015-12-12 17:03:51,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56867, dest: /127.0.0.1:50010, bytes: 46252, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_308101612_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742272_1449, duration: 4158919
2015-12-12 17:03:51,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742272_1449, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:03:51,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742273_1450 src: /127.0.0.1:56868 dest: /127.0.0.1:50010
2015-12-12 17:03:51,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56868, dest: /127.0.0.1:50010, bytes: 103660, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_308101612_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742273_1450, duration: 7913080
2015-12-12 17:03:51,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742273_1450, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:03:55,239 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742265_1442 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742265 for deletion
2015-12-12 17:03:55,244 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742266_1443 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742266 for deletion
2015-12-12 17:03:55,244 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742267_1444 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742267 for deletion
2015-12-12 17:03:55,244 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742268_1445 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742268 for deletion
2015-12-12 17:03:55,245 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742269_1446 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742269 for deletion
2015-12-12 17:03:55,245 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742270_1447 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742270 for deletion
2015-12-12 17:03:55,245 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742265_1442 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742265
2015-12-12 17:03:55,246 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742266_1443 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742266
2015-12-12 17:03:55,246 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742267_1444 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742267
2015-12-12 17:03:55,246 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742268_1445 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742268
2015-12-12 17:03:55,246 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742269_1446 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742269
2015-12-12 17:03:55,247 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742270_1447 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742270
2015-12-12 17:03:59,137 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742271_1448
2015-12-12 17:03:59,137 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742272_1449
2015-12-12 17:03:59,139 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742273_1450
2015-12-12 17:11:45,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742274_1451 src: /127.0.0.1:56885 dest: /127.0.0.1:50010
2015-12-12 17:11:45,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56885, dest: /127.0.0.1:50010, bytes: 5469, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2022732746_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742274_1451, duration: 59404719
2015-12-12 17:11:45,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742274_1451, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:11:45,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742275_1452 src: /127.0.0.1:56886 dest: /127.0.0.1:50010
2015-12-12 17:11:45,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56886, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2022732746_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742275_1452, duration: 4191661
2015-12-12 17:11:45,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742275_1452, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:11:45,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742276_1453 src: /127.0.0.1:56887 dest: /127.0.0.1:50010
2015-12-12 17:11:45,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56887, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2022732746_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742276_1453, duration: 2077662
2015-12-12 17:11:45,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742276_1453, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:11:45,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742277_1454 src: /127.0.0.1:56888 dest: /127.0.0.1:50010
2015-12-12 17:11:45,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56888, dest: /127.0.0.1:50010, bytes: 87269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2022732746_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742277_1454, duration: 13444656
2015-12-12 17:11:45,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742277_1454, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:11:52,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742278_1455 src: /127.0.0.1:56897 dest: /127.0.0.1:50010
2015-12-12 17:11:52,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56897, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1955253105_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742278_1455, duration: 41954698
2015-12-12 17:11:52,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742278_1455, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:11:54,209 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742275_1452
2015-12-12 17:11:54,210 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742276_1453
2015-12-12 17:11:54,210 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742274_1451
2015-12-12 17:11:54,211 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742277_1454
2015-12-12 17:11:59,224 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742278_1455
2015-12-12 17:12:08,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742279_1456 src: /127.0.0.1:56914 dest: /127.0.0.1:50010
2015-12-12 17:12:15,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56914, dest: /127.0.0.1:50010, bytes: 46832, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1955253105_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742279_1456, duration: 6658675478
2015-12-12 17:12:15,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742279_1456, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:12:15,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742280_1457 src: /127.0.0.1:56921 dest: /127.0.0.1:50010
2015-12-12 17:12:15,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56921, dest: /127.0.0.1:50010, bytes: 354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1955253105_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742280_1457, duration: 2317302
2015-12-12 17:12:15,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742280_1457, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:12:15,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742281_1458 src: /127.0.0.1:56923 dest: /127.0.0.1:50010
2015-12-12 17:12:15,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56923, dest: /127.0.0.1:50010, bytes: 46832, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1955253105_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742281_1458, duration: 8020461
2015-12-12 17:12:15,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742281_1458, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:12:15,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742282_1459 src: /127.0.0.1:56924 dest: /127.0.0.1:50010
2015-12-12 17:12:15,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:56924, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1955253105_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742282_1459, duration: 6205814
2015-12-12 17:12:15,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742282_1459, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 17:12:19,308 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742274_1451 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742274 for deletion
2015-12-12 17:12:19,311 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742275_1452 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742275 for deletion
2015-12-12 17:12:19,312 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742276_1453 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742276 for deletion
2015-12-12 17:12:19,313 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742277_1454 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742277 for deletion
2015-12-12 17:12:19,313 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742278_1455 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742278 for deletion
2015-12-12 17:12:19,313 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742279_1456 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742279 for deletion
2015-12-12 17:12:19,390 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742274_1451 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742274
2015-12-12 17:12:19,392 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742275_1452 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742275
2015-12-12 17:12:19,392 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742276_1453 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742276
2015-12-12 17:12:19,392 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742277_1454 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742277
2015-12-12 17:12:19,393 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742278_1455 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742278
2015-12-12 17:12:19,393 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742279_1456 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742279
2015-12-12 17:12:24,244 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742282_1459
2015-12-12 17:12:24,245 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742280_1457
2015-12-12 17:12:24,246 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742281_1458
2015-12-12 18:38:55,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742283_1460 src: /127.0.0.1:57008 dest: /127.0.0.1:50010
2015-12-12 18:38:55,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57008, dest: /127.0.0.1:50010, bytes: 5508, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1328244293_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742283_1460, duration: 57790780
2015-12-12 18:38:55,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742283_1460, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:38:55,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742284_1461 src: /127.0.0.1:57009 dest: /127.0.0.1:50010
2015-12-12 18:38:55,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57009, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1328244293_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742284_1461, duration: 2052009
2015-12-12 18:38:55,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742284_1461, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:38:55,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742285_1462 src: /127.0.0.1:57010 dest: /127.0.0.1:50010
2015-12-12 18:38:55,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57010, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1328244293_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742285_1462, duration: 1967716
2015-12-12 18:38:55,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742285_1462, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:38:56,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742286_1463 src: /127.0.0.1:57011 dest: /127.0.0.1:50010
2015-12-12 18:38:56,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57011, dest: /127.0.0.1:50010, bytes: 87269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1328244293_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742286_1463, duration: 9887860
2015-12-12 18:38:56,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742286_1463, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:39:00,910 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742283_1460
2015-12-12 18:39:00,911 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742285_1462
2015-12-12 18:39:00,911 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742284_1461
2015-12-12 18:39:03,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742287_1464 src: /127.0.0.1:57019 dest: /127.0.0.1:50010
2015-12-12 18:39:03,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57019, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-196434767_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742287_1464, duration: 39456530
2015-12-12 18:39:03,354 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742287_1464, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:39:05,924 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742286_1463
2015-12-12 18:39:10,937 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742287_1464
2015-12-12 18:39:19,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742288_1465 src: /127.0.0.1:57038 dest: /127.0.0.1:50010
2015-12-12 18:39:28,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57038, dest: /127.0.0.1:50010, bytes: 48515, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-196434767_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742288_1465, duration: 8751080166
2015-12-12 18:39:28,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742288_1465, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:39:28,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742289_1466 src: /127.0.0.1:57051 dest: /127.0.0.1:50010
2015-12-12 18:39:28,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57051, dest: /127.0.0.1:50010, bytes: 354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-196434767_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742289_1466, duration: 2045908
2015-12-12 18:39:28,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742289_1466, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:39:28,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742290_1467 src: /127.0.0.1:57053 dest: /127.0.0.1:50010
2015-12-12 18:39:28,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57053, dest: /127.0.0.1:50010, bytes: 48515, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-196434767_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742290_1467, duration: 2426646
2015-12-12 18:39:28,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742290_1467, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:39:28,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742291_1468 src: /127.0.0.1:57054 dest: /127.0.0.1:50010
2015-12-12 18:39:28,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57054, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-196434767_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742291_1468, duration: 12201402
2015-12-12 18:39:28,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742291_1468, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:39:33,192 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742288_1465 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742288 for deletion
2015-12-12 18:39:33,192 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742283_1460 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742283 for deletion
2015-12-12 18:39:33,193 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742288_1465 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742288
2015-12-12 18:39:33,193 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742284_1461 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742284 for deletion
2015-12-12 18:39:33,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742285_1462 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742285 for deletion
2015-12-12 18:39:33,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742283_1460 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742283
2015-12-12 18:39:33,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742286_1463 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742286 for deletion
2015-12-12 18:39:33,196 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742287_1464 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742287 for deletion
2015-12-12 18:39:33,196 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742284_1461 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742284
2015-12-12 18:39:33,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742285_1462 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742285
2015-12-12 18:39:33,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742286_1463 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742286
2015-12-12 18:39:33,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742287_1464 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742287
2015-12-12 18:39:36,029 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742291_1468
2015-12-12 18:39:36,029 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742290_1467
2015-12-12 18:39:36,030 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742289_1466
2015-12-12 18:42:00,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742292_1469 src: /127.0.0.1:57062 dest: /127.0.0.1:50010
2015-12-12 18:42:00,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57062, dest: /127.0.0.1:50010, bytes: 5508, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_144315395_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742292_1469, duration: 60594817
2015-12-12 18:42:00,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742292_1469, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:42:00,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742293_1470 src: /127.0.0.1:57063 dest: /127.0.0.1:50010
2015-12-12 18:42:00,480 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57063, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_144315395_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742293_1470, duration: 11258646
2015-12-12 18:42:00,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742293_1470, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:42:00,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742294_1471 src: /127.0.0.1:57064 dest: /127.0.0.1:50010
2015-12-12 18:42:00,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57064, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_144315395_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742294_1471, duration: 1650757
2015-12-12 18:42:00,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742294_1471, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:42:00,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742295_1472 src: /127.0.0.1:57065 dest: /127.0.0.1:50010
2015-12-12 18:42:00,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57065, dest: /127.0.0.1:50010, bytes: 87269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_144315395_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742295_1472, duration: 18356772
2015-12-12 18:42:00,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742295_1472, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:42:06,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742296_1473 src: /127.0.0.1:57073 dest: /127.0.0.1:50010
2015-12-12 18:42:06,055 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742292_1469
2015-12-12 18:42:06,055 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742293_1470
2015-12-12 18:42:06,056 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742295_1472
2015-12-12 18:42:06,057 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742294_1471
2015-12-12 18:42:06,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57073, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_811875631_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742296_1473, duration: 40186219
2015-12-12 18:42:06,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742296_1473, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:42:16,587 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742296_1473
2015-12-12 18:42:21,249 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2664ms
No GCs detected
2015-12-12 18:42:23,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742297_1474 src: /127.0.0.1:57092 dest: /127.0.0.1:50010
2015-12-12 18:42:33,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57092, dest: /127.0.0.1:50010, bytes: 48518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_811875631_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742297_1474, duration: 9916870020
2015-12-12 18:42:33,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742297_1474, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:42:33,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742298_1475 src: /127.0.0.1:57104 dest: /127.0.0.1:50010
2015-12-12 18:42:33,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57104, dest: /127.0.0.1:50010, bytes: 354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_811875631_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742298_1475, duration: 2637310
2015-12-12 18:42:33,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742298_1475, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:42:33,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742299_1476 src: /127.0.0.1:57106 dest: /127.0.0.1:50010
2015-12-12 18:42:33,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57106, dest: /127.0.0.1:50010, bytes: 48518, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_811875631_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742299_1476, duration: 1769413
2015-12-12 18:42:33,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742299_1476, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:42:33,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742300_1477 src: /127.0.0.1:57107 dest: /127.0.0.1:50010
2015-12-12 18:42:33,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57107, dest: /127.0.0.1:50010, bytes: 103815, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_811875631_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742300_1477, duration: 3936014
2015-12-12 18:42:33,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742300_1477, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 18:42:36,296 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742292_1469 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742292 for deletion
2015-12-12 18:42:36,297 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742293_1470 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742293 for deletion
2015-12-12 18:42:36,298 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742294_1471 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742294 for deletion
2015-12-12 18:42:36,298 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742295_1472 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742295 for deletion
2015-12-12 18:42:36,299 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742296_1473 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742296 for deletion
2015-12-12 18:42:36,301 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742297_1474 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742297 for deletion
2015-12-12 18:42:36,301 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742292_1469 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742292
2015-12-12 18:42:36,302 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742293_1470 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742293
2015-12-12 18:42:36,302 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742294_1471 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742294
2015-12-12 18:42:36,302 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742295_1472 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742295
2015-12-12 18:42:36,303 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742296_1473 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742296
2015-12-12 18:42:36,303 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742297_1474 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742297
2015-12-12 18:42:41,644 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742300_1477
2015-12-12 18:42:41,644 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742298_1475
2015-12-12 18:42:41,645 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742299_1476
2015-12-12 18:54:42,297 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1841603009-127.0.1.1-1449456040486 Total blocks: 157, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-12-12 20:12:23,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from localhost/127.0.0.1:9000 with active state
2015-12-12 20:12:23,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-12 20:12:23,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-12 20:12:23,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 157 blocks total. Took 0 msec to generate and 7 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@76e1f7bf
2015-12-12 20:12:23,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-12 23:28:21,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742301_1478 src: /127.0.0.1:57247 dest: /127.0.0.1:50010
2015-12-12 23:28:21,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57247, dest: /127.0.0.1:50010, bytes: 5541, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1110650358_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742301_1478, duration: 37594753
2015-12-12 23:28:21,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742301_1478, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:28:21,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742302_1479 src: /127.0.0.1:57248 dest: /127.0.0.1:50010
2015-12-12 23:28:21,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57248, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1110650358_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742302_1479, duration: 5401578
2015-12-12 23:28:21,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742302_1479, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:28:21,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742303_1480 src: /127.0.0.1:57249 dest: /127.0.0.1:50010
2015-12-12 23:28:21,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57249, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1110650358_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742303_1480, duration: 2026715
2015-12-12 23:28:21,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742303_1480, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:28:21,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742304_1481 src: /127.0.0.1:57250 dest: /127.0.0.1:50010
2015-12-12 23:28:21,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57250, dest: /127.0.0.1:50010, bytes: 87388, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1110650358_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742304_1481, duration: 25526738
2015-12-12 23:28:21,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742304_1481, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:28:29,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742305_1482 src: /127.0.0.1:57258 dest: /127.0.0.1:50010
2015-12-12 23:28:29,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57258, dest: /127.0.0.1:50010, bytes: 103958, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1482804065_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742305_1482, duration: 41363389
2015-12-12 23:28:29,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742305_1482, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:28:30,098 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742303_1480
2015-12-12 23:28:30,100 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742304_1481
2015-12-12 23:28:30,101 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742302_1479
2015-12-12 23:28:30,101 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742301_1478
2015-12-12 23:28:35,113 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742305_1482
2015-12-12 23:28:44,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742306_1483 src: /127.0.0.1:57276 dest: /127.0.0.1:50010
2015-12-12 23:28:54,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57276, dest: /127.0.0.1:50010, bytes: 48503, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1482804065_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742306_1483, duration: 10091495491
2015-12-12 23:28:54,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742306_1483, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:28:54,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742307_1484 src: /127.0.0.1:57290 dest: /127.0.0.1:50010
2015-12-12 23:28:54,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57290, dest: /127.0.0.1:50010, bytes: 354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1482804065_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742307_1484, duration: 3096756
2015-12-12 23:28:54,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742307_1484, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:28:54,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742308_1485 src: /127.0.0.1:57292 dest: /127.0.0.1:50010
2015-12-12 23:28:54,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57292, dest: /127.0.0.1:50010, bytes: 48503, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1482804065_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742308_1485, duration: 2548303
2015-12-12 23:28:54,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742308_1485, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:28:54,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742309_1486 src: /127.0.0.1:57293 dest: /127.0.0.1:50010
2015-12-12 23:28:54,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57293, dest: /127.0.0.1:50010, bytes: 103958, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1482804065_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742309_1486, duration: 2552635
2015-12-12 23:28:54,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742309_1486, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:28:58,678 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742304_1481 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742304 for deletion
2015-12-12 23:28:58,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742305_1482 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742305 for deletion
2015-12-12 23:28:58,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742306_1483 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742306 for deletion
2015-12-12 23:28:58,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742301_1478 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742301 for deletion
2015-12-12 23:28:58,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742302_1479 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742302 for deletion
2015-12-12 23:28:58,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742303_1480 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742303 for deletion
2015-12-12 23:28:58,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742304_1481 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742304
2015-12-12 23:28:58,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742305_1482 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742305
2015-12-12 23:28:58,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742306_1483 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742306
2015-12-12 23:28:58,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742301_1478 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742301
2015-12-12 23:28:58,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742302_1479 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742302
2015-12-12 23:28:58,682 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742303_1480 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742303
2015-12-12 23:29:00,136 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742309_1486
2015-12-12 23:29:00,136 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742307_1484
2015-12-12 23:29:00,137 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742308_1485
2015-12-12 23:36:36,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742310_1487 src: /127.0.0.1:57306 dest: /127.0.0.1:50010
2015-12-12 23:36:37,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57306, dest: /127.0.0.1:50010, bytes: 5541, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_440718098_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742310_1487, duration: 35622710
2015-12-12 23:36:37,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742310_1487, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:36:37,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742311_1488 src: /127.0.0.1:57307 dest: /127.0.0.1:50010
2015-12-12 23:36:37,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57307, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_440718098_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742311_1488, duration: 4556843
2015-12-12 23:36:37,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742311_1488, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:36:37,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742312_1489 src: /127.0.0.1:57308 dest: /127.0.0.1:50010
2015-12-12 23:36:37,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57308, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_440718098_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742312_1489, duration: 1927487
2015-12-12 23:36:37,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742312_1489, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:36:37,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742313_1490 src: /127.0.0.1:57309 dest: /127.0.0.1:50010
2015-12-12 23:36:37,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57309, dest: /127.0.0.1:50010, bytes: 87388, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_440718098_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742313_1490, duration: 19810739
2015-12-12 23:36:37,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742313_1490, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:36:43,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742314_1491 src: /127.0.0.1:57318 dest: /127.0.0.1:50010
2015-12-12 23:36:43,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57318, dest: /127.0.0.1:50010, bytes: 103958, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1334103886_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742314_1491, duration: 65151991
2015-12-12 23:36:43,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742314_1491, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:36:45,197 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742311_1488
2015-12-12 23:36:45,198 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742313_1490
2015-12-12 23:36:45,217 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742310_1487
2015-12-12 23:36:45,217 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742312_1489
2015-12-12 23:36:50,227 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742314_1491
2015-12-12 23:36:58,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742315_1492 src: /127.0.0.1:57336 dest: /127.0.0.1:50010
2015-12-12 23:37:08,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742316_1493 src: /127.0.0.1:57349 dest: /127.0.0.1:50010
2015-12-12 23:37:09,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57349, dest: /127.0.0.1:50010, bytes: 266813, op: HDFS_WRITE, cliID: DFSClient_attempt_1449939365346_0014_r_000000_0_1603178884_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742316_1493, duration: 318087798
2015-12-12 23:37:09,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742316_1493, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:37:10,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57336, dest: /127.0.0.1:50010, bytes: 48880, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1334103886_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742315_1492, duration: 12404489844
2015-12-12 23:37:10,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742315_1492, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:37:10,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742317_1494 src: /127.0.0.1:57352 dest: /127.0.0.1:50010
2015-12-12 23:37:10,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57352, dest: /127.0.0.1:50010, bytes: 354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1334103886_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742317_1494, duration: 2229830
2015-12-12 23:37:10,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742317_1494, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:37:10,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742318_1495 src: /127.0.0.1:57354 dest: /127.0.0.1:50010
2015-12-12 23:37:10,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57354, dest: /127.0.0.1:50010, bytes: 48880, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1334103886_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742318_1495, duration: 4622216
2015-12-12 23:37:10,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742318_1495, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:37:10,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742319_1496 src: /127.0.0.1:57355 dest: /127.0.0.1:50010
2015-12-12 23:37:10,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57355, dest: /127.0.0.1:50010, bytes: 103958, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1334103886_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742319_1496, duration: 5247397
2015-12-12 23:37:10,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742319_1496, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-12 23:37:15,504 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742316_1493
2015-12-12 23:37:16,750 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742310_1487 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742310 for deletion
2015-12-12 23:37:16,751 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742311_1488 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742311 for deletion
2015-12-12 23:37:16,751 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742312_1489 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742312 for deletion
2015-12-12 23:37:16,751 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742313_1490 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742313 for deletion
2015-12-12 23:37:16,751 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742314_1491 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742314 for deletion
2015-12-12 23:37:16,751 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742315_1492 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742315 for deletion
2015-12-12 23:37:16,752 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742310_1487 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742310
2015-12-12 23:37:16,752 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742311_1488 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742311
2015-12-12 23:37:16,752 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742312_1489 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742312
2015-12-12 23:37:16,752 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742313_1490 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742313
2015-12-12 23:37:16,753 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742314_1491 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742314
2015-12-12 23:37:16,753 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742315_1492 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742315
2015-12-12 23:37:20,514 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742318_1495
2015-12-12 23:37:20,514 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742317_1494
2015-12-12 23:37:20,516 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742319_1496
2015-12-13 00:29:45,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742320_1497 src: /127.0.0.1:57412 dest: /127.0.0.1:50010
2015-12-13 00:29:45,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57412, dest: /127.0.0.1:50010, bytes: 3199, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-677479107_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742320_1497, duration: 41618460
2015-12-13 00:29:45,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742320_1497, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 00:29:45,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742321_1498 src: /127.0.0.1:57413 dest: /127.0.0.1:50010
2015-12-13 00:29:45,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57413, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-677479107_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742321_1498, duration: 2100406
2015-12-13 00:29:45,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742321_1498, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 00:29:45,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742322_1499 src: /127.0.0.1:57414 dest: /127.0.0.1:50010
2015-12-13 00:29:45,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57414, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-677479107_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742322_1499, duration: 2037662
2015-12-13 00:29:45,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742322_1499, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 00:29:45,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742323_1500 src: /127.0.0.1:57415 dest: /127.0.0.1:50010
2015-12-13 00:29:45,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57415, dest: /127.0.0.1:50010, bytes: 87278, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-677479107_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742323_1500, duration: 9659248
2015-12-13 00:29:45,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742323_1500, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 00:29:52,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742324_1501 src: /127.0.0.1:57425 dest: /127.0.0.1:50010
2015-12-13 00:29:52,514 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742321_1498
2015-12-13 00:29:52,517 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742323_1500
2015-12-13 00:29:52,519 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742320_1497
2015-12-13 00:29:52,523 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742322_1499
2015-12-13 00:29:52,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57425, dest: /127.0.0.1:50010, bytes: 103824, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-795535398_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742324_1501, duration: 44898513
2015-12-13 00:29:52,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742324_1501, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 00:30:02,576 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742324_1501
2015-12-13 00:30:09,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742325_1502 src: /127.0.0.1:57442 dest: /127.0.0.1:50010
2015-12-13 00:30:16,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742326_1503 src: /127.0.0.1:57448 dest: /127.0.0.1:50010
2015-12-13 00:30:16,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57448, dest: /127.0.0.1:50010, bytes: 2201, op: HDFS_WRITE, cliID: DFSClient_attempt_1449939365346_0015_r_000000_0_-1490571040_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742326_1503, duration: 44745831
2015-12-13 00:30:16,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742326_1503, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 00:30:17,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57442, dest: /127.0.0.1:50010, bytes: 47075, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-795535398_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742325_1502, duration: 8478097183
2015-12-13 00:30:17,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742325_1502, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 00:30:17,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742327_1504 src: /127.0.0.1:57450 dest: /127.0.0.1:50010
2015-12-13 00:30:17,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57450, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-795535398_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742327_1504, duration: 1752083
2015-12-13 00:30:17,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742327_1504, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 00:30:17,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742328_1505 src: /127.0.0.1:57452 dest: /127.0.0.1:50010
2015-12-13 00:30:17,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57452, dest: /127.0.0.1:50010, bytes: 47075, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-795535398_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742328_1505, duration: 4279676
2015-12-13 00:30:17,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742328_1505, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 00:30:17,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742329_1506 src: /127.0.0.1:57453 dest: /127.0.0.1:50010
2015-12-13 00:30:17,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57453, dest: /127.0.0.1:50010, bytes: 103824, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-795535398_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742329_1506, duration: 4268344
2015-12-13 00:30:17,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742329_1506, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 00:30:22,675 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742326_1503
2015-12-13 00:30:23,889 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742320_1497 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742320 for deletion
2015-12-13 00:30:23,895 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742321_1498 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742321 for deletion
2015-12-13 00:30:23,895 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742320_1497 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742320
2015-12-13 00:30:23,896 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742321_1498 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742321
2015-12-13 00:30:23,895 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742322_1499 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742322 for deletion
2015-12-13 00:30:23,896 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742323_1500 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742323 for deletion
2015-12-13 00:30:23,896 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742324_1501 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742324 for deletion
2015-12-13 00:30:23,897 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742325_1502 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742325 for deletion
2015-12-13 00:30:23,898 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742322_1499 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742322
2015-12-13 00:30:23,899 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742323_1500 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742323
2015-12-13 00:30:23,899 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742324_1501 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742324
2015-12-13 00:30:23,899 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742325_1502 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742325
2015-12-13 00:30:27,682 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742329_1506
2015-12-13 00:30:27,683 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742327_1504
2015-12-13 00:30:27,683 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742328_1505
2015-12-13 00:31:24,104 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "ubuntu/127.0.1.1"; destination host is: "localhost":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:513)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-12-13 00:31:27,900 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-13 00:31:28,744 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-13 00:31:28,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-13 10:34:16,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-13 10:34:16,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-13 10:34:17,275 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-13 10:34:17,335 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-13 10:34:17,335 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-13 10:34:17,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-13 10:34:17,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-13 10:34:17,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-13 10:34:17,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-13 10:34:17,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-13 10:34:17,472 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-13 10:34:17,475 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-13 10:34:17,484 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-13 10:34:17,486 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-13 10:34:17,486 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-13 10:34:17,486 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-13 10:34:17,502 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-13 10:34:17,511 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-13 10:34:17,512 INFO org.mortbay.log: jetty-6.1.26
2015-12-13 10:34:17,816 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-13 10:34:17,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-13 10:34:17,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-13 10:34:18,000 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-13 10:34:18,016 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-13 10:34:18,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-13 10:34:18,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-13 10:34:18,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-13 10:34:18,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-13 10:34:18,095 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-13 10:34:18,096 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-13 10:34:18,378 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-13 10:34:18,394 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 23917@ubuntu
2015-12-13 10:34:18,513 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1841603009-127.0.1.1-1449456040486
2015-12-13 10:34:18,513 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-12-13 10:34:18,514 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-13 10:34:18,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1379726899;bpid=BP-1841603009-127.0.1.1-1449456040486;lv=-56;nsInfo=lv=-60;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0;bpid=BP-1841603009-127.0.1.1-1449456040486;dnuuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-13 10:34:18,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /home/parallels/hadoop/tmp/dfs/data/current
2015-12-13 10:34:18,573 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/parallels/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-13 10:34:18,612 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-13 10:34:18,616 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1450039570615 with interval 21600000
2015-12-13 10:34:18,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-13 10:34:18,617 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-13 10:34:18,630 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1841603009-127.0.1.1-1449456040486 on /home/parallels/hadoop/tmp/dfs/data/current: 13ms
2015-12-13 10:34:18,630 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1841603009-127.0.1.1-1449456040486: 14ms
2015-12-13 10:34:18,631 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-13 10:34:18,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current: 19ms
2015-12-13 10:34:18,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 20ms
2015-12-13 10:34:18,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-13 10:34:18,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-13 10:34:18,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-13 10:34:18,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=4100
2015-12-13 10:34:18,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000
2015-12-13 10:34:18,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 168 blocks total. Took 1 msec to generate and 52 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@e4bdddc
2015-12-13 10:34:18,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-13 10:34:18,886 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-12-13 10:34:18,886 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-13 10:34:18,887 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-12-13 10:34:18,887 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-12-13 10:34:18,888 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-13 10:34:18,894 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1841603009-127.0.1.1-1449456040486 to blockPoolScannerMap, new size=1
2015-12-13 10:35:00,724 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742326_1503 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742326 for deletion
2015-12-13 10:35:00,731 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742326_1503 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742326
2015-12-13 10:36:49,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742330_1507 src: /127.0.0.1:57487 dest: /127.0.0.1:50010
2015-12-13 10:36:49,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57487, dest: /127.0.0.1:50010, bytes: 3427, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_951933323_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742330_1507, duration: 110108358
2015-12-13 10:36:49,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742330_1507, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:36:50,358 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742331_1508 src: /127.0.0.1:57488 dest: /127.0.0.1:50010
2015-12-13 10:36:50,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57488, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_951933323_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742331_1508, duration: 2243687
2015-12-13 10:36:50,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742331_1508, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:36:50,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742332_1509 src: /127.0.0.1:57489 dest: /127.0.0.1:50010
2015-12-13 10:36:50,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57489, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_951933323_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742332_1509, duration: 1909010
2015-12-13 10:36:50,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742332_1509, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:36:51,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742333_1510 src: /127.0.0.1:57490 dest: /127.0.0.1:50010
2015-12-13 10:36:51,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57490, dest: /127.0.0.1:50010, bytes: 87278, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_951933323_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742333_1510, duration: 10611489
2015-12-13 10:36:51,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742333_1510, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:36:58,800 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742330_1507
2015-12-13 10:36:58,803 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742331_1508
2015-12-13 10:36:58,805 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742332_1509
2015-12-13 10:36:58,810 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742333_1510
2015-12-13 10:37:02,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742334_1511 src: /127.0.0.1:57498 dest: /127.0.0.1:50010
2015-12-13 10:37:02,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57498, dest: /127.0.0.1:50010, bytes: 103824, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-419671786_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742334_1511, duration: 50343019
2015-12-13 10:37:02,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742334_1511, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:37:08,946 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742334_1511
2015-12-13 10:37:20,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742335_1512 src: /127.0.0.1:57516 dest: /127.0.0.1:50010
2015-12-13 10:38:01,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57516, dest: /127.0.0.1:50010, bytes: 51033, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-419671786_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742335_1512, duration: 40801737624
2015-12-13 10:38:01,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742335_1512, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:38:01,278 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742336_1513 src: /127.0.0.1:57565 dest: /127.0.0.1:50010
2015-12-13 10:38:01,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57565, dest: /127.0.0.1:50010, bytes: 339, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-419671786_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742336_1513, duration: 3918765
2015-12-13 10:38:01,287 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742336_1513, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:38:01,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742337_1514 src: /127.0.0.1:57567 dest: /127.0.0.1:50010
2015-12-13 10:38:01,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57567, dest: /127.0.0.1:50010, bytes: 51033, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-419671786_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742337_1514, duration: 10164137
2015-12-13 10:38:01,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742337_1514, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:38:01,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742338_1515 src: /127.0.0.1:57568 dest: /127.0.0.1:50010
2015-12-13 10:38:01,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57568, dest: /127.0.0.1:50010, bytes: 103824, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-419671786_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742338_1515, duration: 7646152
2015-12-13 10:38:01,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742338_1515, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:38:03,743 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742330_1507 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742330 for deletion
2015-12-13 10:38:03,745 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742331_1508 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742331 for deletion
2015-12-13 10:38:03,747 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742330_1507 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742330
2015-12-13 10:38:03,748 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742332_1509 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742332 for deletion
2015-12-13 10:38:03,748 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742333_1510 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742333 for deletion
2015-12-13 10:38:03,749 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742334_1511 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742334 for deletion
2015-12-13 10:38:03,749 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742335_1512 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742335 for deletion
2015-12-13 10:38:03,749 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742331_1508 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742331
2015-12-13 10:38:03,750 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742332_1509 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742332
2015-12-13 10:38:03,753 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742333_1510 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742333
2015-12-13 10:38:03,758 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742334_1511 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742334
2015-12-13 10:38:03,760 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742335_1512 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir1/blk_1073742335
2015-12-13 10:38:09,054 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742337_1514
2015-12-13 10:38:09,055 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742338_1515
2015-12-13 10:38:09,057 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742336_1513
2015-12-13 10:38:11,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742339_1516 src: /127.0.0.1:57578 dest: /127.0.0.1:50010
2015-12-13 10:38:11,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57578, dest: /127.0.0.1:50010, bytes: 3411, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1786991234_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742339_1516, duration: 98614886
2015-12-13 10:38:11,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742339_1516, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:38:11,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742340_1517 src: /127.0.0.1:57579 dest: /127.0.0.1:50010
2015-12-13 10:38:11,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57579, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1786991234_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742340_1517, duration: 1398264
2015-12-13 10:38:11,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742340_1517, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:38:11,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742341_1518 src: /127.0.0.1:57580 dest: /127.0.0.1:50010
2015-12-13 10:38:11,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57580, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1786991234_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742341_1518, duration: 1973804
2015-12-13 10:38:11,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742341_1518, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:38:12,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742342_1519 src: /127.0.0.1:57581 dest: /127.0.0.1:50010
2015-12-13 10:38:12,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57581, dest: /127.0.0.1:50010, bytes: 87145, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1786991234_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742342_1519, duration: 31400298
2015-12-13 10:38:12,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742342_1519, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:38:19,537 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742339_1516
2015-12-13 10:38:19,546 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742340_1517
2015-12-13 10:38:19,557 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742342_1519
2015-12-13 10:38:19,561 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742341_1518
2015-12-13 10:38:28,650 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742343_1520 src: /127.0.0.1:57590 dest: /127.0.0.1:50010
2015-12-13 10:38:28,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57590, dest: /127.0.0.1:50010, bytes: 103667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_870862644_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742343_1520, duration: 211801467
2015-12-13 10:38:28,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742343_1520, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:38:34,661 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742343_1520
2015-12-13 10:38:50,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742344_1521 src: /127.0.0.1:57608 dest: /127.0.0.1:50010
2015-12-13 10:39:04,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742345_1522 src: /127.0.0.1:57620 dest: /127.0.0.1:50010
2015-12-13 10:39:04,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57620, dest: /127.0.0.1:50010, bytes: 4232, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0002_r_000000_0_-1827108310_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742345_1522, duration: 118309310
2015-12-13 10:39:04,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742345_1522, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:39:06,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57608, dest: /127.0.0.1:50010, bytes: 48429, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_870862644_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742344_1521, duration: 15969369226
2015-12-13 10:39:06,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742344_1521, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:39:06,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742346_1523 src: /127.0.0.1:57622 dest: /127.0.0.1:50010
2015-12-13 10:39:06,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57622, dest: /127.0.0.1:50010, bytes: 354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_870862644_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742346_1523, duration: 3031554
2015-12-13 10:39:06,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742346_1523, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:39:06,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742347_1524 src: /127.0.0.1:57624 dest: /127.0.0.1:50010
2015-12-13 10:39:06,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57624, dest: /127.0.0.1:50010, bytes: 48429, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_870862644_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742347_1524, duration: 5671227
2015-12-13 10:39:06,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742347_1524, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:39:06,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742348_1525 src: /127.0.0.1:57625 dest: /127.0.0.1:50010
2015-12-13 10:39:06,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57625, dest: /127.0.0.1:50010, bytes: 103667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_870862644_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742348_1525, duration: 13359045
2015-12-13 10:39:06,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742348_1525, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:39:09,751 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742339_1516 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742339 for deletion
2015-12-13 10:39:09,754 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742340_1517 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742340 for deletion
2015-12-13 10:39:09,754 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742341_1518 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742341 for deletion
2015-12-13 10:39:09,754 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742342_1519 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742342 for deletion
2015-12-13 10:39:09,755 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742343_1520 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742343 for deletion
2015-12-13 10:39:09,755 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742344_1521 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742344 for deletion
2015-12-13 10:39:09,755 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742339_1516 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742339
2015-12-13 10:39:09,755 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742340_1517 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742340
2015-12-13 10:39:09,756 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742341_1518 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742341
2015-12-13 10:39:09,756 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742342_1519 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742342
2015-12-13 10:39:09,756 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742343_1520 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742343
2015-12-13 10:39:09,756 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742344_1521 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742344
2015-12-13 10:39:14,853 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742346_1523
2015-12-13 10:39:14,855 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742348_1525
2015-12-13 10:39:14,856 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742345_1522
2015-12-13 10:39:14,857 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742347_1524
2015-12-13 10:45:18,803 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742345_1522 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742345 for deletion
2015-12-13 10:45:18,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742345_1522 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742345
2015-12-13 10:45:21,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742349_1526 src: /127.0.0.1:57638 dest: /127.0.0.1:50010
2015-12-13 10:45:21,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57638, dest: /127.0.0.1:50010, bytes: 3407, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1147815938_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742349_1526, duration: 57140892
2015-12-13 10:45:21,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742349_1526, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:45:21,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742350_1527 src: /127.0.0.1:57639 dest: /127.0.0.1:50010
2015-12-13 10:45:21,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57639, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1147815938_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742350_1527, duration: 1795491
2015-12-13 10:45:21,951 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742350_1527, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:45:21,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742351_1528 src: /127.0.0.1:57640 dest: /127.0.0.1:50010
2015-12-13 10:45:21,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57640, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1147815938_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742351_1528, duration: 1905951
2015-12-13 10:45:21,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742351_1528, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:45:22,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742352_1529 src: /127.0.0.1:57641 dest: /127.0.0.1:50010
2015-12-13 10:45:22,311 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57641, dest: /127.0.0.1:50010, bytes: 87145, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1147815938_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742352_1529, duration: 14188239
2015-12-13 10:45:22,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742352_1529, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:45:29,586 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742353_1530 src: /127.0.0.1:57650 dest: /127.0.0.1:50010
2015-12-13 10:45:29,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57650, dest: /127.0.0.1:50010, bytes: 103667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1945949903_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742353_1530, duration: 35765296
2015-12-13 10:45:29,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742353_1530, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:45:29,931 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742349_1526
2015-12-13 10:45:29,932 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742351_1528
2015-12-13 10:45:29,933 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742352_1529
2015-12-13 10:45:29,941 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742350_1527
2015-12-13 10:45:34,997 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742353_1530
2015-12-13 10:45:44,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742354_1531 src: /127.0.0.1:57668 dest: /127.0.0.1:50010
2015-12-13 10:46:07,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742355_1532 src: /127.0.0.1:57695 dest: /127.0.0.1:50010
2015-12-13 10:46:08,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57695, dest: /127.0.0.1:50010, bytes: 4232, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0003_r_000000_0_1466824740_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742355_1532, duration: 81126860
2015-12-13 10:46:08,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742355_1532, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:46:09,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57668, dest: /127.0.0.1:50010, bytes: 53674, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1945949903_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742354_1531, duration: 24718820676
2015-12-13 10:46:09,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742354_1531, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:46:09,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742356_1533 src: /127.0.0.1:57698 dest: /127.0.0.1:50010
2015-12-13 10:46:09,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57698, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1945949903_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742356_1533, duration: 1345938
2015-12-13 10:46:09,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742356_1533, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:46:09,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742357_1534 src: /127.0.0.1:57700 dest: /127.0.0.1:50010
2015-12-13 10:46:09,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57700, dest: /127.0.0.1:50010, bytes: 53674, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1945949903_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742357_1534, duration: 7407383
2015-12-13 10:46:09,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742357_1534, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:46:09,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742358_1535 src: /127.0.0.1:57701 dest: /127.0.0.1:50010
2015-12-13 10:46:09,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57701, dest: /127.0.0.1:50010, bytes: 103667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1945949903_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742358_1535, duration: 8314503
2015-12-13 10:46:09,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742358_1535, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:46:12,818 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742352_1529 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742352 for deletion
2015-12-13 10:46:12,819 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742353_1530 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742353 for deletion
2015-12-13 10:46:12,820 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742354_1531 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742354 for deletion
2015-12-13 10:46:12,823 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742349_1526 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742349 for deletion
2015-12-13 10:46:12,824 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742350_1527 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742350 for deletion
2015-12-13 10:46:12,825 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742351_1528 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742351 for deletion
2015-12-13 10:46:12,825 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742352_1529 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742352
2015-12-13 10:46:12,826 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742353_1530 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742353
2015-12-13 10:46:12,826 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742354_1531 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742354
2015-12-13 10:46:12,828 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742349_1526 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742349
2015-12-13 10:46:12,828 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742350_1527 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742350
2015-12-13 10:46:12,828 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742351_1528 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742351
2015-12-13 10:46:15,091 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742358_1535
2015-12-13 10:46:15,093 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742356_1533
2015-12-13 10:46:15,098 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742357_1534
2015-12-13 10:46:15,101 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742355_1532
2015-12-13 10:48:42,832 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742355_1532 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742355 for deletion
2015-12-13 10:48:42,833 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742355_1532 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742355
2015-12-13 10:48:46,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742359_1536 src: /127.0.0.1:57711 dest: /127.0.0.1:50010
2015-12-13 10:48:46,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57711, dest: /127.0.0.1:50010, bytes: 3412, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1327220972_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742359_1536, duration: 56089804
2015-12-13 10:48:46,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742359_1536, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:48:46,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742360_1537 src: /127.0.0.1:57712 dest: /127.0.0.1:50010
2015-12-13 10:48:46,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57712, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1327220972_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742360_1537, duration: 9369997
2015-12-13 10:48:46,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742360_1537, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:48:46,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742361_1538 src: /127.0.0.1:57713 dest: /127.0.0.1:50010
2015-12-13 10:48:46,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57713, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1327220972_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742361_1538, duration: 1671267
2015-12-13 10:48:46,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742361_1538, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:48:46,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742362_1539 src: /127.0.0.1:57714 dest: /127.0.0.1:50010
2015-12-13 10:48:46,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57714, dest: /127.0.0.1:50010, bytes: 87145, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1327220972_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742362_1539, duration: 56573380
2015-12-13 10:48:46,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742362_1539, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:48:51,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742363_1540 src: /127.0.0.1:57722 dest: /127.0.0.1:50010
2015-12-13 10:48:51,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57722, dest: /127.0.0.1:50010, bytes: 103667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2080986298_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742363_1540, duration: 55261865
2015-12-13 10:48:51,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742363_1540, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:48:55,129 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742359_1536
2015-12-13 10:48:55,131 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742362_1539
2015-12-13 10:48:55,131 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742360_1537
2015-12-13 10:48:55,131 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742361_1538
2015-12-13 10:49:00,154 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742363_1540
2015-12-13 10:49:08,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742364_1541 src: /127.0.0.1:57741 dest: /127.0.0.1:50010
2015-12-13 10:49:19,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742365_1542 src: /127.0.0.1:57753 dest: /127.0.0.1:50010
2015-12-13 10:49:19,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57753, dest: /127.0.0.1:50010, bytes: 4232, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0004_r_000000_0_-1196534749_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742365_1542, duration: 108452540
2015-12-13 10:49:19,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742365_1542, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:49:19,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57741, dest: /127.0.0.1:50010, bytes: 48411, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2080986298_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742364_1541, duration: 11558653319
2015-12-13 10:49:19,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742364_1541, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:49:19,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742366_1543 src: /127.0.0.1:57756 dest: /127.0.0.1:50010
2015-12-13 10:49:19,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57756, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2080986298_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742366_1543, duration: 2259363
2015-12-13 10:49:19,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742366_1543, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:49:19,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742367_1544 src: /127.0.0.1:57758 dest: /127.0.0.1:50010
2015-12-13 10:49:19,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57758, dest: /127.0.0.1:50010, bytes: 48411, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2080986298_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742367_1544, duration: 3696847
2015-12-13 10:49:19,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742367_1544, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:49:19,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742368_1545 src: /127.0.0.1:57759 dest: /127.0.0.1:50010
2015-12-13 10:49:19,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57759, dest: /127.0.0.1:50010, bytes: 103667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2080986298_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742368_1545, duration: 2942996
2015-12-13 10:49:19,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742368_1545, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:49:24,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742359_1536 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742359 for deletion
2015-12-13 10:49:24,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742360_1537 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742360 for deletion
2015-12-13 10:49:24,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742361_1538 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742361 for deletion
2015-12-13 10:49:24,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742362_1539 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742362 for deletion
2015-12-13 10:49:24,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742363_1540 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742363 for deletion
2015-12-13 10:49:24,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742364_1541 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742364 for deletion
2015-12-13 10:49:24,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742359_1536 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742359
2015-12-13 10:49:24,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742360_1537 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742360
2015-12-13 10:49:24,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742361_1538 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742361
2015-12-13 10:49:24,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742362_1539 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742362
2015-12-13 10:49:24,850 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742363_1540 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742363
2015-12-13 10:49:24,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742364_1541 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742364
2015-12-13 10:49:25,288 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742368_1545
2015-12-13 10:49:25,288 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742365_1542
2015-12-13 10:49:25,289 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742366_1543
2015-12-13 10:49:25,290 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742367_1544
2015-12-13 10:53:09,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742369_1546 src: /127.0.0.1:57770 dest: /127.0.0.1:50010
2015-12-13 10:53:09,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57770, dest: /127.0.0.1:50010, bytes: 3414, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_4402715_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742369_1546, duration: 35775612
2015-12-13 10:53:09,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742369_1546, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:53:09,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742370_1547 src: /127.0.0.1:57771 dest: /127.0.0.1:50010
2015-12-13 10:53:09,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57771, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_4402715_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742370_1547, duration: 2439988
2015-12-13 10:53:09,831 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742370_1547, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:53:09,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742371_1548 src: /127.0.0.1:57772 dest: /127.0.0.1:50010
2015-12-13 10:53:09,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57772, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_4402715_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742371_1548, duration: 1993403
2015-12-13 10:53:09,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742371_1548, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:53:09,870 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742365_1542 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742365 for deletion
2015-12-13 10:53:09,871 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742365_1542 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742365
2015-12-13 10:53:10,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742372_1549 src: /127.0.0.1:57773 dest: /127.0.0.1:50010
2015-12-13 10:53:10,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57773, dest: /127.0.0.1:50010, bytes: 87145, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_4402715_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742372_1549, duration: 13267746
2015-12-13 10:53:10,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742372_1549, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:53:15,329 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742371_1548
2015-12-13 10:53:15,330 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742369_1546
2015-12-13 10:53:15,330 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742370_1547
2015-12-13 10:53:15,331 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742372_1549
2015-12-13 10:53:16,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742373_1550 src: /127.0.0.1:57781 dest: /127.0.0.1:50010
2015-12-13 10:53:16,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57781, dest: /127.0.0.1:50010, bytes: 103667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1442857560_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742373_1550, duration: 35620921
2015-12-13 10:53:16,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742373_1550, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:53:25,376 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742373_1550
2015-12-13 10:53:31,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742374_1551 src: /127.0.0.1:57799 dest: /127.0.0.1:50010
2015-12-13 10:53:44,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742375_1552 src: /127.0.0.1:57817 dest: /127.0.0.1:50010
2015-12-13 10:53:44,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57817, dest: /127.0.0.1:50010, bytes: 4232, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0005_r_000000_0_618554573_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742375_1552, duration: 114974931
2015-12-13 10:53:44,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742375_1552, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:53:45,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57799, dest: /127.0.0.1:50010, bytes: 49846, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1442857560_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742374_1551, duration: 14544240088
2015-12-13 10:53:45,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742374_1551, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:53:45,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742376_1553 src: /127.0.0.1:57820 dest: /127.0.0.1:50010
2015-12-13 10:53:45,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57820, dest: /127.0.0.1:50010, bytes: 354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1442857560_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742376_1553, duration: 1808570
2015-12-13 10:53:45,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742376_1553, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:53:45,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742377_1554 src: /127.0.0.1:57822 dest: /127.0.0.1:50010
2015-12-13 10:53:45,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57822, dest: /127.0.0.1:50010, bytes: 49846, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1442857560_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742377_1554, duration: 8385257
2015-12-13 10:53:45,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742377_1554, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:53:46,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742378_1555 src: /127.0.0.1:57823 dest: /127.0.0.1:50010
2015-12-13 10:53:46,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:57823, dest: /127.0.0.1:50010, bytes: 103667, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1442857560_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742378_1555, duration: 10711573
2015-12-13 10:53:46,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742378_1555, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 10:53:50,429 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742375_1552
2015-12-13 10:53:51,899 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742369_1546 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742369 for deletion
2015-12-13 10:53:51,900 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742370_1547 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742370 for deletion
2015-12-13 10:53:51,900 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742371_1548 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742371 for deletion
2015-12-13 10:53:51,900 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742372_1549 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742372 for deletion
2015-12-13 10:53:51,900 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742373_1550 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742373 for deletion
2015-12-13 10:53:51,900 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742374_1551 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742374 for deletion
2015-12-13 10:53:51,903 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742369_1546 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742369
2015-12-13 10:53:51,904 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742370_1547 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742370
2015-12-13 10:53:51,904 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742371_1548 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742371
2015-12-13 10:53:51,905 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742372_1549 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742372
2015-12-13 10:53:51,905 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742373_1550 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742373
2015-12-13 10:53:51,905 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742374_1551 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742374
2015-12-13 10:53:55,469 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742378_1555
2015-12-13 10:53:55,471 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742377_1554
2015-12-13 10:53:55,473 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742376_1553
2015-12-13 12:01:16,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 183 blocks total. Took 2 msec to generate and 48 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@6098b14d
2015-12-13 12:01:16,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-13 13:00:47,404 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742375_1552 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742375 for deletion
2015-12-13 13:00:47,405 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742375_1552 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742375
2015-12-13 13:00:49,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742379_1556 src: /127.0.0.1:58044 dest: /127.0.0.1:50010
2015-12-13 13:00:49,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58044, dest: /127.0.0.1:50010, bytes: 5329, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1427144070_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742379_1556, duration: 48647661
2015-12-13 13:00:49,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742379_1556, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:00:49,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742380_1557 src: /127.0.0.1:58045 dest: /127.0.0.1:50010
2015-12-13 13:00:49,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58045, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1427144070_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742380_1557, duration: 2327109
2015-12-13 13:00:49,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742380_1557, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:00:49,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742381_1558 src: /127.0.0.1:58046 dest: /127.0.0.1:50010
2015-12-13 13:00:49,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58046, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1427144070_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742381_1558, duration: 1980900
2015-12-13 13:00:49,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742381_1558, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:00:50,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742382_1559 src: /127.0.0.1:58047 dest: /127.0.0.1:50010
2015-12-13 13:00:50,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58047, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1427144070_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742382_1559, duration: 14556162
2015-12-13 13:00:50,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742382_1559, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:00:54,717 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742379_1556
2015-12-13 13:00:59,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742383_1560 src: /127.0.0.1:58057 dest: /127.0.0.1:50010
2015-12-13 13:00:59,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58057, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1867177369_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742383_1560, duration: 38850614
2015-12-13 13:00:59,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742383_1560, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:00:59,727 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742381_1558
2015-12-13 13:00:59,729 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742382_1559
2015-12-13 13:00:59,729 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742380_1557
2015-12-13 13:01:04,751 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742383_1560
2015-12-13 13:01:14,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742384_1561 src: /127.0.0.1:58074 dest: /127.0.0.1:50010
2015-12-13 13:01:24,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742385_1562 src: /127.0.0.1:58086 dest: /127.0.0.1:50010
2015-12-13 13:01:25,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58086, dest: /127.0.0.1:50010, bytes: 2201, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0006_r_000000_0_292627908_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742385_1562, duration: 46053193
2015-12-13 13:01:25,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742385_1562, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:01:26,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58074, dest: /127.0.0.1:50010, bytes: 48185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1867177369_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742384_1561, duration: 12064819481
2015-12-13 13:01:26,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742384_1561, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:01:26,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742386_1563 src: /127.0.0.1:58089 dest: /127.0.0.1:50010
2015-12-13 13:01:26,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58089, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1867177369_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742386_1563, duration: 1934681
2015-12-13 13:01:26,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742386_1563, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:01:26,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742387_1564 src: /127.0.0.1:58091 dest: /127.0.0.1:50010
2015-12-13 13:01:26,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58091, dest: /127.0.0.1:50010, bytes: 48185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1867177369_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742387_1564, duration: 8555721
2015-12-13 13:01:26,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742387_1564, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:01:26,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742388_1565 src: /127.0.0.1:58092 dest: /127.0.0.1:50010
2015-12-13 13:01:26,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58092, dest: /127.0.0.1:50010, bytes: 103662, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1867177369_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742388_1565, duration: 2902280
2015-12-13 13:01:26,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742388_1565, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:01:32,414 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742384_1561 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742384 for deletion
2015-12-13 13:01:32,415 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742379_1556 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742379 for deletion
2015-12-13 13:01:32,415 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742380_1557 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742380 for deletion
2015-12-13 13:01:32,415 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742381_1558 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742381 for deletion
2015-12-13 13:01:32,415 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742382_1559 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742382 for deletion
2015-12-13 13:01:32,416 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742383_1560 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742383 for deletion
2015-12-13 13:01:32,419 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742384_1561 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742384
2015-12-13 13:01:32,420 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742379_1556 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742379
2015-12-13 13:01:32,420 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742380_1557 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742380
2015-12-13 13:01:32,420 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742381_1558 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742381
2015-12-13 13:01:32,420 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742382_1559 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742382
2015-12-13 13:01:32,421 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742383_1560 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742383
2015-12-13 13:01:34,898 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742388_1565
2015-12-13 13:01:34,943 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742387_1564
2015-12-13 13:01:34,943 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742386_1563
2015-12-13 13:01:34,943 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742385_1562
2015-12-13 13:31:05,658 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742385_1562 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742385 for deletion
2015-12-13 13:31:05,663 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742385_1562 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742385
2015-12-13 13:31:07,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742389_1566 src: /127.0.0.1:58133 dest: /127.0.0.1:50010
2015-12-13 13:31:07,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58133, dest: /127.0.0.1:50010, bytes: 5329, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_258690400_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742389_1566, duration: 59211961
2015-12-13 13:31:07,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742389_1566, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:31:07,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742390_1567 src: /127.0.0.1:58134 dest: /127.0.0.1:50010
2015-12-13 13:31:07,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58134, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_258690400_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742390_1567, duration: 2082893
2015-12-13 13:31:07,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742390_1567, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:31:07,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742391_1568 src: /127.0.0.1:58135 dest: /127.0.0.1:50010
2015-12-13 13:31:07,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58135, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_258690400_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742391_1568, duration: 2051505
2015-12-13 13:31:07,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742391_1568, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:31:07,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742392_1569 src: /127.0.0.1:58136 dest: /127.0.0.1:50010
2015-12-13 13:31:07,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58136, dest: /127.0.0.1:50010, bytes: 87140, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_258690400_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742392_1569, duration: 8895618
2015-12-13 13:31:07,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742392_1569, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:31:15,151 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742390_1567
2015-12-13 13:31:15,151 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742392_1569
2015-12-13 13:31:15,152 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742389_1566
2015-12-13 13:31:15,152 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742391_1568
2015-12-13 13:31:33,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742393_1570 src: /127.0.0.1:58140 dest: /127.0.0.1:50010
2015-12-13 13:31:33,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58140, dest: /127.0.0.1:50010, bytes: 5329, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-532726123_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742393_1570, duration: 53159324
2015-12-13 13:31:33,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742393_1570, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:31:40,167 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742393_1570
2015-12-13 13:31:59,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742394_1571 src: /127.0.0.1:58145 dest: /127.0.0.1:50010
2015-12-13 13:31:59,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58145, dest: /127.0.0.1:50010, bytes: 5214, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2103325452_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742394_1571, duration: 46033586
2015-12-13 13:31:59,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742394_1571, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:31:59,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742395_1572 src: /127.0.0.1:58146 dest: /127.0.0.1:50010
2015-12-13 13:31:59,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58146, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2103325452_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742395_1572, duration: 1991425
2015-12-13 13:31:59,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742395_1572, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:32:00,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742396_1573 src: /127.0.0.1:58147 dest: /127.0.0.1:50010
2015-12-13 13:32:00,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58147, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2103325452_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742396_1573, duration: 2177835
2015-12-13 13:32:00,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742396_1573, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:32:00,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742397_1574 src: /127.0.0.1:58148 dest: /127.0.0.1:50010
2015-12-13 13:32:00,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58148, dest: /127.0.0.1:50010, bytes: 87274, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2103325452_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742397_1574, duration: 12245199
2015-12-13 13:32:00,230 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742397_1574, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:32:05,181 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742396_1573
2015-12-13 13:32:05,182 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742394_1571
2015-12-13 13:32:05,182 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742395_1572
2015-12-13 13:32:06,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742398_1575 src: /127.0.0.1:58156 dest: /127.0.0.1:50010
2015-12-13 13:32:06,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58156, dest: /127.0.0.1:50010, bytes: 103820, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_573764308_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742398_1575, duration: 54534433
2015-12-13 13:32:06,466 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742398_1575, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:32:10,243 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742397_1574
2015-12-13 13:32:15,276 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742398_1575
2015-12-13 13:32:19,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742399_1576 src: /127.0.0.1:58173 dest: /127.0.0.1:50010
2015-12-13 13:32:52,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58173, dest: /127.0.0.1:50010, bytes: 44350, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_573764308_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742399_1576, duration: 33038983823
2015-12-13 13:32:52,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742399_1576, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:32:52,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742400_1577 src: /127.0.0.1:58220 dest: /127.0.0.1:50010
2015-12-13 13:32:52,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58220, dest: /127.0.0.1:50010, bytes: 335, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_573764308_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742400_1577, duration: 5864873
2015-12-13 13:32:52,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742400_1577, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:32:53,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742401_1578 src: /127.0.0.1:58222 dest: /127.0.0.1:50010
2015-12-13 13:32:53,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58222, dest: /127.0.0.1:50010, bytes: 44350, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_573764308_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742401_1578, duration: 14272726
2015-12-13 13:32:53,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742401_1578, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:32:53,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742402_1579 src: /127.0.0.1:58223 dest: /127.0.0.1:50010
2015-12-13 13:32:53,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58223, dest: /127.0.0.1:50010, bytes: 103820, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_573764308_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742402_1579, duration: 8030111
2015-12-13 13:32:53,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742402_1579, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:32:56,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742394_1571 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742394 for deletion
2015-12-13 13:32:56,684 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742395_1572 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742395 for deletion
2015-12-13 13:32:56,684 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742396_1573 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742396 for deletion
2015-12-13 13:32:56,684 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742397_1574 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742397 for deletion
2015-12-13 13:32:56,685 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742398_1575 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742398 for deletion
2015-12-13 13:32:56,685 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742399_1576 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742399 for deletion
2015-12-13 13:32:56,685 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742394_1571 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742394
2015-12-13 13:32:56,685 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742395_1572 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742395
2015-12-13 13:32:56,685 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742396_1573 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742396
2015-12-13 13:32:56,686 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742397_1574 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742397
2015-12-13 13:32:56,686 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742398_1575 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742398
2015-12-13 13:32:56,686 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742399_1576 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742399
2015-12-13 13:33:00,434 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742402_1579
2015-12-13 13:33:00,437 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742400_1577
2015-12-13 13:33:00,443 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742401_1578
2015-12-13 13:33:02,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742403_1580 src: /127.0.0.1:58228 dest: /127.0.0.1:50010
2015-12-13 13:33:02,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58228, dest: /127.0.0.1:50010, bytes: 5217, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-357452811_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742403_1580, duration: 39462657
2015-12-13 13:33:02,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742403_1580, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:33:02,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742404_1581 src: /127.0.0.1:58229 dest: /127.0.0.1:50010
2015-12-13 13:33:02,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58229, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-357452811_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742404_1581, duration: 5001423
2015-12-13 13:33:02,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742404_1581, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:33:02,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742405_1582 src: /127.0.0.1:58230 dest: /127.0.0.1:50010
2015-12-13 13:33:02,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58230, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-357452811_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742405_1582, duration: 2271503
2015-12-13 13:33:02,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742405_1582, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:33:02,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742406_1583 src: /127.0.0.1:58231 dest: /127.0.0.1:50010
2015-12-13 13:33:02,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58231, dest: /127.0.0.1:50010, bytes: 87277, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-357452811_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742406_1583, duration: 24686430
2015-12-13 13:33:02,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742406_1583, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:33:08,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742407_1584 src: /127.0.0.1:58240 dest: /127.0.0.1:50010
2015-12-13 13:33:08,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58240, dest: /127.0.0.1:50010, bytes: 103823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-120064290_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742407_1584, duration: 52416734
2015-12-13 13:33:08,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742407_1584, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:33:10,456 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742405_1582
2015-12-13 13:33:10,457 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742403_1580
2015-12-13 13:33:10,457 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742404_1581
2015-12-13 13:33:10,458 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742406_1583
2015-12-13 13:33:15,491 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742407_1584
2015-12-13 13:33:20,964 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1532ms
No GCs detected
2015-12-13 13:33:23,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742408_1585 src: /127.0.0.1:58257 dest: /127.0.0.1:50010
2015-12-13 13:33:33,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742409_1586 src: /127.0.0.1:58270 dest: /127.0.0.1:50010
2015-12-13 13:33:33,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58270, dest: /127.0.0.1:50010, bytes: 8295, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0010_r_000000_0_1309357671_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742409_1586, duration: 128871008
2015-12-13 13:33:33,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742409_1586, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:33:33,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58257, dest: /127.0.0.1:50010, bytes: 48753, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-120064290_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742408_1585, duration: 10033891727
2015-12-13 13:33:33,845 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742408_1585, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:33:33,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742410_1587 src: /127.0.0.1:58272 dest: /127.0.0.1:50010
2015-12-13 13:33:33,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58272, dest: /127.0.0.1:50010, bytes: 349, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-120064290_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742410_1587, duration: 2716208
2015-12-13 13:33:33,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742410_1587, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:33:33,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742411_1588 src: /127.0.0.1:58274 dest: /127.0.0.1:50010
2015-12-13 13:33:33,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58274, dest: /127.0.0.1:50010, bytes: 48753, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-120064290_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742411_1588, duration: 2032650
2015-12-13 13:33:33,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742411_1588, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:33:33,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742412_1589 src: /127.0.0.1:58275 dest: /127.0.0.1:50010
2015-12-13 13:33:33,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58275, dest: /127.0.0.1:50010, bytes: 103823, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-120064290_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742412_1589, duration: 3538326
2015-12-13 13:33:33,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742412_1589, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 13:33:38,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742403_1580 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742403 for deletion
2015-12-13 13:33:38,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742403_1580 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742403
2015-12-13 13:33:38,674 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742404_1581 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742404 for deletion
2015-12-13 13:33:38,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742405_1582 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742405 for deletion
2015-12-13 13:33:38,675 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742406_1583 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742406 for deletion
2015-12-13 13:33:38,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742404_1581 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742404
2015-12-13 13:33:38,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742405_1582 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742405
2015-12-13 13:33:38,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742406_1583 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742406
2015-12-13 13:33:38,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742407_1584 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742407 for deletion
2015-12-13 13:33:38,676 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742408_1585 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742408 for deletion
2015-12-13 13:33:38,677 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742407_1584 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742407
2015-12-13 13:33:38,677 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742408_1585 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742408
2015-12-13 13:33:40,852 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742411_1588
2015-12-13 13:33:40,853 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742409_1586
2015-12-13 13:33:40,854 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742412_1589
2015-12-13 13:33:40,854 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742410_1587
2015-12-13 14:57:30,621 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742409_1586 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742409 for deletion
2015-12-13 14:57:30,622 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742409_1586 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742409
2015-12-13 14:57:32,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742413_1590 src: /127.0.0.1:58367 dest: /127.0.0.1:50010
2015-12-13 14:57:32,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58367, dest: /127.0.0.1:50010, bytes: 6836, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_565831135_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742413_1590, duration: 48856656
2015-12-13 14:57:32,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742413_1590, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 14:57:32,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742414_1591 src: /127.0.0.1:58368 dest: /127.0.0.1:50010
2015-12-13 14:57:32,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58368, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_565831135_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742414_1591, duration: 1992935
2015-12-13 14:57:32,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742414_1591, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 14:57:32,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742415_1592 src: /127.0.0.1:58369 dest: /127.0.0.1:50010
2015-12-13 14:57:32,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58369, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_565831135_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742415_1592, duration: 2321440
2015-12-13 14:57:32,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742415_1592, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 14:57:32,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742416_1593 src: /127.0.0.1:58370 dest: /127.0.0.1:50010
2015-12-13 14:57:32,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58370, dest: /127.0.0.1:50010, bytes: 87289, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_565831135_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742416_1593, duration: 9411765
2015-12-13 14:57:32,729 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742416_1593, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 14:57:38,543 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742416_1593
2015-12-13 14:57:38,544 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742415_1592
2015-12-13 14:57:38,544 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742414_1591
2015-12-13 14:57:38,545 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742413_1590
2015-12-13 14:57:39,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742417_1594 src: /127.0.0.1:58378 dest: /127.0.0.1:50010
2015-12-13 14:57:39,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58378, dest: /127.0.0.1:50010, bytes: 103835, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1356742496_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742417_1594, duration: 29249002
2015-12-13 14:57:39,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742417_1594, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 14:57:48,636 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742417_1594
2015-12-13 14:57:57,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742418_1595 src: /127.0.0.1:58396 dest: /127.0.0.1:50010
2015-12-13 14:58:04,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742419_1596 src: /127.0.0.1:58402 dest: /127.0.0.1:50010
2015-12-13 14:58:04,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58402, dest: /127.0.0.1:50010, bytes: 8295, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0011_r_000000_0_1195389825_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742419_1596, duration: 85874205
2015-12-13 14:58:04,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742419_1596, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 14:58:06,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58396, dest: /127.0.0.1:50010, bytes: 47118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1356742496_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742418_1595, duration: 8266321362
2015-12-13 14:58:06,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742418_1595, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 14:58:06,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742420_1597 src: /127.0.0.1:58404 dest: /127.0.0.1:50010
2015-12-13 14:58:06,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58404, dest: /127.0.0.1:50010, bytes: 350, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1356742496_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742420_1597, duration: 3268197
2015-12-13 14:58:06,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742420_1597, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 14:58:06,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742421_1598 src: /127.0.0.1:58406 dest: /127.0.0.1:50010
2015-12-13 14:58:06,237 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58406, dest: /127.0.0.1:50010, bytes: 47118, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1356742496_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742421_1598, duration: 7515046
2015-12-13 14:58:06,237 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742421_1598, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 14:58:06,267 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742422_1599 src: /127.0.0.1:58407 dest: /127.0.0.1:50010
2015-12-13 14:58:06,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58407, dest: /127.0.0.1:50010, bytes: 103835, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1356742496_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742422_1599, duration: 61905017
2015-12-13 14:58:06,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742422_1599, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 14:58:09,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742416_1593 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742416 for deletion
2015-12-13 14:58:09,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742417_1594 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742417 for deletion
2015-12-13 14:58:09,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742418_1595 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742418 for deletion
2015-12-13 14:58:09,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742413_1590 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742413 for deletion
2015-12-13 14:58:09,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742414_1591 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742414 for deletion
2015-12-13 14:58:09,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742415_1592 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742415 for deletion
2015-12-13 14:58:09,638 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742416_1593 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742416
2015-12-13 14:58:09,639 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742417_1594 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742417
2015-12-13 14:58:09,639 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742418_1595 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742418
2015-12-13 14:58:09,639 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742413_1590 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742413
2015-12-13 14:58:09,639 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742414_1591 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742414
2015-12-13 14:58:09,639 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742415_1592 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742415
2015-12-13 14:58:13,660 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742421_1598
2015-12-13 14:58:13,661 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742422_1599
2015-12-13 14:58:13,662 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742419_1596
2015-12-13 14:58:13,662 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742420_1597
2015-12-13 16:12:54,797 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742419_1596 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742419 for deletion
2015-12-13 16:12:54,803 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742419_1596 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742419
2015-12-13 16:13:02,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742423_1600 src: /127.0.0.1:58467 dest: /127.0.0.1:50010
2015-12-13 16:13:02,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58467, dest: /127.0.0.1:50010, bytes: 6946, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_325369101_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742423_1600, duration: 48712241
2015-12-13 16:13:02,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742423_1600, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:13:02,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742424_1601 src: /127.0.0.1:58468 dest: /127.0.0.1:50010
2015-12-13 16:13:02,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58468, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_325369101_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742424_1601, duration: 2105123
2015-12-13 16:13:02,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742424_1601, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:13:02,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742425_1602 src: /127.0.0.1:58469 dest: /127.0.0.1:50010
2015-12-13 16:13:02,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58469, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_325369101_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742425_1602, duration: 1975763
2015-12-13 16:13:02,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742425_1602, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:13:03,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742426_1603 src: /127.0.0.1:58470 dest: /127.0.0.1:50010
2015-12-13 16:13:03,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58470, dest: /127.0.0.1:50010, bytes: 87289, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_325369101_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742426_1603, duration: 20578640
2015-12-13 16:13:03,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742426_1603, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:13:10,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742427_1604 src: /127.0.0.1:58478 dest: /127.0.0.1:50010
2015-12-13 16:13:10,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58478, dest: /127.0.0.1:50010, bytes: 103835, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_98842116_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742427_1604, duration: 65630590
2015-12-13 16:13:10,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742427_1604, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:13:11,772 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742426_1603
2015-12-13 16:13:11,773 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742424_1601
2015-12-13 16:13:11,774 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742423_1600
2015-12-13 16:13:11,775 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742425_1602
2015-12-13 16:13:16,935 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742427_1604
2015-12-13 16:13:28,259 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742428_1605 src: /127.0.0.1:58497 dest: /127.0.0.1:50010
2015-12-13 16:13:37,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742429_1606 src: /127.0.0.1:58510 dest: /127.0.0.1:50010
2015-12-13 16:13:38,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58510, dest: /127.0.0.1:50010, bytes: 8295, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0012_r_000000_0_32935543_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742429_1606, duration: 43924094
2015-12-13 16:13:38,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742429_1606, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:13:39,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58497, dest: /127.0.0.1:50010, bytes: 48768, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_98842116_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742428_1605, duration: 11183142564
2015-12-13 16:13:39,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742428_1605, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:13:39,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742430_1607 src: /127.0.0.1:58513 dest: /127.0.0.1:50010
2015-12-13 16:13:39,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58513, dest: /127.0.0.1:50010, bytes: 350, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_98842116_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742430_1607, duration: 2007386
2015-12-13 16:13:39,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742430_1607, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:13:39,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742431_1608 src: /127.0.0.1:58515 dest: /127.0.0.1:50010
2015-12-13 16:13:39,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58515, dest: /127.0.0.1:50010, bytes: 48768, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_98842116_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742431_1608, duration: 4687535
2015-12-13 16:13:39,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742431_1608, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:13:39,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742432_1609 src: /127.0.0.1:58516 dest: /127.0.0.1:50010
2015-12-13 16:13:39,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58516, dest: /127.0.0.1:50010, bytes: 103835, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_98842116_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742432_1609, duration: 11738470
2015-12-13 16:13:39,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742432_1609, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:13:43,464 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742423_1600 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742423 for deletion
2015-12-13 16:13:43,464 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742424_1601 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742424 for deletion
2015-12-13 16:13:43,465 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742425_1602 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742425 for deletion
2015-12-13 16:13:43,465 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742426_1603 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742426 for deletion
2015-12-13 16:13:43,465 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742427_1604 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742427 for deletion
2015-12-13 16:13:43,465 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742428_1605 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742428 for deletion
2015-12-13 16:13:43,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742423_1600 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742423
2015-12-13 16:13:43,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742424_1601 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742424
2015-12-13 16:13:43,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742425_1602 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742425
2015-12-13 16:13:43,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742426_1603 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742426
2015-12-13 16:13:43,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742427_1604 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742427
2015-12-13 16:13:43,467 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742428_1605 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742428
2015-12-13 16:13:46,962 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742430_1607
2015-12-13 16:13:46,963 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742432_1609
2015-12-13 16:13:46,964 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742431_1608
2015-12-13 16:13:46,964 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742429_1606
2015-12-13 16:27:04,586 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742429_1606 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742429 for deletion
2015-12-13 16:27:04,586 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742429_1606 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742429
2015-12-13 16:27:05,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742433_1610 src: /127.0.0.1:58543 dest: /127.0.0.1:50010
2015-12-13 16:27:05,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58543, dest: /127.0.0.1:50010, bytes: 6959, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-768707732_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742433_1610, duration: 43267956
2015-12-13 16:27:05,733 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742433_1610, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:27:05,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742434_1611 src: /127.0.0.1:58544 dest: /127.0.0.1:50010
2015-12-13 16:27:05,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58544, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-768707732_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742434_1611, duration: 1968156
2015-12-13 16:27:05,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742434_1611, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:27:05,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742435_1612 src: /127.0.0.1:58545 dest: /127.0.0.1:50010
2015-12-13 16:27:05,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58545, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-768707732_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742435_1612, duration: 1953675
2015-12-13 16:27:05,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742435_1612, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:27:06,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742436_1613 src: /127.0.0.1:58546 dest: /127.0.0.1:50010
2015-12-13 16:27:06,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58546, dest: /127.0.0.1:50010, bytes: 87289, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-768707732_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742436_1613, duration: 14350323
2015-12-13 16:27:06,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742436_1613, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:27:11,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742437_1614 src: /127.0.0.1:58554 dest: /127.0.0.1:50010
2015-12-13 16:27:11,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58554, dest: /127.0.0.1:50010, bytes: 103835, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2002467459_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742437_1614, duration: 97532395
2015-12-13 16:27:11,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742437_1614, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:27:12,073 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742436_1613
2015-12-13 16:27:12,074 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742434_1611
2015-12-13 16:27:12,074 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742433_1610
2015-12-13 16:27:12,075 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742435_1612
2015-12-13 16:27:17,182 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742437_1614
2015-12-13 16:27:25,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742438_1615 src: /127.0.0.1:58572 dest: /127.0.0.1:50010
2015-12-13 16:27:35,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742439_1616 src: /127.0.0.1:58583 dest: /127.0.0.1:50010
2015-12-13 16:27:35,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58583, dest: /127.0.0.1:50010, bytes: 8295, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0013_r_000000_0_-1332557209_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742439_1616, duration: 83331201
2015-12-13 16:27:35,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742439_1616, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:27:37,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58572, dest: /127.0.0.1:50010, bytes: 48756, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2002467459_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742438_1615, duration: 11285470361
2015-12-13 16:27:37,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742438_1615, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:27:37,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742440_1617 src: /127.0.0.1:58586 dest: /127.0.0.1:50010
2015-12-13 16:27:37,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58586, dest: /127.0.0.1:50010, bytes: 350, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2002467459_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742440_1617, duration: 2188327
2015-12-13 16:27:37,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742440_1617, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:27:37,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742441_1618 src: /127.0.0.1:58588 dest: /127.0.0.1:50010
2015-12-13 16:27:37,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58588, dest: /127.0.0.1:50010, bytes: 48756, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2002467459_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742441_1618, duration: 3459432
2015-12-13 16:27:37,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742441_1618, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:27:37,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742442_1619 src: /127.0.0.1:58589 dest: /127.0.0.1:50010
2015-12-13 16:27:37,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58589, dest: /127.0.0.1:50010, bytes: 103835, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2002467459_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742442_1619, duration: 14969992
2015-12-13 16:27:37,245 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742442_1619, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:27:40,604 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742433_1610 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742433 for deletion
2015-12-13 16:27:40,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742434_1611 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742434 for deletion
2015-12-13 16:27:40,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742435_1612 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742435 for deletion
2015-12-13 16:27:40,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742436_1613 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742436 for deletion
2015-12-13 16:27:40,605 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742437_1614 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742437 for deletion
2015-12-13 16:27:40,606 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742438_1615 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742438 for deletion
2015-12-13 16:27:40,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742433_1610 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742433
2015-12-13 16:27:40,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742434_1611 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742434
2015-12-13 16:27:40,608 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742435_1612 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742435
2015-12-13 16:27:40,608 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742436_1613 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742436
2015-12-13 16:27:40,608 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742437_1614 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742437
2015-12-13 16:27:40,609 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742438_1615 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742438
2015-12-13 16:27:42,242 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742439_1616
2015-12-13 16:27:42,242 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742440_1617
2015-12-13 16:27:42,243 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742441_1618
2015-12-13 16:27:47,263 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742442_1619
2015-12-13 16:29:34,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742439_1616 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742439 for deletion
2015-12-13 16:29:34,608 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742439_1616 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742439
2015-12-13 16:29:36,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742443_1620 src: /127.0.0.1:58604 dest: /127.0.0.1:50010
2015-12-13 16:29:37,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58604, dest: /127.0.0.1:50010, bytes: 6961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_175554112_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742443_1620, duration: 59011440
2015-12-13 16:29:37,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742443_1620, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:29:37,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742444_1621 src: /127.0.0.1:58605 dest: /127.0.0.1:50010
2015-12-13 16:29:37,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58605, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_175554112_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742444_1621, duration: 3106415
2015-12-13 16:29:37,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742444_1621, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:29:37,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742445_1622 src: /127.0.0.1:58606 dest: /127.0.0.1:50010
2015-12-13 16:29:37,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58606, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_175554112_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742445_1622, duration: 1794219
2015-12-13 16:29:37,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742445_1622, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:29:37,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742446_1623 src: /127.0.0.1:58607 dest: /127.0.0.1:50010
2015-12-13 16:29:37,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58607, dest: /127.0.0.1:50010, bytes: 87289, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_175554112_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742446_1623, duration: 8325972
2015-12-13 16:29:37,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742446_1623, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:29:42,285 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742445_1622
2015-12-13 16:29:42,285 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742443_1620
2015-12-13 16:29:42,286 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742444_1621
2015-12-13 16:29:44,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742447_1624 src: /127.0.0.1:58615 dest: /127.0.0.1:50010
2015-12-13 16:29:44,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58615, dest: /127.0.0.1:50010, bytes: 103835, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-797443036_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742447_1624, duration: 37886648
2015-12-13 16:29:44,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742447_1624, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:29:47,306 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742446_1623
2015-12-13 16:29:52,340 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742447_1624
2015-12-13 16:29:57,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742448_1625 src: /127.0.0.1:58633 dest: /127.0.0.1:50010
2015-12-13 16:30:18,313 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742449_1626 src: /127.0.0.1:58657 dest: /127.0.0.1:50010
2015-12-13 16:30:18,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58657, dest: /127.0.0.1:50010, bytes: 8295, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0014_r_000000_0_-229086529_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742449_1626, duration: 69395262
2015-12-13 16:30:18,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742449_1626, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:30:19,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58633, dest: /127.0.0.1:50010, bytes: 51811, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-797443036_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742448_1625, duration: 22053014322
2015-12-13 16:30:19,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742448_1625, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:30:19,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742450_1627 src: /127.0.0.1:58659 dest: /127.0.0.1:50010
2015-12-13 16:30:19,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58659, dest: /127.0.0.1:50010, bytes: 351, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-797443036_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742450_1627, duration: 1634038
2015-12-13 16:30:19,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742450_1627, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:30:20,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742451_1628 src: /127.0.0.1:58661 dest: /127.0.0.1:50010
2015-12-13 16:30:20,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58661, dest: /127.0.0.1:50010, bytes: 51811, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-797443036_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742451_1628, duration: 7484111
2015-12-13 16:30:20,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742451_1628, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:30:20,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742452_1629 src: /127.0.0.1:58662 dest: /127.0.0.1:50010
2015-12-13 16:30:20,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58662, dest: /127.0.0.1:50010, bytes: 103835, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-797443036_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742452_1629, duration: 8148978
2015-12-13 16:30:20,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742452_1629, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:30:22,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742453_1630 src: /127.0.0.1:58665 dest: /127.0.0.1:50010
2015-12-13 16:30:22,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58665, dest: /127.0.0.1:50010, bytes: 6961, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_175554112_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742453_1630, duration: 6613442
2015-12-13 16:30:22,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742453_1630, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:30:22,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742454_1631 src: /127.0.0.1:58666 dest: /127.0.0.1:50010
2015-12-13 16:30:22,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58666, dest: /127.0.0.1:50010, bytes: 151, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_175554112_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742454_1631, duration: 11565832
2015-12-13 16:30:22,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742454_1631, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:30:22,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742455_1632 src: /127.0.0.1:58667 dest: /127.0.0.1:50010
2015-12-13 16:30:22,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58667, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_175554112_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742455_1632, duration: 2679491
2015-12-13 16:30:22,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742455_1632, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:30:22,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742456_1633 src: /127.0.0.1:58668 dest: /127.0.0.1:50010
2015-12-13 16:30:22,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58668, dest: /127.0.0.1:50010, bytes: 87156, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_175554112_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742456_1633, duration: 3771475
2015-12-13 16:30:22,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742456_1633, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:30:25,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742448_1625 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742448 for deletion
2015-12-13 16:30:25,635 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742443_1620 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742443 for deletion
2015-12-13 16:30:25,635 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742444_1621 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742444 for deletion
2015-12-13 16:30:25,635 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742445_1622 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742445 for deletion
2015-12-13 16:30:25,635 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742446_1623 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742446 for deletion
2015-12-13 16:30:25,635 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742447_1624 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742447 for deletion
2015-12-13 16:30:25,636 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742448_1625 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742448
2015-12-13 16:30:25,636 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742443_1620 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742443
2015-12-13 16:30:25,636 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742444_1621 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742444
2015-12-13 16:30:25,636 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742445_1622 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742445
2015-12-13 16:30:25,636 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742446_1623 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742446
2015-12-13 16:30:25,636 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742447_1624 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742447
2015-12-13 16:30:27,432 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742449_1626
2015-12-13 16:30:27,433 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742452_1629
2015-12-13 16:30:27,434 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742450_1627
2015-12-13 16:30:27,435 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742451_1628
2015-12-13 16:30:32,456 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742455_1632
2015-12-13 16:30:32,456 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742454_1631
2015-12-13 16:30:32,459 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742456_1633
2015-12-13 16:30:32,459 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742453_1630
2015-12-13 16:30:33,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742457_1634 src: /127.0.0.1:58678 dest: /127.0.0.1:50010
2015-12-13 16:30:33,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58678, dest: /127.0.0.1:50010, bytes: 103678, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-723917045_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742457_1634, duration: 55688239
2015-12-13 16:30:33,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742457_1634, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:30:38,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742458_1635 src: /127.0.0.1:58684 dest: /127.0.0.1:50010
2015-12-13 16:30:42,520 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742457_1634
2015-12-13 16:30:55,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58684, dest: /127.0.0.1:50010, bytes: 22455, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-723917045_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742458_1635, duration: 16962933519
2015-12-13 16:30:55,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742458_1635, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:30:55,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742459_1636 src: /127.0.0.1:58700 dest: /127.0.0.1:50010
2015-12-13 16:30:55,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58700, dest: /127.0.0.1:50010, bytes: 335, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-723917045_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742459_1636, duration: 1946901
2015-12-13 16:30:55,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742459_1636, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:30:55,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742460_1637 src: /127.0.0.1:58702 dest: /127.0.0.1:50010
2015-12-13 16:30:55,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58702, dest: /127.0.0.1:50010, bytes: 22455, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-723917045_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742460_1637, duration: 1881257
2015-12-13 16:30:55,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742460_1637, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:30:55,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742461_1638 src: /127.0.0.1:58703 dest: /127.0.0.1:50010
2015-12-13 16:30:55,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58703, dest: /127.0.0.1:50010, bytes: 103678, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-723917045_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742461_1638, duration: 9636985
2015-12-13 16:30:55,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742461_1638, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:30:58,633 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742453_1630 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742453 for deletion
2015-12-13 16:30:58,633 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742454_1631 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742454 for deletion
2015-12-13 16:30:58,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742455_1632 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742455 for deletion
2015-12-13 16:30:58,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742456_1633 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742456 for deletion
2015-12-13 16:30:58,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742453_1630 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742453
2015-12-13 16:30:58,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742457_1634 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742457 for deletion
2015-12-13 16:30:58,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742454_1631 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742454
2015-12-13 16:30:58,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742458_1635 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742458 for deletion
2015-12-13 16:30:58,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742455_1632 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742455
2015-12-13 16:30:58,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742456_1633 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742456
2015-12-13 16:30:58,635 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742457_1634 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742457
2015-12-13 16:30:58,635 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742458_1635 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742458
2015-12-13 16:31:02,541 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742459_1636
2015-12-13 16:31:02,543 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742461_1638
2015-12-13 16:31:02,543 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742460_1637
2015-12-13 16:35:55,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742449_1626 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742449 for deletion
2015-12-13 16:35:55,673 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742449_1626 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742449
2015-12-13 16:35:56,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742462_1639 src: /127.0.0.1:58714 dest: /127.0.0.1:50010
2015-12-13 16:35:56,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58714, dest: /127.0.0.1:50010, bytes: 7002, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1463941228_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742462_1639, duration: 40918496
2015-12-13 16:35:56,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742462_1639, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:35:56,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742463_1640 src: /127.0.0.1:58715 dest: /127.0.0.1:50010
2015-12-13 16:35:56,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58715, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1463941228_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742463_1640, duration: 1674535
2015-12-13 16:35:56,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742463_1640, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:35:56,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742464_1641 src: /127.0.0.1:58716 dest: /127.0.0.1:50010
2015-12-13 16:35:56,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58716, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1463941228_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742464_1641, duration: 1943685
2015-12-13 16:35:56,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742464_1641, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:35:56,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742465_1642 src: /127.0.0.1:58718 dest: /127.0.0.1:50010
2015-12-13 16:35:56,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58718, dest: /127.0.0.1:50010, bytes: 87289, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1463941228_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742465_1642, duration: 11811607
2015-12-13 16:35:56,593 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742465_1642, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:36:02,596 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742463_1640
2015-12-13 16:36:02,597 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742464_1641
2015-12-13 16:36:02,597 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742462_1639
2015-12-13 16:36:02,599 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742465_1642
2015-12-13 16:36:02,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742466_1643 src: /127.0.0.1:58726 dest: /127.0.0.1:50010
2015-12-13 16:36:02,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58726, dest: /127.0.0.1:50010, bytes: 103835, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_983214958_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742466_1643, duration: 37444185
2015-12-13 16:36:02,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742466_1643, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:36:12,658 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742466_1643
2015-12-13 16:36:16,435 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1664ms
No GCs detected
2015-12-13 16:36:19,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742467_1644 src: /127.0.0.1:58746 dest: /127.0.0.1:50010
2015-12-13 16:36:28,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742468_1645 src: /127.0.0.1:58757 dest: /127.0.0.1:50010
2015-12-13 16:36:29,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58757, dest: /127.0.0.1:50010, bytes: 8295, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0016_r_000000_0_-923351591_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742468_1645, duration: 55113696
2015-12-13 16:36:29,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742468_1645, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:36:29,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58746, dest: /127.0.0.1:50010, bytes: 48756, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_983214958_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742467_1644, duration: 10441605679
2015-12-13 16:36:29,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742467_1644, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:36:29,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742469_1646 src: /127.0.0.1:58760 dest: /127.0.0.1:50010
2015-12-13 16:36:29,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58760, dest: /127.0.0.1:50010, bytes: 350, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_983214958_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742469_1646, duration: 2687202
2015-12-13 16:36:29,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742469_1646, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:36:29,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742470_1647 src: /127.0.0.1:58762 dest: /127.0.0.1:50010
2015-12-13 16:36:29,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58762, dest: /127.0.0.1:50010, bytes: 48756, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_983214958_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742470_1647, duration: 9413156
2015-12-13 16:36:29,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742470_1647, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:36:29,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742471_1648 src: /127.0.0.1:58763 dest: /127.0.0.1:50010
2015-12-13 16:36:29,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58763, dest: /127.0.0.1:50010, bytes: 103835, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_983214958_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742471_1648, duration: 4439297
2015-12-13 16:36:29,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742471_1648, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:36:31,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742464_1641 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742464 for deletion
2015-12-13 16:36:31,679 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742465_1642 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742465 for deletion
2015-12-13 16:36:31,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742466_1643 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742466 for deletion
2015-12-13 16:36:31,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742467_1644 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742467 for deletion
2015-12-13 16:36:31,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742462_1639 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742462 for deletion
2015-12-13 16:36:31,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742463_1640 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742463 for deletion
2015-12-13 16:36:31,682 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742464_1641 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742464
2015-12-13 16:36:31,682 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742465_1642 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742465
2015-12-13 16:36:31,682 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742466_1643 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742466
2015-12-13 16:36:31,682 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742467_1644 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742467
2015-12-13 16:36:31,683 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742462_1639 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742462
2015-12-13 16:36:31,684 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742463_1640 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742463
2015-12-13 16:36:31,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742472_1649 src: /127.0.0.1:58766 dest: /127.0.0.1:50010
2015-12-13 16:36:31,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58766, dest: /127.0.0.1:50010, bytes: 7002, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1463941228_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742472_1649, duration: 15647687
2015-12-13 16:36:31,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742472_1649, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:36:31,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742473_1650 src: /127.0.0.1:58767 dest: /127.0.0.1:50010
2015-12-13 16:36:31,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58767, dest: /127.0.0.1:50010, bytes: 151, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1463941228_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742473_1650, duration: 8694220
2015-12-13 16:36:31,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742473_1650, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:36:31,997 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742474_1651 src: /127.0.0.1:58768 dest: /127.0.0.1:50010
2015-12-13 16:36:32,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58768, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1463941228_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742474_1651, duration: 3072637
2015-12-13 16:36:32,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742474_1651, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:36:32,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742475_1652 src: /127.0.0.1:58769 dest: /127.0.0.1:50010
2015-12-13 16:36:32,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58769, dest: /127.0.0.1:50010, bytes: 87156, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1463941228_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742475_1652, duration: 4678314
2015-12-13 16:36:32,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742475_1652, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:36:37,678 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742475_1652
2015-12-13 16:36:37,678 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742470_1647
2015-12-13 16:36:37,679 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742468_1645
2015-12-13 16:36:37,679 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742474_1651
2015-12-13 16:36:42,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742476_1653 src: /127.0.0.1:58778 dest: /127.0.0.1:50010
2015-12-13 16:36:42,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58778, dest: /127.0.0.1:50010, bytes: 103678, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_837182434_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742476_1653, duration: 31492967
2015-12-13 16:36:42,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742476_1653, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:36:47,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742477_1654 src: /127.0.0.1:58785 dest: /127.0.0.1:50010
2015-12-13 16:36:47,702 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742476_1653
2015-12-13 16:36:54,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742478_1655 src: /127.0.0.1:58790 dest: /127.0.0.1:50010
2015-12-13 16:36:54,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58790, dest: /127.0.0.1:50010, bytes: 8963, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0017_r_000000_0_-70954622_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742478_1655, duration: 82225435
2015-12-13 16:36:54,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742478_1655, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:36:54,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58785, dest: /127.0.0.1:50010, bytes: 33371, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_837182434_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742477_1654, duration: 6925150694
2015-12-13 16:36:54,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742477_1654, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:36:54,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742479_1656 src: /127.0.0.1:58792 dest: /127.0.0.1:50010
2015-12-13 16:36:54,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58792, dest: /127.0.0.1:50010, bytes: 349, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_837182434_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742479_1656, duration: 1931882
2015-12-13 16:36:54,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742479_1656, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:36:54,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742480_1657 src: /127.0.0.1:58794 dest: /127.0.0.1:50010
2015-12-13 16:36:54,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58794, dest: /127.0.0.1:50010, bytes: 33371, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_837182434_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742480_1657, duration: 1828636
2015-12-13 16:36:54,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742480_1657, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:36:54,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742481_1658 src: /127.0.0.1:58795 dest: /127.0.0.1:50010
2015-12-13 16:36:54,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58795, dest: /127.0.0.1:50010, bytes: 103678, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_837182434_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742481_1658, duration: 1952922
2015-12-13 16:36:54,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742481_1658, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:36:58,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742472_1649 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742472 for deletion
2015-12-13 16:36:58,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742472_1649 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742472
2015-12-13 16:36:58,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742473_1650 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742473 for deletion
2015-12-13 16:36:58,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742473_1650 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742473
2015-12-13 16:36:58,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742474_1651 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742474 for deletion
2015-12-13 16:36:58,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742474_1651 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742474
2015-12-13 16:36:58,680 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742475_1652 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742475 for deletion
2015-12-13 16:36:58,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742475_1652 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742475
2015-12-13 16:36:58,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742476_1653 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742476 for deletion
2015-12-13 16:36:58,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742476_1653 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742476
2015-12-13 16:36:58,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742477_1654 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742477 for deletion
2015-12-13 16:36:58,681 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742477_1654 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742477
2015-12-13 16:37:02,730 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742480_1657
2015-12-13 16:37:02,731 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742478_1655
2015-12-13 16:37:02,731 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742479_1656
2015-12-13 16:37:02,732 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742481_1658
2015-12-13 16:38:58,686 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742478_1655 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742478 for deletion
2015-12-13 16:38:58,687 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742478_1655 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742478
2015-12-13 16:39:01,686 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742468_1645 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742468 for deletion
2015-12-13 16:39:01,686 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742468_1645 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742468
2015-12-13 16:39:04,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742482_1659 src: /127.0.0.1:58806 dest: /127.0.0.1:50010
2015-12-13 16:39:04,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58806, dest: /127.0.0.1:50010, bytes: 7002, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1623357415_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742482_1659, duration: 40609355
2015-12-13 16:39:04,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742482_1659, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:39:04,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742483_1660 src: /127.0.0.1:58807 dest: /127.0.0.1:50010
2015-12-13 16:39:04,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58807, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1623357415_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742483_1660, duration: 1882880
2015-12-13 16:39:04,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742483_1660, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:39:04,795 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742484_1661 src: /127.0.0.1:58808 dest: /127.0.0.1:50010
2015-12-13 16:39:04,798 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58808, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1623357415_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742484_1661, duration: 2038563
2015-12-13 16:39:04,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742484_1661, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:39:04,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742485_1662 src: /127.0.0.1:58809 dest: /127.0.0.1:50010
2015-12-13 16:39:05,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58809, dest: /127.0.0.1:50010, bytes: 87289, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1623357415_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742485_1662, duration: 24619231
2015-12-13 16:39:05,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742485_1662, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:39:10,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742486_1663 src: /127.0.0.1:58818 dest: /127.0.0.1:50010
2015-12-13 16:39:10,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58818, dest: /127.0.0.1:50010, bytes: 103835, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_58113917_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742486_1663, duration: 30725534
2015-12-13 16:39:10,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742486_1663, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:39:12,933 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742482_1659
2015-12-13 16:39:12,934 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742485_1662
2015-12-13 16:39:12,934 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742484_1661
2015-12-13 16:39:12,935 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742483_1660
2015-12-13 16:39:17,944 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742486_1663
2015-12-13 16:39:27,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742487_1664 src: /127.0.0.1:58835 dest: /127.0.0.1:50010
2015-12-13 16:39:37,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742488_1665 src: /127.0.0.1:58846 dest: /127.0.0.1:50010
2015-12-13 16:39:37,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58846, dest: /127.0.0.1:50010, bytes: 8295, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0018_r_000000_0_1082447436_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742488_1665, duration: 61589579
2015-12-13 16:39:37,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742488_1665, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:39:37,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58835, dest: /127.0.0.1:50010, bytes: 48752, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_58113917_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742487_1664, duration: 10139614895
2015-12-13 16:39:37,679 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742487_1664, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:39:37,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742489_1666 src: /127.0.0.1:58849 dest: /127.0.0.1:50010
2015-12-13 16:39:37,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58849, dest: /127.0.0.1:50010, bytes: 350, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_58113917_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742489_1666, duration: 8863092
2015-12-13 16:39:37,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742489_1666, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:39:37,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742490_1667 src: /127.0.0.1:58851 dest: /127.0.0.1:50010
2015-12-13 16:39:37,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58851, dest: /127.0.0.1:50010, bytes: 48752, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_58113917_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742490_1667, duration: 3423511
2015-12-13 16:39:37,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742490_1667, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:39:37,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742491_1668 src: /127.0.0.1:58852 dest: /127.0.0.1:50010
2015-12-13 16:39:37,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58852, dest: /127.0.0.1:50010, bytes: 103835, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_58113917_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742491_1668, duration: 3509123
2015-12-13 16:39:37,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742491_1668, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:39:39,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742492_1669 src: /127.0.0.1:58855 dest: /127.0.0.1:50010
2015-12-13 16:39:39,838 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58855, dest: /127.0.0.1:50010, bytes: 7002, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1623357415_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742492_1669, duration: 7536847
2015-12-13 16:39:39,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742492_1669, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:39:39,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742493_1670 src: /127.0.0.1:58856 dest: /127.0.0.1:50010
2015-12-13 16:39:39,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58856, dest: /127.0.0.1:50010, bytes: 151, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1623357415_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742493_1670, duration: 9450016
2015-12-13 16:39:39,895 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742493_1670, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:39:39,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742494_1671 src: /127.0.0.1:58857 dest: /127.0.0.1:50010
2015-12-13 16:39:39,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58857, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1623357415_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742494_1671, duration: 6849887
2015-12-13 16:39:39,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742494_1671, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:39:40,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742495_1672 src: /127.0.0.1:58858 dest: /127.0.0.1:50010
2015-12-13 16:39:40,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58858, dest: /127.0.0.1:50010, bytes: 87156, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1623357415_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742495_1672, duration: 5489215
2015-12-13 16:39:40,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742495_1672, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:39:40,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742482_1659 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742482 for deletion
2015-12-13 16:39:40,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742483_1660 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742483 for deletion
2015-12-13 16:39:40,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742484_1661 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742484 for deletion
2015-12-13 16:39:40,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742482_1659 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742482
2015-12-13 16:39:40,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742485_1662 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742485 for deletion
2015-12-13 16:39:40,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742483_1660 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742483
2015-12-13 16:39:40,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742486_1663 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742486 for deletion
2015-12-13 16:39:40,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742484_1661 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742484
2015-12-13 16:39:40,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742487_1664 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742487 for deletion
2015-12-13 16:39:40,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742485_1662 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742485
2015-12-13 16:39:40,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742486_1663 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742486
2015-12-13 16:39:40,806 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742487_1664 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742487
2015-12-13 16:39:43,442 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742491_1668
2015-12-13 16:39:43,443 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742490_1667
2015-12-13 16:39:43,444 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742488_1665
2015-12-13 16:39:43,444 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742489_1666
2015-12-13 16:39:48,455 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742495_1672
2015-12-13 16:39:48,456 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742494_1671
2015-12-13 16:39:48,456 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742493_1670
2015-12-13 16:39:48,457 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742492_1669
2015-12-13 16:39:50,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742496_1673 src: /127.0.0.1:58868 dest: /127.0.0.1:50010
2015-12-13 16:39:50,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58868, dest: /127.0.0.1:50010, bytes: 103678, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-840323409_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742496_1673, duration: 69244998
2015-12-13 16:39:50,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742496_1673, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:39:55,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742497_1674 src: /127.0.0.1:58874 dest: /127.0.0.1:50010
2015-12-13 16:39:58,473 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742496_1673
2015-12-13 16:40:02,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742498_1675 src: /127.0.0.1:58880 dest: /127.0.0.1:50010
2015-12-13 16:40:02,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58880, dest: /127.0.0.1:50010, bytes: 8295, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0019_r_000000_0_-1640173744_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742498_1675, duration: 66536626
2015-12-13 16:40:02,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742498_1675, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:40:02,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58874, dest: /127.0.0.1:50010, bytes: 33359, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-840323409_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742497_1674, duration: 7302357040
2015-12-13 16:40:02,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742497_1674, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:40:02,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742499_1676 src: /127.0.0.1:58882 dest: /127.0.0.1:50010
2015-12-13 16:40:02,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58882, dest: /127.0.0.1:50010, bytes: 349, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-840323409_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742499_1676, duration: 1541238
2015-12-13 16:40:02,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742499_1676, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:40:02,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742500_1677 src: /127.0.0.1:58884 dest: /127.0.0.1:50010
2015-12-13 16:40:02,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58884, dest: /127.0.0.1:50010, bytes: 33359, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-840323409_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742500_1677, duration: 1447578
2015-12-13 16:40:02,979 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742500_1677, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:40:03,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742501_1678 src: /127.0.0.1:58885 dest: /127.0.0.1:50010
2015-12-13 16:40:03,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58885, dest: /127.0.0.1:50010, bytes: 103678, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-840323409_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742501_1678, duration: 1559425
2015-12-13 16:40:03,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742501_1678, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:40:04,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742496_1673 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742496 for deletion
2015-12-13 16:40:04,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742497_1674 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742497 for deletion
2015-12-13 16:40:04,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742492_1669 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742492 for deletion
2015-12-13 16:40:04,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742496_1673 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742496
2015-12-13 16:40:04,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742493_1670 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742493 for deletion
2015-12-13 16:40:04,804 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742494_1671 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742494 for deletion
2015-12-13 16:40:04,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742497_1674 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742497
2015-12-13 16:40:04,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742495_1672 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742495 for deletion
2015-12-13 16:40:04,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742492_1669 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742492
2015-12-13 16:40:04,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742493_1670 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742493
2015-12-13 16:40:04,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742494_1671 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742494
2015-12-13 16:40:04,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742495_1672 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742495
2015-12-13 16:40:08,482 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742499_1676
2015-12-13 16:40:08,483 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742498_1675
2015-12-13 16:40:08,485 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742501_1678
2015-12-13 16:40:08,485 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742500_1677
2015-12-13 16:47:04,860 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742498_1675 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742498 for deletion
2015-12-13 16:47:04,861 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742498_1675 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742498
2015-12-13 16:47:07,862 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742488_1665 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742488 for deletion
2015-12-13 16:47:07,862 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742488_1665 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742488
2015-12-13 16:50:22,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742502_1679 src: /127.0.0.1:58910 dest: /127.0.0.1:50010
2015-12-13 16:50:22,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58910, dest: /127.0.0.1:50010, bytes: 7236, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988857330_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742502_1679, duration: 62980632
2015-12-13 16:50:22,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742502_1679, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:50:22,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742503_1680 src: /127.0.0.1:58911 dest: /127.0.0.1:50010
2015-12-13 16:50:22,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58911, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988857330_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742503_1680, duration: 1982412
2015-12-13 16:50:22,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742503_1680, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:50:22,278 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742504_1681 src: /127.0.0.1:58912 dest: /127.0.0.1:50010
2015-12-13 16:50:22,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58912, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988857330_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742504_1681, duration: 1400758
2015-12-13 16:50:22,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742504_1681, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:50:22,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742505_1682 src: /127.0.0.1:58913 dest: /127.0.0.1:50010
2015-12-13 16:50:22,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58913, dest: /127.0.0.1:50010, bytes: 87283, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988857330_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742505_1682, duration: 14079927
2015-12-13 16:50:22,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742505_1682, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:50:29,065 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742504_1681
2015-12-13 16:50:29,066 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742503_1680
2015-12-13 16:50:29,067 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742505_1682
2015-12-13 16:50:29,068 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742502_1679
2015-12-13 16:50:29,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742506_1683 src: /127.0.0.1:58922 dest: /127.0.0.1:50010
2015-12-13 16:50:29,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58922, dest: /127.0.0.1:50010, bytes: 103829, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1581314340_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742506_1683, duration: 33060795
2015-12-13 16:50:29,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742506_1683, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:50:39,074 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742506_1683
2015-12-13 16:50:44,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742507_1684 src: /127.0.0.1:58939 dest: /127.0.0.1:50010
2015-12-13 16:50:53,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742508_1685 src: /127.0.0.1:58950 dest: /127.0.0.1:50010
2015-12-13 16:50:53,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58950, dest: /127.0.0.1:50010, bytes: 7900, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0021_r_000000_0_1415239889_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742508_1685, duration: 54501066
2015-12-13 16:50:53,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742508_1685, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:50:54,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58939, dest: /127.0.0.1:50010, bytes: 48767, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1581314340_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742507_1684, duration: 10454260291
2015-12-13 16:50:54,720 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742507_1684, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:50:54,743 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742509_1686 src: /127.0.0.1:58953 dest: /127.0.0.1:50010
2015-12-13 16:50:54,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58953, dest: /127.0.0.1:50010, bytes: 350, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1581314340_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742509_1686, duration: 2141816
2015-12-13 16:50:54,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742509_1686, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:50:54,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742510_1687 src: /127.0.0.1:58955 dest: /127.0.0.1:50010
2015-12-13 16:50:54,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58955, dest: /127.0.0.1:50010, bytes: 48767, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1581314340_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742510_1687, duration: 6791859
2015-12-13 16:50:54,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742510_1687, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:50:54,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742511_1688 src: /127.0.0.1:58956 dest: /127.0.0.1:50010
2015-12-13 16:50:54,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58956, dest: /127.0.0.1:50010, bytes: 103829, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1581314340_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742511_1688, duration: 7620003
2015-12-13 16:50:54,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742511_1688, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:50:57,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742512_1689 src: /127.0.0.1:58959 dest: /127.0.0.1:50010
2015-12-13 16:50:57,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58959, dest: /127.0.0.1:50010, bytes: 7236, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988857330_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742512_1689, duration: 6276172
2015-12-13 16:50:57,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742512_1689, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:50:57,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742513_1690 src: /127.0.0.1:58960 dest: /127.0.0.1:50010
2015-12-13 16:50:57,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58960, dest: /127.0.0.1:50010, bytes: 151, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988857330_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742513_1690, duration: 2428686
2015-12-13 16:50:57,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742513_1690, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:50:57,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742514_1691 src: /127.0.0.1:58961 dest: /127.0.0.1:50010
2015-12-13 16:50:57,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58961, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988857330_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742514_1691, duration: 1436497
2015-12-13 16:50:57,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742514_1691, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:50:57,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742515_1692 src: /127.0.0.1:58962 dest: /127.0.0.1:50010
2015-12-13 16:50:57,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58962, dest: /127.0.0.1:50010, bytes: 87156, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1988857330_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742515_1692, duration: 12567218
2015-12-13 16:50:57,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742515_1692, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:50:59,294 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742508_1685
2015-12-13 16:50:59,556 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742502_1679 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742502 for deletion
2015-12-13 16:50:59,556 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742503_1680 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742503 for deletion
2015-12-13 16:50:59,556 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742504_1681 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742504 for deletion
2015-12-13 16:50:59,556 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742502_1679 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742502
2015-12-13 16:50:59,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742505_1682 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742505 for deletion
2015-12-13 16:50:59,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742503_1680 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742503
2015-12-13 16:50:59,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742506_1683 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742506 for deletion
2015-12-13 16:50:59,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742504_1681 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742504
2015-12-13 16:50:59,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742507_1684 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742507 for deletion
2015-12-13 16:50:59,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742505_1682 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742505
2015-12-13 16:50:59,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742506_1683 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742506
2015-12-13 16:50:59,557 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742507_1684 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742507
2015-12-13 16:51:04,298 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742512_1689
2015-12-13 16:51:04,299 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742509_1686
2015-12-13 16:51:04,300 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742515_1692
2015-12-13 16:51:04,301 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742510_1687
2015-12-13 16:51:07,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742516_1693 src: /127.0.0.1:58972 dest: /127.0.0.1:50010
2015-12-13 16:51:07,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58972, dest: /127.0.0.1:50010, bytes: 103678, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-816585658_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742516_1693, duration: 43618949
2015-12-13 16:51:07,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742516_1693, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:51:12,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742517_1694 src: /127.0.0.1:58979 dest: /127.0.0.1:50010
2015-12-13 16:51:14,318 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742516_1693
2015-12-13 16:51:19,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742518_1695 src: /127.0.0.1:58984 dest: /127.0.0.1:50010
2015-12-13 16:51:19,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58984, dest: /127.0.0.1:50010, bytes: 8234, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0022_r_000000_0_1196280041_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742518_1695, duration: 77082253
2015-12-13 16:51:19,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742518_1695, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:51:19,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58979, dest: /127.0.0.1:50010, bytes: 33358, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-816585658_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742517_1694, duration: 7204402171
2015-12-13 16:51:19,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742517_1694, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:51:19,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742519_1696 src: /127.0.0.1:58986 dest: /127.0.0.1:50010
2015-12-13 16:51:19,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58986, dest: /127.0.0.1:50010, bytes: 349, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-816585658_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742519_1696, duration: 1326530
2015-12-13 16:51:19,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742519_1696, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:51:19,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742520_1697 src: /127.0.0.1:58988 dest: /127.0.0.1:50010
2015-12-13 16:51:19,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58988, dest: /127.0.0.1:50010, bytes: 33358, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-816585658_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742520_1697, duration: 1509423
2015-12-13 16:51:19,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742520_1697, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:51:19,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742521_1698 src: /127.0.0.1:58989 dest: /127.0.0.1:50010
2015-12-13 16:51:19,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58989, dest: /127.0.0.1:50010, bytes: 103678, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-816585658_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742521_1698, duration: 3634017
2015-12-13 16:51:19,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742521_1698, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 16:51:24,324 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742518_1695
2015-12-13 16:51:26,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742512_1689 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742512 for deletion
2015-12-13 16:51:26,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742513_1690 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742513 for deletion
2015-12-13 16:51:26,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742514_1691 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742514 for deletion
2015-12-13 16:51:26,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742512_1689 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742512
2015-12-13 16:51:26,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742515_1692 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742515 for deletion
2015-12-13 16:51:26,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742513_1690 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742513
2015-12-13 16:51:26,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742516_1693 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742516 for deletion
2015-12-13 16:51:26,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742514_1691 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742514
2015-12-13 16:51:26,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742517_1694 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742517 for deletion
2015-12-13 16:51:26,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742515_1692 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742515
2015-12-13 16:51:26,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742516_1693 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742516
2015-12-13 16:51:26,554 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742517_1694 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742517
2015-12-13 16:51:29,331 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742521_1698
2015-12-13 16:51:29,331 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742520_1697
2015-12-13 16:51:29,332 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742519_1696
2015-12-13 16:53:29,537 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1841603009-127.0.1.1-1449456040486 Total blocks: 231, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-12-13 18:48:40,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from localhost/127.0.0.1:9000 with active state
2015-12-13 18:48:40,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-13 18:48:40,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-13 18:48:40,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 231 blocks total. Took 0 msec to generate and 23 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@18f2825e
2015-12-13 18:48:40,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-13 18:58:22,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 231 blocks total. Took 1 msec to generate and 16 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@752de125
2015-12-13 18:58:22,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-13 19:40:31,903 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742518_1695 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742518 for deletion
2015-12-13 19:40:31,904 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742518_1695 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742518
2015-12-13 19:40:34,898 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742508_1685 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742508 for deletion
2015-12-13 19:40:34,899 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742508_1685 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742508
2015-12-13 19:41:04,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742522_1699 src: /127.0.0.1:59079 dest: /127.0.0.1:50010
2015-12-13 19:41:05,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59079, dest: /127.0.0.1:50010, bytes: 7236, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-239153680_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742522_1699, duration: 49313407
2015-12-13 19:41:05,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742522_1699, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:41:10,113 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742522_1699
2015-12-13 19:41:49,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742523_1700 src: /127.0.0.1:59085 dest: /127.0.0.1:50010
2015-12-13 19:41:49,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59085, dest: /127.0.0.1:50010, bytes: 7331, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1792814267_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742523_1700, duration: 42134210
2015-12-13 19:41:49,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742523_1700, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:41:49,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742524_1701 src: /127.0.0.1:59086 dest: /127.0.0.1:50010
2015-12-13 19:41:49,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59086, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1792814267_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742524_1701, duration: 1625943
2015-12-13 19:41:49,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742524_1701, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:41:49,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742525_1702 src: /127.0.0.1:59087 dest: /127.0.0.1:50010
2015-12-13 19:41:49,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59087, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1792814267_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742525_1702, duration: 5024675
2015-12-13 19:41:49,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742525_1702, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:41:49,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742526_1703 src: /127.0.0.1:59088 dest: /127.0.0.1:50010
2015-12-13 19:41:49,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59088, dest: /127.0.0.1:50010, bytes: 87283, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1792814267_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742526_1703, duration: 16857689
2015-12-13 19:41:49,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742526_1703, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:41:55,327 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742524_1701
2015-12-13 19:41:55,327 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742523_1700
2015-12-13 19:41:55,328 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742525_1702
2015-12-13 19:41:55,329 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742526_1703
2015-12-13 19:41:56,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742527_1704 src: /127.0.0.1:59097 dest: /127.0.0.1:50010
2015-12-13 19:41:56,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59097, dest: /127.0.0.1:50010, bytes: 103829, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1400787560_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742527_1704, duration: 55494565
2015-12-13 19:41:56,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742527_1704, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:42:05,492 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742527_1704
2015-12-13 19:42:10,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742528_1705 src: /127.0.0.1:59113 dest: /127.0.0.1:50010
2015-12-13 19:42:40,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59113, dest: /127.0.0.1:50010, bytes: 43354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1400787560_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742528_1705, duration: 30552245066
2015-12-13 19:42:40,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742528_1705, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:42:41,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742529_1706 src: /127.0.0.1:59157 dest: /127.0.0.1:50010
2015-12-13 19:42:41,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59157, dest: /127.0.0.1:50010, bytes: 336, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1400787560_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742529_1706, duration: 1273260
2015-12-13 19:42:41,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742529_1706, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:42:41,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742530_1707 src: /127.0.0.1:59159 dest: /127.0.0.1:50010
2015-12-13 19:42:41,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59159, dest: /127.0.0.1:50010, bytes: 43354, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1400787560_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742530_1707, duration: 3960584
2015-12-13 19:42:41,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742530_1707, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:42:41,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742531_1708 src: /127.0.0.1:59160 dest: /127.0.0.1:50010
2015-12-13 19:42:41,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59160, dest: /127.0.0.1:50010, bytes: 103829, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1400787560_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742531_1708, duration: 3706682
2015-12-13 19:42:41,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742531_1708, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:42:46,935 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742528_1705 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742528 for deletion
2015-12-13 19:42:46,936 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742523_1700 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742523 for deletion
2015-12-13 19:42:46,936 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742524_1701 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742524 for deletion
2015-12-13 19:42:46,936 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742528_1705 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742528
2015-12-13 19:42:46,936 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742525_1702 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742525 for deletion
2015-12-13 19:42:46,936 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742526_1703 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742526 for deletion
2015-12-13 19:42:46,937 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742527_1704 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742527 for deletion
2015-12-13 19:42:46,937 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742523_1700 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742523
2015-12-13 19:42:46,937 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742524_1701 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742524
2015-12-13 19:42:46,937 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742525_1702 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742525
2015-12-13 19:42:46,938 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742526_1703 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742526
2015-12-13 19:42:46,938 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742527_1704 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742527
2015-12-13 19:42:50,881 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742529_1706
2015-12-13 19:42:50,883 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742531_1708
2015-12-13 19:42:50,883 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742530_1707
2015-12-13 19:44:00,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742532_1709 src: /127.0.0.1:59167 dest: /127.0.0.1:50010
2015-12-13 19:44:01,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59167, dest: /127.0.0.1:50010, bytes: 7343, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1295307344_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742532_1709, duration: 66078230
2015-12-13 19:44:01,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742532_1709, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:01,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742533_1710 src: /127.0.0.1:59168 dest: /127.0.0.1:50010
2015-12-13 19:44:01,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59168, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1295307344_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742533_1710, duration: 1402908
2015-12-13 19:44:01,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742533_1710, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:01,124 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742534_1711 src: /127.0.0.1:59169 dest: /127.0.0.1:50010
2015-12-13 19:44:01,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59169, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1295307344_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742534_1711, duration: 2883137
2015-12-13 19:44:01,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742534_1711, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:01,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742535_1712 src: /127.0.0.1:59170 dest: /127.0.0.1:50010
2015-12-13 19:44:01,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59170, dest: /127.0.0.1:50010, bytes: 87280, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1295307344_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742535_1712, duration: 7132799
2015-12-13 19:44:01,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742535_1712, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:06,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742536_1713 src: /127.0.0.1:59178 dest: /127.0.0.1:50010
2015-12-13 19:44:06,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59178, dest: /127.0.0.1:50010, bytes: 103826, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-200040914_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742536_1713, duration: 33482079
2015-12-13 19:44:06,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742536_1713, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:10,980 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742532_1709
2015-12-13 19:44:10,980 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742534_1711
2015-12-13 19:44:10,982 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742535_1712
2015-12-13 19:44:10,982 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742533_1710
2015-12-13 19:44:15,990 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742536_1713
2015-12-13 19:44:20,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742537_1714 src: /127.0.0.1:59196 dest: /127.0.0.1:50010
2015-12-13 19:44:29,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742538_1715 src: /127.0.0.1:59207 dest: /127.0.0.1:50010
2015-12-13 19:44:29,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59207, dest: /127.0.0.1:50010, bytes: 2212, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0026_r_000000_0_1450381624_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742538_1715, duration: 72521302
2015-12-13 19:44:29,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742538_1715, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:30,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59196, dest: /127.0.0.1:50010, bytes: 48749, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-200040914_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742537_1714, duration: 10468942783
2015-12-13 19:44:30,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742537_1714, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:30,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742539_1716 src: /127.0.0.1:59210 dest: /127.0.0.1:50010
2015-12-13 19:44:30,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59210, dest: /127.0.0.1:50010, bytes: 350, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-200040914_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742539_1716, duration: 1617801
2015-12-13 19:44:30,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742539_1716, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:30,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742540_1717 src: /127.0.0.1:59212 dest: /127.0.0.1:50010
2015-12-13 19:44:30,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59212, dest: /127.0.0.1:50010, bytes: 48749, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-200040914_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742540_1717, duration: 2883250
2015-12-13 19:44:30,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742540_1717, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:30,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742541_1718 src: /127.0.0.1:59213 dest: /127.0.0.1:50010
2015-12-13 19:44:31,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59213, dest: /127.0.0.1:50010, bytes: 103826, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-200040914_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742541_1718, duration: 32881487
2015-12-13 19:44:31,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742541_1718, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:33,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742542_1719 src: /127.0.0.1:59217 dest: /127.0.0.1:50010
2015-12-13 19:44:33,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59217, dest: /127.0.0.1:50010, bytes: 7343, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1295307344_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742542_1719, duration: 19400732
2015-12-13 19:44:33,556 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742542_1719, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:33,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742543_1720 src: /127.0.0.1:59218 dest: /127.0.0.1:50010
2015-12-13 19:44:33,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59218, dest: /127.0.0.1:50010, bytes: 151, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1295307344_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742543_1720, duration: 1349921
2015-12-13 19:44:33,611 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742543_1720, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:33,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742544_1721 src: /127.0.0.1:59219 dest: /127.0.0.1:50010
2015-12-13 19:44:33,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59219, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1295307344_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742544_1721, duration: 9443665
2015-12-13 19:44:33,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742544_1721, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:33,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742545_1722 src: /127.0.0.1:59220 dest: /127.0.0.1:50010
2015-12-13 19:44:33,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59220, dest: /127.0.0.1:50010, bytes: 87156, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1295307344_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742545_1722, duration: 9024684
2015-12-13 19:44:33,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742545_1722, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:34,946 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742532_1709 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742532 for deletion
2015-12-13 19:44:34,947 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742533_1710 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742533 for deletion
2015-12-13 19:44:34,947 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742534_1711 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742534 for deletion
2015-12-13 19:44:34,947 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742532_1709 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742532
2015-12-13 19:44:34,947 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742535_1712 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742535 for deletion
2015-12-13 19:44:34,947 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742536_1713 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742536 for deletion
2015-12-13 19:44:34,947 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742533_1710 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742533
2015-12-13 19:44:34,947 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742537_1714 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742537 for deletion
2015-12-13 19:44:34,948 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742534_1711 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742534
2015-12-13 19:44:34,948 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742535_1712 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742535
2015-12-13 19:44:34,948 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742536_1713 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742536
2015-12-13 19:44:34,948 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742537_1714 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742537
2015-12-13 19:44:36,076 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742538_1715
2015-12-13 19:44:36,077 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742541_1718
2015-12-13 19:44:36,078 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742540_1717
2015-12-13 19:44:36,078 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742539_1716
2015-12-13 19:44:41,081 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742544_1721
2015-12-13 19:44:41,082 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742543_1720
2015-12-13 19:44:41,085 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742545_1722
2015-12-13 19:44:41,086 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742542_1719
2015-12-13 19:44:42,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742546_1723 src: /127.0.0.1:59229 dest: /127.0.0.1:50010
2015-12-13 19:44:42,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59229, dest: /127.0.0.1:50010, bytes: 103678, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1401250137_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742546_1723, duration: 53854829
2015-12-13 19:44:42,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742546_1723, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:47,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742547_1724 src: /127.0.0.1:59236 dest: /127.0.0.1:50010
2015-12-13 19:44:51,092 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742546_1723
2015-12-13 19:44:53,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742548_1725 src: /127.0.0.1:59241 dest: /127.0.0.1:50010
2015-12-13 19:44:53,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59241, dest: /127.0.0.1:50010, bytes: 8227, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0027_r_000000_0_-1480492386_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742548_1725, duration: 37180640
2015-12-13 19:44:53,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742548_1725, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:53,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59236, dest: /127.0.0.1:50010, bytes: 33360, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1401250137_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742547_1724, duration: 6214866226
2015-12-13 19:44:53,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742547_1724, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:53,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742549_1726 src: /127.0.0.1:59243 dest: /127.0.0.1:50010
2015-12-13 19:44:53,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59243, dest: /127.0.0.1:50010, bytes: 349, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1401250137_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742549_1726, duration: 1359292
2015-12-13 19:44:53,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742549_1726, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:53,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742550_1727 src: /127.0.0.1:59245 dest: /127.0.0.1:50010
2015-12-13 19:44:53,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59245, dest: /127.0.0.1:50010, bytes: 33360, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1401250137_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742550_1727, duration: 1522759
2015-12-13 19:44:53,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742550_1727, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:53,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742551_1728 src: /127.0.0.1:59246 dest: /127.0.0.1:50010
2015-12-13 19:44:53,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59246, dest: /127.0.0.1:50010, bytes: 103678, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1401250137_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742551_1728, duration: 1638012
2015-12-13 19:44:53,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742551_1728, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:44:58,946 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742544_1721 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742544 for deletion
2015-12-13 19:44:58,946 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742545_1722 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742545 for deletion
2015-12-13 19:44:58,946 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742546_1723 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742546 for deletion
2015-12-13 19:44:58,946 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742547_1724 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742547 for deletion
2015-12-13 19:44:58,947 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742542_1719 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742542 for deletion
2015-12-13 19:44:58,947 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742543_1720 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742543 for deletion
2015-12-13 19:44:58,947 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742544_1721 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742544
2015-12-13 19:44:58,947 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742545_1722 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742545
2015-12-13 19:44:58,947 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742546_1723 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742546
2015-12-13 19:44:58,948 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742547_1724 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742547
2015-12-13 19:44:58,948 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742542_1719 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742542
2015-12-13 19:44:58,948 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742543_1720 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742543
2015-12-13 19:45:01,100 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742549_1726
2015-12-13 19:45:01,100 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742548_1725
2015-12-13 19:45:01,101 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742550_1727
2015-12-13 19:45:01,102 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742551_1728
2015-12-13 19:56:41,043 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742548_1725 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742548 for deletion
2015-12-13 19:56:41,044 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742548_1725 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742548
2015-12-13 19:56:44,040 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742538_1715 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742538 for deletion
2015-12-13 19:56:44,040 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742538_1715 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742538
2015-12-13 19:56:45,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742552_1729 src: /127.0.0.1:59269 dest: /127.0.0.1:50010
2015-12-13 19:56:45,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59269, dest: /127.0.0.1:50010, bytes: 7343, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2012034550_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742552_1729, duration: 38262318
2015-12-13 19:56:45,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742552_1729, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:56:45,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742553_1730 src: /127.0.0.1:59270 dest: /127.0.0.1:50010
2015-12-13 19:56:45,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59270, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2012034550_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742553_1730, duration: 9692518
2015-12-13 19:56:45,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742553_1730, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:56:45,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742554_1731 src: /127.0.0.1:59271 dest: /127.0.0.1:50010
2015-12-13 19:56:45,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59271, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2012034550_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742554_1731, duration: 1560752
2015-12-13 19:56:45,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742554_1731, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:56:45,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742555_1732 src: /127.0.0.1:59272 dest: /127.0.0.1:50010
2015-12-13 19:56:45,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59272, dest: /127.0.0.1:50010, bytes: 87280, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2012034550_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742555_1732, duration: 7110146
2015-12-13 19:56:45,777 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742555_1732, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:56:51,861 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742554_1731
2015-12-13 19:56:51,862 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742553_1730
2015-12-13 19:56:51,863 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742552_1729
2015-12-13 19:56:51,864 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742555_1732
2015-12-13 19:56:52,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742556_1733 src: /127.0.0.1:59281 dest: /127.0.0.1:50010
2015-12-13 19:56:52,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59281, dest: /127.0.0.1:50010, bytes: 103826, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_215337525_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742556_1733, duration: 30712966
2015-12-13 19:56:52,918 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742556_1733, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:57:01,888 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742556_1733
2015-12-13 19:57:07,186 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2033ms
No GCs detected
2015-12-13 19:57:10,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742557_1734 src: /127.0.0.1:59300 dest: /127.0.0.1:50010
2015-12-13 19:57:20,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742558_1735 src: /127.0.0.1:59311 dest: /127.0.0.1:50010
2015-12-13 19:57:20,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59311, dest: /127.0.0.1:50010, bytes: 2212, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0028_r_000000_0_-1994037348_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742558_1735, duration: 65740137
2015-12-13 19:57:20,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742558_1735, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:57:22,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59300, dest: /127.0.0.1:50010, bytes: 48791, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_215337525_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742557_1734, duration: 11588956961
2015-12-13 19:57:22,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742557_1734, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:57:22,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742559_1736 src: /127.0.0.1:59314 dest: /127.0.0.1:50010
2015-12-13 19:57:22,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59314, dest: /127.0.0.1:50010, bytes: 350, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_215337525_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742559_1736, duration: 1862986
2015-12-13 19:57:22,125 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742559_1736, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:57:22,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742560_1737 src: /127.0.0.1:59316 dest: /127.0.0.1:50010
2015-12-13 19:57:22,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59316, dest: /127.0.0.1:50010, bytes: 48791, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_215337525_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742560_1737, duration: 3311960
2015-12-13 19:57:22,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742560_1737, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:57:22,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742561_1738 src: /127.0.0.1:59317 dest: /127.0.0.1:50010
2015-12-13 19:57:22,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59317, dest: /127.0.0.1:50010, bytes: 103826, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_215337525_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742561_1738, duration: 1702899
2015-12-13 19:57:22,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742561_1738, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:57:24,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742562_1739 src: /127.0.0.1:59320 dest: /127.0.0.1:50010
2015-12-13 19:57:24,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59320, dest: /127.0.0.1:50010, bytes: 7343, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2012034550_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742562_1739, duration: 7505645
2015-12-13 19:57:24,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742562_1739, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:57:24,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742563_1740 src: /127.0.0.1:59321 dest: /127.0.0.1:50010
2015-12-13 19:57:24,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59321, dest: /127.0.0.1:50010, bytes: 151, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2012034550_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742563_1740, duration: 6018380
2015-12-13 19:57:24,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742563_1740, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:57:24,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742564_1741 src: /127.0.0.1:59322 dest: /127.0.0.1:50010
2015-12-13 19:57:24,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59322, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2012034550_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742564_1741, duration: 5473673
2015-12-13 19:57:24,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742564_1741, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:57:24,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742565_1742 src: /127.0.0.1:59323 dest: /127.0.0.1:50010
2015-12-13 19:57:24,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59323, dest: /127.0.0.1:50010, bytes: 87156, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2012034550_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742565_1742, duration: 4238158
2015-12-13 19:57:24,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742565_1742, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:57:27,286 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742560_1737
2015-12-13 19:57:27,287 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742561_1738
2015-12-13 19:57:27,288 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742558_1735
2015-12-13 19:57:27,288 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742557_1734
2015-12-13 19:57:27,288 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742559_1736
2015-12-13 19:57:28,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742552_1729 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742552 for deletion
2015-12-13 19:57:28,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742553_1730 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742553 for deletion
2015-12-13 19:57:28,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742554_1731 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742554 for deletion
2015-12-13 19:57:28,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742555_1732 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742555 for deletion
2015-12-13 19:57:28,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742556_1733 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742556 for deletion
2015-12-13 19:57:28,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742557_1734 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742557 for deletion
2015-12-13 19:57:28,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742552_1729 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742552
2015-12-13 19:57:28,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742553_1730 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742553
2015-12-13 19:57:28,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742554_1731 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742554
2015-12-13 19:57:28,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742555_1732 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742555
2015-12-13 19:57:28,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742556_1733 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742556
2015-12-13 19:57:28,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742557_1734 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742557
2015-12-13 19:57:32,293 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742562_1739
2015-12-13 19:57:32,294 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742565_1742
2015-12-13 19:57:32,294 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742564_1741
2015-12-13 19:57:32,295 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742563_1740
2015-12-13 19:57:34,924 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742566_1743 src: /127.0.0.1:59334 dest: /127.0.0.1:50010
2015-12-13 19:57:34,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59334, dest: /127.0.0.1:50010, bytes: 103678, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1807072964_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742566_1743, duration: 59776724
2015-12-13 19:57:34,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742566_1743, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:57:39,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742567_1744 src: /127.0.0.1:59340 dest: /127.0.0.1:50010
2015-12-13 19:57:42,321 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742566_1743
2015-12-13 19:57:46,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742568_1745 src: /127.0.0.1:59345 dest: /127.0.0.1:50010
2015-12-13 19:57:47,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59345, dest: /127.0.0.1:50010, bytes: 8227, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0029_r_000000_0_1638786630_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742568_1745, duration: 50213283
2015-12-13 19:57:47,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742568_1745, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:57:47,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59340, dest: /127.0.0.1:50010, bytes: 33370, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1807072964_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742567_1744, duration: 7424601018
2015-12-13 19:57:47,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742567_1744, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:57:47,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742569_1746 src: /127.0.0.1:59347 dest: /127.0.0.1:50010
2015-12-13 19:57:47,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59347, dest: /127.0.0.1:50010, bytes: 349, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1807072964_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742569_1746, duration: 1278914
2015-12-13 19:57:47,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742569_1746, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:57:47,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742570_1747 src: /127.0.0.1:59349 dest: /127.0.0.1:50010
2015-12-13 19:57:47,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59349, dest: /127.0.0.1:50010, bytes: 33370, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1807072964_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742570_1747, duration: 1516526
2015-12-13 19:57:47,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742570_1747, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:57:47,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742571_1748 src: /127.0.0.1:59350 dest: /127.0.0.1:50010
2015-12-13 19:57:47,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59350, dest: /127.0.0.1:50010, bytes: 103678, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1807072964_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742571_1748, duration: 4740018
2015-12-13 19:57:47,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742571_1748, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 19:57:52,131 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742562_1739 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742562 for deletion
2015-12-13 19:57:52,131 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742563_1740 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742563 for deletion
2015-12-13 19:57:52,131 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742564_1741 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742564 for deletion
2015-12-13 19:57:52,131 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742562_1739 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742562
2015-12-13 19:57:52,131 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742565_1742 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742565 for deletion
2015-12-13 19:57:52,131 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742566_1743 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742566 for deletion
2015-12-13 19:57:52,131 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742563_1740 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742563
2015-12-13 19:57:52,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742567_1744 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742567 for deletion
2015-12-13 19:57:52,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742564_1741 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742564
2015-12-13 19:57:52,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742565_1742 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742565
2015-12-13 19:57:52,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742566_1743 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742566
2015-12-13 19:57:52,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742567_1744 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742567
2015-12-13 19:57:52,328 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742568_1745
2015-12-13 19:57:57,336 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742570_1747
2015-12-13 19:57:57,337 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742571_1748
2015-12-13 19:57:57,338 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742569_1746
2015-12-13 20:10:10,212 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742568_1745 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742568 for deletion
2015-12-13 20:10:10,213 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742568_1745 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742568
2015-12-13 20:10:13,213 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742558_1735 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742558 for deletion
2015-12-13 20:10:13,213 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742558_1735 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742558
2015-12-13 20:10:13,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742572_1749 src: /127.0.0.1:59370 dest: /127.0.0.1:50010
2015-12-13 20:10:13,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59370, dest: /127.0.0.1:50010, bytes: 7412, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1705163570_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742572_1749, duration: 38753597
2015-12-13 20:10:13,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742572_1749, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:10:13,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742573_1750 src: /127.0.0.1:59371 dest: /127.0.0.1:50010
2015-12-13 20:10:13,518 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59371, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1705163570_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742573_1750, duration: 2060153
2015-12-13 20:10:13,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742573_1750, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:10:13,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742574_1751 src: /127.0.0.1:59372 dest: /127.0.0.1:50010
2015-12-13 20:10:13,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59372, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1705163570_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742574_1751, duration: 1714363
2015-12-13 20:10:13,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742574_1751, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:10:13,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742575_1752 src: /127.0.0.1:59373 dest: /127.0.0.1:50010
2015-12-13 20:10:13,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59373, dest: /127.0.0.1:50010, bytes: 87280, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1705163570_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742575_1752, duration: 12502410
2015-12-13 20:10:13,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742575_1752, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:10:19,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742576_1753 src: /127.0.0.1:59382 dest: /127.0.0.1:50010
2015-12-13 20:10:19,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59382, dest: /127.0.0.1:50010, bytes: 103826, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2039051372_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742576_1753, duration: 42397269
2015-12-13 20:10:19,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742576_1753, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:10:23,146 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742572_1749
2015-12-13 20:10:23,147 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742573_1750
2015-12-13 20:10:23,148 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742574_1751
2015-12-13 20:10:23,150 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742575_1752
2015-12-13 20:10:28,167 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742576_1753
2015-12-13 20:10:34,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742577_1754 src: /127.0.0.1:59400 dest: /127.0.0.1:50010
2015-12-13 20:10:44,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742578_1755 src: /127.0.0.1:59411 dest: /127.0.0.1:50010
2015-12-13 20:10:44,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59411, dest: /127.0.0.1:50010, bytes: 2212, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0030_r_000000_0_-792959229_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742578_1755, duration: 100452967
2015-12-13 20:10:44,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742578_1755, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:10:45,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59400, dest: /127.0.0.1:50010, bytes: 48769, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2039051372_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742577_1754, duration: 11316019994
2015-12-13 20:10:45,594 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742577_1754, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:10:45,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742579_1756 src: /127.0.0.1:59414 dest: /127.0.0.1:50010
2015-12-13 20:10:45,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59414, dest: /127.0.0.1:50010, bytes: 350, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2039051372_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742579_1756, duration: 2837147
2015-12-13 20:10:45,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742579_1756, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:10:45,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742580_1757 src: /127.0.0.1:59416 dest: /127.0.0.1:50010
2015-12-13 20:10:45,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59416, dest: /127.0.0.1:50010, bytes: 48769, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2039051372_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742580_1757, duration: 17351763
2015-12-13 20:10:45,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742580_1757, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:10:45,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742581_1758 src: /127.0.0.1:59417 dest: /127.0.0.1:50010
2015-12-13 20:10:45,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59417, dest: /127.0.0.1:50010, bytes: 103826, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2039051372_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742581_1758, duration: 4589912
2015-12-13 20:10:45,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742581_1758, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:10:48,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742582_1759 src: /127.0.0.1:59420 dest: /127.0.0.1:50010
2015-12-13 20:10:48,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59420, dest: /127.0.0.1:50010, bytes: 7412, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1705163570_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742582_1759, duration: 15436347
2015-12-13 20:10:48,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742582_1759, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:10:48,084 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742583_1760 src: /127.0.0.1:59421 dest: /127.0.0.1:50010
2015-12-13 20:10:48,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59421, dest: /127.0.0.1:50010, bytes: 151, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1705163570_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742583_1760, duration: 9679732
2015-12-13 20:10:48,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742583_1760, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:10:48,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742584_1761 src: /127.0.0.1:59422 dest: /127.0.0.1:50010
2015-12-13 20:10:48,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59422, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1705163570_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742584_1761, duration: 1686529
2015-12-13 20:10:48,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742584_1761, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:10:48,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742585_1762 src: /127.0.0.1:59423 dest: /127.0.0.1:50010
2015-12-13 20:10:48,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59423, dest: /127.0.0.1:50010, bytes: 87156, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1705163570_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742585_1762, duration: 5928193
2015-12-13 20:10:48,206 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742585_1762, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:10:52,227 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742576_1753 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742576 for deletion
2015-12-13 20:10:52,227 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742577_1754 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742577 for deletion
2015-12-13 20:10:52,227 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742572_1749 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742572 for deletion
2015-12-13 20:10:52,227 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742573_1750 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742573 for deletion
2015-12-13 20:10:52,228 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742574_1751 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742574 for deletion
2015-12-13 20:10:52,228 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742575_1752 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742575 for deletion
2015-12-13 20:10:52,229 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742576_1753 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742576
2015-12-13 20:10:52,229 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742577_1754 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742577
2015-12-13 20:10:52,229 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742572_1749 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742572
2015-12-13 20:10:52,229 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742573_1750 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742573
2015-12-13 20:10:52,229 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742574_1751 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742574
2015-12-13 20:10:52,229 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742575_1752 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742575
2015-12-13 20:10:53,550 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742584_1761
2015-12-13 20:10:53,551 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742581_1758
2015-12-13 20:10:53,552 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742583_1760
2015-12-13 20:10:53,552 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742579_1756
2015-12-13 20:10:58,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742586_1763 src: /127.0.0.1:59433 dest: /127.0.0.1:50010
2015-12-13 20:10:58,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59433, dest: /127.0.0.1:50010, bytes: 103678, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_65559680_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742586_1763, duration: 50345616
2015-12-13 20:10:58,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742586_1763, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:11:04,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742587_1764 src: /127.0.0.1:59439 dest: /127.0.0.1:50010
2015-12-13 20:11:08,824 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742586_1763
2015-12-13 20:11:10,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742588_1765 src: /127.0.0.1:59444 dest: /127.0.0.1:50010
2015-12-13 20:11:10,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59444, dest: /127.0.0.1:50010, bytes: 8237, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0031_r_000000_0_826764021_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742588_1765, duration: 88314716
2015-12-13 20:11:10,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742588_1765, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:11:10,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59439, dest: /127.0.0.1:50010, bytes: 33359, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_65559680_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742587_1764, duration: 6923374991
2015-12-13 20:11:10,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742587_1764, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:11:10,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742589_1766 src: /127.0.0.1:59446 dest: /127.0.0.1:50010
2015-12-13 20:11:10,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59446, dest: /127.0.0.1:50010, bytes: 349, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_65559680_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742589_1766, duration: 1460152
2015-12-13 20:11:10,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742589_1766, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:11:10,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742590_1767 src: /127.0.0.1:59448 dest: /127.0.0.1:50010
2015-12-13 20:11:10,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59448, dest: /127.0.0.1:50010, bytes: 33359, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_65559680_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742590_1767, duration: 1652716
2015-12-13 20:11:10,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742590_1767, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:11:11,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742591_1768 src: /127.0.0.1:59449 dest: /127.0.0.1:50010
2015-12-13 20:11:11,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59449, dest: /127.0.0.1:50010, bytes: 103678, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_65559680_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742591_1768, duration: 3223058
2015-12-13 20:11:11,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742591_1768, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:11:16,220 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742582_1759 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742582 for deletion
2015-12-13 20:11:16,221 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742583_1760 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742583 for deletion
2015-12-13 20:11:16,221 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742584_1761 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742584 for deletion
2015-12-13 20:11:16,221 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742582_1759 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742582
2015-12-13 20:11:16,221 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742585_1762 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742585 for deletion
2015-12-13 20:11:16,221 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742586_1763 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742586 for deletion
2015-12-13 20:11:16,221 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742583_1760 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742583
2015-12-13 20:11:16,221 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742587_1764 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742587 for deletion
2015-12-13 20:11:16,221 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742584_1761 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742584
2015-12-13 20:11:16,221 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742585_1762 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742585
2015-12-13 20:11:16,221 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742586_1763 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742586
2015-12-13 20:11:16,221 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742587_1764 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742587
2015-12-13 20:11:18,831 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742591_1768
2015-12-13 20:11:18,831 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742590_1767
2015-12-13 20:11:18,832 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742588_1765
2015-12-13 20:11:18,832 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742589_1766
2015-12-13 20:15:49,255 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742578_1755 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742578 for deletion
2015-12-13 20:15:49,255 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742588_1765 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742588 for deletion
2015-12-13 20:15:49,255 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742578_1755 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742578
2015-12-13 20:15:49,255 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742588_1765 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir2/blk_1073742588
2015-12-13 20:15:50,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742592_1769 src: /127.0.0.1:59462 dest: /127.0.0.1:50010
2015-12-13 20:15:50,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59462, dest: /127.0.0.1:50010, bytes: 7451, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-766214267_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742592_1769, duration: 55907525
2015-12-13 20:15:50,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742592_1769, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:15:50,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742593_1770 src: /127.0.0.1:59463 dest: /127.0.0.1:50010
2015-12-13 20:15:50,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59463, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-766214267_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742593_1770, duration: 1466386
2015-12-13 20:15:50,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742593_1770, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:15:50,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742594_1771 src: /127.0.0.1:59464 dest: /127.0.0.1:50010
2015-12-13 20:15:50,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59464, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-766214267_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742594_1771, duration: 3324955
2015-12-13 20:15:50,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742594_1771, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:15:51,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742595_1772 src: /127.0.0.1:59465 dest: /127.0.0.1:50010
2015-12-13 20:15:51,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59465, dest: /127.0.0.1:50010, bytes: 87280, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-766214267_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742595_1772, duration: 7941141
2015-12-13 20:15:51,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742595_1772, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:15:57,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742596_1773 src: /127.0.0.1:59474 dest: /127.0.0.1:50010
2015-12-13 20:15:57,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59474, dest: /127.0.0.1:50010, bytes: 103826, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_931145019_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742596_1773, duration: 73649752
2015-12-13 20:15:57,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742596_1773, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:15:59,160 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742592_1769
2015-12-13 20:15:59,162 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742594_1771
2015-12-13 20:15:59,164 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742595_1772
2015-12-13 20:15:59,165 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742593_1770
2015-12-13 20:16:04,182 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742596_1773
2015-12-13 20:16:11,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742597_1774 src: /127.0.0.1:59491 dest: /127.0.0.1:50010
2015-12-13 20:16:21,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742598_1775 src: /127.0.0.1:59502 dest: /127.0.0.1:50010
2015-12-13 20:16:21,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59502, dest: /127.0.0.1:50010, bytes: 2212, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0032_r_000000_0_-242327616_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742598_1775, duration: 6235438
2015-12-13 20:16:21,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742598_1775, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:16:23,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59491, dest: /127.0.0.1:50010, bytes: 48773, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_931145019_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742597_1774, duration: 11055969658
2015-12-13 20:16:23,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742597_1774, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:16:23,037 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742599_1776 src: /127.0.0.1:59505 dest: /127.0.0.1:50010
2015-12-13 20:16:23,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59505, dest: /127.0.0.1:50010, bytes: 350, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_931145019_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742599_1776, duration: 1459397
2015-12-13 20:16:23,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742599_1776, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:16:23,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742600_1777 src: /127.0.0.1:59507 dest: /127.0.0.1:50010
2015-12-13 20:16:23,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59507, dest: /127.0.0.1:50010, bytes: 48773, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_931145019_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742600_1777, duration: 3339538
2015-12-13 20:16:23,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742600_1777, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:16:23,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742601_1778 src: /127.0.0.1:59508 dest: /127.0.0.1:50010
2015-12-13 20:16:23,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59508, dest: /127.0.0.1:50010, bytes: 103826, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_931145019_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742601_1778, duration: 7965801
2015-12-13 20:16:23,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742601_1778, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:16:25,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742602_1779 src: /127.0.0.1:59511 dest: /127.0.0.1:50010
2015-12-13 20:16:25,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59511, dest: /127.0.0.1:50010, bytes: 7451, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-766214267_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742602_1779, duration: 11261914
2015-12-13 20:16:25,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742602_1779, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:16:25,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742603_1780 src: /127.0.0.1:59512 dest: /127.0.0.1:50010
2015-12-13 20:16:25,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59512, dest: /127.0.0.1:50010, bytes: 151, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-766214267_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742603_1780, duration: 14532840
2015-12-13 20:16:25,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742603_1780, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:16:25,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742604_1781 src: /127.0.0.1:59513 dest: /127.0.0.1:50010
2015-12-13 20:16:25,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59513, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-766214267_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742604_1781, duration: 1675190
2015-12-13 20:16:25,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742604_1781, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:16:25,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742605_1782 src: /127.0.0.1:59514 dest: /127.0.0.1:50010
2015-12-13 20:16:25,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59514, dest: /127.0.0.1:50010, bytes: 87156, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-766214267_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742605_1782, duration: 9536264
2015-12-13 20:16:25,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742605_1782, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:16:28,264 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742592_1769 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742592 for deletion
2015-12-13 20:16:28,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742593_1770 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742593 for deletion
2015-12-13 20:16:28,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742594_1771 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742594 for deletion
2015-12-13 20:16:28,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742595_1772 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742595 for deletion
2015-12-13 20:16:28,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742596_1773 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742596 for deletion
2015-12-13 20:16:28,266 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742597_1774 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742597 for deletion
2015-12-13 20:16:28,267 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742592_1769 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742592
2015-12-13 20:16:28,267 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742593_1770 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742593
2015-12-13 20:16:28,267 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742594_1771 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742594
2015-12-13 20:16:28,268 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742595_1772 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742595
2015-12-13 20:16:28,268 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742596_1773 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742596
2015-12-13 20:16:28,268 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742597_1774 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742597
2015-12-13 20:16:29,251 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742599_1776
2015-12-13 20:16:29,252 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742601_1778
2015-12-13 20:16:29,252 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742600_1777
2015-12-13 20:16:29,253 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742598_1775
2015-12-13 20:16:34,258 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742602_1779
2015-12-13 20:16:34,259 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742605_1782
2015-12-13 20:16:34,259 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742603_1780
2015-12-13 20:16:34,260 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742604_1781
2015-12-13 20:16:35,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742606_1783 src: /127.0.0.1:59524 dest: /127.0.0.1:50010
2015-12-13 20:16:35,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59524, dest: /127.0.0.1:50010, bytes: 103678, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1869567362_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742606_1783, duration: 42938387
2015-12-13 20:16:35,432 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742606_1783, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:16:41,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742607_1784 src: /127.0.0.1:59531 dest: /127.0.0.1:50010
2015-12-13 20:16:44,281 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742606_1783
2015-12-13 20:16:48,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742608_1785 src: /127.0.0.1:59536 dest: /127.0.0.1:50010
2015-12-13 20:16:48,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59536, dest: /127.0.0.1:50010, bytes: 8237, op: HDFS_WRITE, cliID: DFSClient_attempt_1450020873492_0033_r_000000_0_-121312067_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742608_1785, duration: 82222985
2015-12-13 20:16:48,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742608_1785, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:16:48,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59531, dest: /127.0.0.1:50010, bytes: 33359, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1869567362_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742607_1784, duration: 7346675088
2015-12-13 20:16:48,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742607_1784, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:16:48,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742609_1786 src: /127.0.0.1:59538 dest: /127.0.0.1:50010
2015-12-13 20:16:48,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59538, dest: /127.0.0.1:50010, bytes: 349, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1869567362_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742609_1786, duration: 1559489
2015-12-13 20:16:48,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742609_1786, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:16:48,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742610_1787 src: /127.0.0.1:59540 dest: /127.0.0.1:50010
2015-12-13 20:16:48,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59540, dest: /127.0.0.1:50010, bytes: 33359, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1869567362_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742610_1787, duration: 1679998
2015-12-13 20:16:48,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742610_1787, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:16:48,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742611_1788 src: /127.0.0.1:59541 dest: /127.0.0.1:50010
2015-12-13 20:16:48,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59541, dest: /127.0.0.1:50010, bytes: 103678, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1869567362_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742611_1788, duration: 2078477
2015-12-13 20:16:48,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742611_1788, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-13 20:16:54,291 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742607_1784
2015-12-13 20:16:54,293 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742611_1788
2015-12-13 20:16:54,293 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742608_1785
2015-12-13 20:16:54,294 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742610_1787
2015-12-13 20:16:55,257 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742602_1779 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742602 for deletion
2015-12-13 20:16:55,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742603_1780 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742603 for deletion
2015-12-13 20:16:55,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742604_1781 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742604 for deletion
2015-12-13 20:16:55,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742602_1779 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742602
2015-12-13 20:16:55,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742605_1782 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742605 for deletion
2015-12-13 20:16:55,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742603_1780 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742603
2015-12-13 20:16:55,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742606_1783 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742606 for deletion
2015-12-13 20:16:55,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742604_1781 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742604
2015-12-13 20:16:55,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742605_1782 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742605
2015-12-13 20:16:55,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742606_1783 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742606
2015-12-13 20:16:55,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742607_1784 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742607 for deletion
2015-12-13 20:16:55,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742607_1784 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742607
2015-12-13 20:38:34,477 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "ubuntu/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-12-13 20:38:38,448 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-13 20:38:38,950 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-13 20:38:38,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-15 01:11:12,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-15 01:11:12,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-15 01:11:13,805 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-15 01:11:13,925 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-15 01:11:13,925 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-15 01:11:13,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-15 01:11:13,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-15 01:11:14,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-15 01:11:14,017 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-15 01:11:14,017 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-15 01:11:14,166 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-15 01:11:14,169 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-15 01:11:14,179 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-15 01:11:14,182 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-15 01:11:14,182 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-15 01:11:14,182 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-15 01:11:14,210 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-15 01:11:14,229 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-15 01:11:14,229 INFO org.mortbay.log: jetty-6.1.26
2015-12-15 01:11:14,548 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-15 01:11:14,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-15 01:11:14,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-15 01:11:14,903 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-15 01:11:14,925 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-15 01:11:14,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-15 01:11:14,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-15 01:11:15,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-15 01:11:15,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-15 01:11:15,070 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-15 01:11:15,071 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-15 01:11:15,486 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-15 01:11:15,596 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 3745@ubuntu
2015-12-15 01:11:15,699 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1841603009-127.0.1.1-1449456040486
2015-12-15 01:11:15,699 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-12-15 01:11:15,701 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-15 01:11:15,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1379726899;bpid=BP-1841603009-127.0.1.1-1449456040486;lv=-56;nsInfo=lv=-60;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0;bpid=BP-1841603009-127.0.1.1-1449456040486;dnuuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-15 01:11:15,780 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /home/parallels/hadoop/tmp/dfs/data/current
2015-12-15 01:11:15,782 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/parallels/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-15 01:11:15,852 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-15 01:11:15,857 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1450164733857 with interval 21600000
2015-12-15 01:11:15,857 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-15 01:11:15,859 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-15 01:11:15,972 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1841603009-127.0.1.1-1449456040486 on /home/parallels/hadoop/tmp/dfs/data/current: 114ms
2015-12-15 01:11:15,973 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1841603009-127.0.1.1-1449456040486: 115ms
2015-12-15 01:11:15,973 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-15 01:11:16,024 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current: 51ms
2015-12-15 01:11:16,024 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 52ms
2015-12-15 01:11:16,029 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-15 01:11:16,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-15 01:11:16,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-15 01:11:16,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=6389
2015-12-15 01:11:16,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000
2015-12-15 01:11:16,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 259 blocks total. Took 7 msec to generate and 202 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@626495c2
2015-12-15 01:11:16,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-15 01:11:16,514 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-12-15 01:11:16,514 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-15 01:11:16,518 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-12-15 01:11:16,518 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-12-15 01:11:16,518 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-15 01:11:16,542 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1841603009-127.0.1.1-1449456040486 to blockPoolScannerMap, new size=1
2015-12-15 01:11:21,168 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742471_1648
2015-12-15 01:11:21,174 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742511_1688
2015-12-15 01:11:21,177 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742609_1786
2015-12-15 01:11:21,178 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742469_1646
2015-12-15 01:11:52,181 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742598_1775 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742598 for deletion
2015-12-15 01:11:52,187 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742598_1775 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742598
2015-12-15 01:13:31,129 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742608_1785 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742608 for deletion
2015-12-15 01:13:31,131 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742608_1785 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742608
2015-12-15 01:14:05,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742612_1789 src: /127.0.0.1:37263 dest: /127.0.0.1:50010
2015-12-15 01:14:05,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37263, dest: /127.0.0.1:50010, bytes: 7781, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_389115373_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742612_1789, duration: 102076850
2015-12-15 01:14:05,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742612_1789, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:14:05,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742613_1790 src: /127.0.0.1:37264 dest: /127.0.0.1:50010
2015-12-15 01:14:05,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37264, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_389115373_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742613_1790, duration: 2194219
2015-12-15 01:14:05,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742613_1790, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:14:05,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742614_1791 src: /127.0.0.1:37265 dest: /127.0.0.1:50010
2015-12-15 01:14:05,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37265, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_389115373_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742614_1791, duration: 2484415
2015-12-15 01:14:05,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742614_1791, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:14:06,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742615_1792 src: /127.0.0.1:37266 dest: /127.0.0.1:50010
2015-12-15 01:14:06,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37266, dest: /127.0.0.1:50010, bytes: 87280, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_389115373_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742615_1792, duration: 15659361
2015-12-15 01:14:06,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742615_1792, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:14:17,259 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742616_1793 src: /127.0.0.1:37287 dest: /127.0.0.1:50010
2015-12-15 01:14:17,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37287, dest: /127.0.0.1:50010, bytes: 103826, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1903893461_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742616_1793, duration: 38523273
2015-12-15 01:14:17,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742616_1793, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:14:31,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742617_1794 src: /127.0.0.1:37316 dest: /127.0.0.1:50010
2015-12-15 01:14:42,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742618_1795 src: /127.0.0.1:37328 dest: /127.0.0.1:50010
2015-12-15 01:14:42,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37328, dest: /127.0.0.1:50010, bytes: 2201, op: HDFS_WRITE, cliID: DFSClient_attempt_1450159889622_0001_r_000000_0_1200234909_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742618_1795, duration: 170406414
2015-12-15 01:14:42,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742618_1795, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:14:43,897 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37316, dest: /127.0.0.1:50010, bytes: 49670, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1903893461_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742617_1794, duration: 12551341527
2015-12-15 01:14:43,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742617_1794, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:14:43,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742619_1796 src: /127.0.0.1:37332 dest: /127.0.0.1:50010
2015-12-15 01:14:43,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37332, dest: /127.0.0.1:50010, bytes: 350, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1903893461_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742619_1796, duration: 2702625
2015-12-15 01:14:43,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742619_1796, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:14:44,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742620_1797 src: /127.0.0.1:37334 dest: /127.0.0.1:50010
2015-12-15 01:14:44,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37334, dest: /127.0.0.1:50010, bytes: 49670, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1903893461_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742620_1797, duration: 6186475
2015-12-15 01:14:44,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742620_1797, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:14:44,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742621_1798 src: /127.0.0.1:37335 dest: /127.0.0.1:50010
2015-12-15 01:14:44,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37335, dest: /127.0.0.1:50010, bytes: 103826, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1903893461_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742621_1798, duration: 9916162
2015-12-15 01:14:44,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742621_1798, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:14:49,288 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742612_1789 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742612 for deletion
2015-12-15 01:14:49,288 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742613_1790 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742613 for deletion
2015-12-15 01:14:49,289 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742614_1791 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742614 for deletion
2015-12-15 01:14:49,289 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742615_1792 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742615 for deletion
2015-12-15 01:14:49,289 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742616_1793 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742616 for deletion
2015-12-15 01:14:49,290 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742617_1794 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742617 for deletion
2015-12-15 01:14:49,292 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742612_1789 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742612
2015-12-15 01:14:49,292 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742613_1790 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742613
2015-12-15 01:14:49,292 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742614_1791 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742614
2015-12-15 01:14:49,292 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742615_1792 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742615
2015-12-15 01:14:49,293 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742616_1793 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742616
2015-12-15 01:14:49,293 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742617_1794 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742617
2015-12-15 01:15:37,292 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "ubuntu/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-12-15 01:15:41,288 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-15 01:15:41,377 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-15 01:15:41,381 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2015-12-15 01:16:16,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /home/parallels/Development/hadoop-2.6.0/etc/hadoop:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/home/parallels/Development/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'jenkins' on 2014-11-13T21:10Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-12-15 01:16:16,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-12-15 01:16:17,682 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-12-15 01:16:17,753 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-12-15 01:16:17,753 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-12-15 01:16:17,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2015-12-15 01:16:17,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-12-15 01:16:17,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-12-15 01:16:17,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-12-15 01:16:17,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-12-15 01:16:17,880 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-12-15 01:16:17,883 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-12-15 01:16:17,893 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-12-15 01:16:17,895 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-12-15 01:16:17,895 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-12-15 01:16:17,895 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-12-15 01:16:17,913 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-12-15 01:16:17,916 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-12-15 01:16:17,916 INFO org.mortbay.log: jetty-6.1.26
2015-12-15 01:16:18,182 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-12-15 01:16:18,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = parallels
2015-12-15 01:16:18,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-12-15 01:16:18,409 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-12-15 01:16:18,431 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-12-15 01:16:18,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-12-15 01:16:18,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-12-15 01:16:18,490 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-12-15 01:16:18,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-12-15 01:16:18,515 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-12-15 01:16:18,516 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-12-15 01:16:18,855 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-12-15 01:16:18,865 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/parallels/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 7241@ubuntu
2015-12-15 01:16:18,931 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1841603009-127.0.1.1-1449456040486
2015-12-15 01:16:18,931 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-12-15 01:16:18,932 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-12-15 01:16:18,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1379726899;bpid=BP-1841603009-127.0.1.1-1449456040486;lv=-56;nsInfo=lv=-60;cid=CID-0742ff35-4678-46e5-8b5b-5c16ff3429a1;nsid=1379726899;c=0;bpid=BP-1841603009-127.0.1.1-1449456040486;dnuuid=80c0a626-2c99-47f4-b1c0-43fb6652c24b
2015-12-15 01:16:18,989 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /home/parallels/hadoop/tmp/dfs/data/current
2015-12-15 01:16:18,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/parallels/hadoop/tmp/dfs/data/current, StorageType: DISK
2015-12-15 01:16:19,054 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-12-15 01:16:19,057 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1450169174057 with interval 21600000
2015-12-15 01:16:19,058 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-15 01:16:19,059 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-15 01:16:19,070 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current: 36465984
2015-12-15 01:16:19,076 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1841603009-127.0.1.1-1449456040486 on /home/parallels/hadoop/tmp/dfs/data/current: 16ms
2015-12-15 01:16:19,076 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1841603009-127.0.1.1-1449456040486: 19ms
2015-12-15 01:16:19,077 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current...
2015-12-15 01:16:19,118 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1841603009-127.0.1.1-1449456040486 on volume /home/parallels/hadoop/tmp/dfs/data/current: 42ms
2015-12-15 01:16:19,119 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 42ms
2015-12-15 01:16:19,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-15 01:16:19,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-15 01:16:19,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-12-15 01:16:19,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=6477
2015-12-15 01:16:19,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000
2015-12-15 01:16:19,374 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 261 blocks total. Took 2 msec to generate and 121 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@5123fa9f
2015-12-15 01:16:19,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-15 01:16:19,378 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-12-15 01:16:19,378 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-12-15 01:16:19,379 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-12-15 01:16:19,379 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-12-15 01:16:19,381 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-15 01:16:19,390 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1841603009-127.0.1.1-1449456040486 to blockPoolScannerMap, new size=1
2015-12-15 01:16:24,171 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742580_1757
2015-12-15 01:16:24,172 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742619_1796
2015-12-15 01:16:24,174 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742620_1797
2015-12-15 01:16:24,174 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1841603009-127.0.1.1-1449456040486:blk_1073742618_1795
2015-12-15 01:18:01,205 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742618_1795 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742618 for deletion
2015-12-15 01:18:01,209 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742618_1795 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742618
2015-12-15 01:18:13,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742622_1799 src: /127.0.0.1:37370 dest: /127.0.0.1:50010
2015-12-15 01:18:13,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37370, dest: /127.0.0.1:50010, bytes: 7767, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_742191953_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742622_1799, duration: 81080479
2015-12-15 01:18:13,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742622_1799, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:18:13,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742623_1800 src: /127.0.0.1:37371 dest: /127.0.0.1:50010
2015-12-15 01:18:13,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37371, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_742191953_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742623_1800, duration: 1925830
2015-12-15 01:18:13,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742623_1800, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:18:13,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742624_1801 src: /127.0.0.1:37372 dest: /127.0.0.1:50010
2015-12-15 01:18:13,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37372, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_742191953_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742624_1801, duration: 1893086
2015-12-15 01:18:13,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742624_1801, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:18:13,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742625_1802 src: /127.0.0.1:37373 dest: /127.0.0.1:50010
2015-12-15 01:18:13,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37373, dest: /127.0.0.1:50010, bytes: 87280, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_742191953_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742625_1802, duration: 11470801
2015-12-15 01:18:13,983 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742625_1802, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:18:22,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742626_1803 src: /127.0.0.1:37382 dest: /127.0.0.1:50010
2015-12-15 01:18:22,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37382, dest: /127.0.0.1:50010, bytes: 103826, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_990837136_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742626_1803, duration: 43592368
2015-12-15 01:18:22,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742626_1803, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:18:40,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742627_1804 src: /127.0.0.1:37398 dest: /127.0.0.1:50010
2015-12-15 01:18:50,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742628_1805 src: /127.0.0.1:37411 dest: /127.0.0.1:50010
2015-12-15 01:18:50,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37411, dest: /127.0.0.1:50010, bytes: 2201, op: HDFS_WRITE, cliID: DFSClient_attempt_1450160190262_0001_r_000000_0_298606197_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742628_1805, duration: 91538323
2015-12-15 01:18:50,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742628_1805, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:18:51,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37398, dest: /127.0.0.1:50010, bytes: 48752, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_990837136_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742627_1804, duration: 11707672459
2015-12-15 01:18:51,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742627_1804, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:18:51,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742629_1806 src: /127.0.0.1:37414 dest: /127.0.0.1:50010
2015-12-15 01:18:51,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37414, dest: /127.0.0.1:50010, bytes: 350, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_990837136_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742629_1806, duration: 1627004
2015-12-15 01:18:51,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742629_1806, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:18:51,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742630_1807 src: /127.0.0.1:37416 dest: /127.0.0.1:50010
2015-12-15 01:18:51,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37416, dest: /127.0.0.1:50010, bytes: 48752, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_990837136_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742630_1807, duration: 2346417
2015-12-15 01:18:51,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742630_1807, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:18:51,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742631_1808 src: /127.0.0.1:37417 dest: /127.0.0.1:50010
2015-12-15 01:18:51,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37417, dest: /127.0.0.1:50010, bytes: 103826, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_990837136_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742631_1808, duration: 2317769
2015-12-15 01:18:51,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742631_1808, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:18:54,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742632_1809 src: /127.0.0.1:37420 dest: /127.0.0.1:50010
2015-12-15 01:18:54,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37420, dest: /127.0.0.1:50010, bytes: 7767, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_742191953_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742632_1809, duration: 4018190
2015-12-15 01:18:54,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742632_1809, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:18:54,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742633_1810 src: /127.0.0.1:37421 dest: /127.0.0.1:50010
2015-12-15 01:18:54,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37421, dest: /127.0.0.1:50010, bytes: 151, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_742191953_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742633_1810, duration: 5633270
2015-12-15 01:18:54,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742633_1810, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:18:54,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742634_1811 src: /127.0.0.1:37422 dest: /127.0.0.1:50010
2015-12-15 01:18:54,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37422, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_742191953_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742634_1811, duration: 2538563
2015-12-15 01:18:54,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742634_1811, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:18:54,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742635_1812 src: /127.0.0.1:37423 dest: /127.0.0.1:50010
2015-12-15 01:18:54,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37423, dest: /127.0.0.1:50010, bytes: 87146, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_742191953_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742635_1812, duration: 2117667
2015-12-15 01:18:54,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742635_1812, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:18:55,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742624_1801 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742624 for deletion
2015-12-15 01:18:55,196 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742625_1802 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742625 for deletion
2015-12-15 01:18:55,196 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742626_1803 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742626 for deletion
2015-12-15 01:18:55,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742627_1804 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742627 for deletion
2015-12-15 01:18:55,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742622_1799 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742622 for deletion
2015-12-15 01:18:55,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742624_1801 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742624
2015-12-15 01:18:55,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742623_1800 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742623 for deletion
2015-12-15 01:18:55,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742625_1802 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742625
2015-12-15 01:18:55,197 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742626_1803 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742626
2015-12-15 01:18:55,198 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742627_1804 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742627
2015-12-15 01:18:55,199 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742622_1799 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742622
2015-12-15 01:18:55,199 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742623_1800 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742623
2015-12-15 01:19:04,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742636_1813 src: /127.0.0.1:37433 dest: /127.0.0.1:50010
2015-12-15 01:19:04,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37433, dest: /127.0.0.1:50010, bytes: 103668, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1974662525_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742636_1813, duration: 73685571
2015-12-15 01:19:04,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742636_1813, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:19:10,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742637_1814 src: /127.0.0.1:37439 dest: /127.0.0.1:50010
2015-12-15 01:19:16,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742638_1815 src: /127.0.0.1:37445 dest: /127.0.0.1:50010
2015-12-15 01:19:16,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37445, dest: /127.0.0.1:50010, bytes: 4205, op: HDFS_WRITE, cliID: DFSClient_attempt_1450160190262_0002_r_000000_0_988753512_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742638_1815, duration: 93899112
2015-12-15 01:19:16,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742638_1815, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:19:17,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37439, dest: /127.0.0.1:50010, bytes: 33352, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1974662525_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742637_1814, duration: 6988590891
2015-12-15 01:19:17,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742637_1814, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:19:17,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742639_1816 src: /127.0.0.1:37447 dest: /127.0.0.1:50010
2015-12-15 01:19:17,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37447, dest: /127.0.0.1:50010, bytes: 349, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1974662525_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742639_1816, duration: 1257848
2015-12-15 01:19:17,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742639_1816, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:19:17,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742640_1817 src: /127.0.0.1:37449 dest: /127.0.0.1:50010
2015-12-15 01:19:17,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37449, dest: /127.0.0.1:50010, bytes: 33352, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1974662525_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742640_1817, duration: 1835942
2015-12-15 01:19:17,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742640_1817, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:19:17,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742641_1818 src: /127.0.0.1:37450 dest: /127.0.0.1:50010
2015-12-15 01:19:17,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37450, dest: /127.0.0.1:50010, bytes: 103668, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1974662525_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742641_1818, duration: 2516104
2015-12-15 01:19:17,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742641_1818, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:19:19,194 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742632_1809 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742632 for deletion
2015-12-15 01:19:19,194 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742633_1810 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742633 for deletion
2015-12-15 01:19:19,194 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742634_1811 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742634 for deletion
2015-12-15 01:19:19,194 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742635_1812 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742635 for deletion
2015-12-15 01:19:19,194 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742636_1813 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742636 for deletion
2015-12-15 01:19:19,194 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742637_1814 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742637 for deletion
2015-12-15 01:19:19,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742632_1809 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742632
2015-12-15 01:19:19,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742633_1810 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742633
2015-12-15 01:19:19,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742634_1811 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742634
2015-12-15 01:19:19,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742635_1812 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742635
2015-12-15 01:19:19,198 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742636_1813 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742636
2015-12-15 01:19:19,198 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742637_1814 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742637
2015-12-15 01:22:22,225 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742638_1815 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742638 for deletion
2015-12-15 01:22:22,226 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742638_1815 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742638
2015-12-15 01:22:25,225 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742628_1805 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742628 for deletion
2015-12-15 01:22:25,226 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742628_1805 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742628
2015-12-15 01:22:27,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742642_1819 src: /127.0.0.1:37463 dest: /127.0.0.1:50010
2015-12-15 01:22:27,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37463, dest: /127.0.0.1:50010, bytes: 7767, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-737380171_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742642_1819, duration: 110922383
2015-12-15 01:22:27,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742642_1819, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:22:28,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742643_1820 src: /127.0.0.1:37464 dest: /127.0.0.1:50010
2015-12-15 01:22:28,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37464, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-737380171_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742643_1820, duration: 3500178
2015-12-15 01:22:28,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742643_1820, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:22:28,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742644_1821 src: /127.0.0.1:37465 dest: /127.0.0.1:50010
2015-12-15 01:22:28,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37465, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-737380171_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742644_1821, duration: 1638748
2015-12-15 01:22:28,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742644_1821, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:22:28,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742645_1822 src: /127.0.0.1:37466 dest: /127.0.0.1:50010
2015-12-15 01:22:28,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37466, dest: /127.0.0.1:50010, bytes: 87280, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-737380171_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742645_1822, duration: 38453488
2015-12-15 01:22:28,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742645_1822, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:22:35,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742646_1823 src: /127.0.0.1:37475 dest: /127.0.0.1:50010
2015-12-15 01:22:35,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37475, dest: /127.0.0.1:50010, bytes: 103826, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-637170363_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742646_1823, duration: 37305109
2015-12-15 01:22:35,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742646_1823, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:22:52,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742647_1824 src: /127.0.0.1:37496 dest: /127.0.0.1:50010
2015-12-15 01:23:02,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742648_1825 src: /127.0.0.1:37508 dest: /127.0.0.1:50010
2015-12-15 01:23:03,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37508, dest: /127.0.0.1:50010, bytes: 2201, op: HDFS_WRITE, cliID: DFSClient_attempt_1450160190262_0003_r_000000_0_1558599516_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742648_1825, duration: 99945431
2015-12-15 01:23:03,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742648_1825, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:23:03,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37496, dest: /127.0.0.1:50010, bytes: 48740, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-637170363_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742647_1824, duration: 11231895257
2015-12-15 01:23:03,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742647_1824, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:23:03,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742649_1826 src: /127.0.0.1:37511 dest: /127.0.0.1:50010
2015-12-15 01:23:03,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37511, dest: /127.0.0.1:50010, bytes: 350, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-637170363_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742649_1826, duration: 2136305
2015-12-15 01:23:03,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742649_1826, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:23:03,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742650_1827 src: /127.0.0.1:37513 dest: /127.0.0.1:50010
2015-12-15 01:23:03,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37513, dest: /127.0.0.1:50010, bytes: 48740, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-637170363_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742650_1827, duration: 10187733
2015-12-15 01:23:03,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742650_1827, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:23:03,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742651_1828 src: /127.0.0.1:37514 dest: /127.0.0.1:50010
2015-12-15 01:23:03,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37514, dest: /127.0.0.1:50010, bytes: 103826, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-637170363_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742651_1828, duration: 19251693
2015-12-15 01:23:03,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742651_1828, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:23:06,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742652_1829 src: /127.0.0.1:37517 dest: /127.0.0.1:50010
2015-12-15 01:23:06,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37517, dest: /127.0.0.1:50010, bytes: 7767, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-737380171_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742652_1829, duration: 9505677
2015-12-15 01:23:06,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742652_1829, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:23:06,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742653_1830 src: /127.0.0.1:37518 dest: /127.0.0.1:50010
2015-12-15 01:23:06,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37518, dest: /127.0.0.1:50010, bytes: 151, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-737380171_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742653_1830, duration: 15186668
2015-12-15 01:23:06,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742653_1830, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:23:06,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742654_1831 src: /127.0.0.1:37519 dest: /127.0.0.1:50010
2015-12-15 01:23:06,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37519, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-737380171_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742654_1831, duration: 1928336
2015-12-15 01:23:06,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742654_1831, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:23:06,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742655_1832 src: /127.0.0.1:37520 dest: /127.0.0.1:50010
2015-12-15 01:23:06,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37520, dest: /127.0.0.1:50010, bytes: 87146, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-737380171_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742655_1832, duration: 6980862
2015-12-15 01:23:06,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742655_1832, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:23:07,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742642_1819 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742642 for deletion
2015-12-15 01:23:07,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742643_1820 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742643 for deletion
2015-12-15 01:23:07,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742644_1821 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742644 for deletion
2015-12-15 01:23:07,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742645_1822 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742645 for deletion
2015-12-15 01:23:07,250 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742646_1823 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742646 for deletion
2015-12-15 01:23:07,250 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742647_1824 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742647 for deletion
2015-12-15 01:23:07,250 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742642_1819 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742642
2015-12-15 01:23:07,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742643_1820 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742643
2015-12-15 01:23:07,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742644_1821 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742644
2015-12-15 01:23:07,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742645_1822 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742645
2015-12-15 01:23:07,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742646_1823 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742646
2015-12-15 01:23:07,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742647_1824 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742647
2015-12-15 01:23:17,664 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742656_1833 src: /127.0.0.1:37530 dest: /127.0.0.1:50010
2015-12-15 01:23:17,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37530, dest: /127.0.0.1:50010, bytes: 103668, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1688126327_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742656_1833, duration: 74121514
2015-12-15 01:23:17,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742656_1833, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:23:23,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742657_1834 src: /127.0.0.1:37536 dest: /127.0.0.1:50010
2015-12-15 01:23:30,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742658_1835 src: /127.0.0.1:37542 dest: /127.0.0.1:50010
2015-12-15 01:23:30,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37542, dest: /127.0.0.1:50010, bytes: 4205, op: HDFS_WRITE, cliID: DFSClient_attempt_1450160190262_0004_r_000000_0_-394416122_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742658_1835, duration: 53766156
2015-12-15 01:23:30,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742658_1835, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:23:31,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37536, dest: /127.0.0.1:50010, bytes: 33351, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1688126327_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742657_1834, duration: 7774475556
2015-12-15 01:23:31,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742657_1834, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:23:31,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742659_1836 src: /127.0.0.1:37544 dest: /127.0.0.1:50010
2015-12-15 01:23:31,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37544, dest: /127.0.0.1:50010, bytes: 349, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1688126327_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742659_1836, duration: 1726158
2015-12-15 01:23:31,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742659_1836, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:23:31,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742660_1837 src: /127.0.0.1:37546 dest: /127.0.0.1:50010
2015-12-15 01:23:31,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37546, dest: /127.0.0.1:50010, bytes: 33351, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1688126327_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742660_1837, duration: 1848395
2015-12-15 01:23:31,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742660_1837, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:23:31,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742661_1838 src: /127.0.0.1:37547 dest: /127.0.0.1:50010
2015-12-15 01:23:31,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37547, dest: /127.0.0.1:50010, bytes: 103668, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1688126327_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742661_1838, duration: 2178840
2015-12-15 01:23:31,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742661_1838, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:23:34,240 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742656_1833 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742656 for deletion
2015-12-15 01:23:34,240 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742657_1834 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742657 for deletion
2015-12-15 01:23:34,240 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742652_1829 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742652 for deletion
2015-12-15 01:23:34,240 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742656_1833 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742656
2015-12-15 01:23:34,240 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742653_1830 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742653 for deletion
2015-12-15 01:23:34,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742654_1831 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742654 for deletion
2015-12-15 01:23:34,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742655_1832 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742655 for deletion
2015-12-15 01:23:34,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742657_1834 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742657
2015-12-15 01:23:34,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742652_1829 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742652
2015-12-15 01:23:34,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742653_1830 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742653
2015-12-15 01:23:34,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742654_1831 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742654
2015-12-15 01:23:34,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742655_1832 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742655
2015-12-15 01:26:28,263 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742658_1835 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742658 for deletion
2015-12-15 01:26:28,264 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742658_1835 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742658
2015-12-15 01:26:31,262 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742648_1825 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742648 for deletion
2015-12-15 01:26:31,262 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742648_1825 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742648
2015-12-15 01:26:33,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742662_1839 src: /127.0.0.1:37558 dest: /127.0.0.1:50010
2015-12-15 01:26:33,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37558, dest: /127.0.0.1:50010, bytes: 7792, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-125775680_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742662_1839, duration: 59582839
2015-12-15 01:26:33,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742662_1839, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:26:33,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742663_1840 src: /127.0.0.1:37559 dest: /127.0.0.1:50010
2015-12-15 01:26:33,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37559, dest: /127.0.0.1:50010, bytes: 445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-125775680_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742663_1840, duration: 5189938
2015-12-15 01:26:33,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742663_1840, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:26:33,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742664_1841 src: /127.0.0.1:37560 dest: /127.0.0.1:50010
2015-12-15 01:26:33,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37560, dest: /127.0.0.1:50010, bytes: 52, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-125775680_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742664_1841, duration: 1887072
2015-12-15 01:26:33,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742664_1841, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:26:33,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742665_1842 src: /127.0.0.1:37561 dest: /127.0.0.1:50010
2015-12-15 01:26:33,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37561, dest: /127.0.0.1:50010, bytes: 87280, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-125775680_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742665_1842, duration: 27801663
2015-12-15 01:26:33,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742665_1842, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:26:39,012 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742666_1843 src: /127.0.0.1:37570 dest: /127.0.0.1:50010
2015-12-15 01:26:39,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37570, dest: /127.0.0.1:50010, bytes: 103826, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-182392407_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742666_1843, duration: 31049138
2015-12-15 01:26:39,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742666_1843, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:26:53,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742667_1844 src: /127.0.0.1:37587 dest: /127.0.0.1:50010
2015-12-15 01:27:02,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742668_1845 src: /127.0.0.1:37598 dest: /127.0.0.1:50010
2015-12-15 01:27:02,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37598, dest: /127.0.0.1:50010, bytes: 2201, op: HDFS_WRITE, cliID: DFSClient_attempt_1450160190262_0005_r_000000_0_-538118467_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742668_1845, duration: 90970804
2015-12-15 01:27:02,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742668_1845, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:27:04,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37587, dest: /127.0.0.1:50010, bytes: 48755, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-182392407_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742667_1844, duration: 10793193279
2015-12-15 01:27:04,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742667_1844, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:27:04,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742669_1846 src: /127.0.0.1:37601 dest: /127.0.0.1:50010
2015-12-15 01:27:04,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37601, dest: /127.0.0.1:50010, bytes: 350, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-182392407_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742669_1846, duration: 2272068
2015-12-15 01:27:04,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742669_1846, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:27:04,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742670_1847 src: /127.0.0.1:37603 dest: /127.0.0.1:50010
2015-12-15 01:27:04,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37603, dest: /127.0.0.1:50010, bytes: 48755, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-182392407_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742670_1847, duration: 4239290
2015-12-15 01:27:04,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742670_1847, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:27:04,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742671_1848 src: /127.0.0.1:37604 dest: /127.0.0.1:50010
2015-12-15 01:27:04,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37604, dest: /127.0.0.1:50010, bytes: 103826, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-182392407_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742671_1848, duration: 3929231
2015-12-15 01:27:04,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742671_1848, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:27:06,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742672_1849 src: /127.0.0.1:37607 dest: /127.0.0.1:50010
2015-12-15 01:27:06,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37607, dest: /127.0.0.1:50010, bytes: 7792, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-125775680_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742672_1849, duration: 7560658
2015-12-15 01:27:06,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742672_1849, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:27:06,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742673_1850 src: /127.0.0.1:37608 dest: /127.0.0.1:50010
2015-12-15 01:27:06,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37608, dest: /127.0.0.1:50010, bytes: 151, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-125775680_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742673_1850, duration: 6337002
2015-12-15 01:27:06,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742673_1850, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:27:06,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742674_1851 src: /127.0.0.1:37609 dest: /127.0.0.1:50010
2015-12-15 01:27:06,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37609, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-125775680_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742674_1851, duration: 1257861
2015-12-15 01:27:06,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742674_1851, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:27:06,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742675_1852 src: /127.0.0.1:37610 dest: /127.0.0.1:50010
2015-12-15 01:27:06,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37610, dest: /127.0.0.1:50010, bytes: 87146, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-125775680_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742675_1852, duration: 6552685
2015-12-15 01:27:06,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742675_1852, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:27:07,274 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742662_1839 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742662 for deletion
2015-12-15 01:27:07,274 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742663_1840 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742663 for deletion
2015-12-15 01:27:07,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742664_1841 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742664 for deletion
2015-12-15 01:27:07,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742665_1842 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742665 for deletion
2015-12-15 01:27:07,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742666_1843 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742666 for deletion
2015-12-15 01:27:07,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742667_1844 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742667 for deletion
2015-12-15 01:27:07,277 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742662_1839 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742662
2015-12-15 01:27:07,277 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742663_1840 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742663
2015-12-15 01:27:07,278 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742664_1841 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742664
2015-12-15 01:27:07,278 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742665_1842 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742665
2015-12-15 01:27:07,278 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742666_1843 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742666
2015-12-15 01:27:07,278 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742667_1844 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742667
2015-12-15 01:27:15,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742676_1853 src: /127.0.0.1:37619 dest: /127.0.0.1:50010
2015-12-15 01:27:15,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37619, dest: /127.0.0.1:50010, bytes: 103668, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-425851021_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742676_1853, duration: 60922539
2015-12-15 01:27:15,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742676_1853, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:27:20,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742677_1854 src: /127.0.0.1:37626 dest: /127.0.0.1:50010
2015-12-15 01:27:26,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742678_1855 src: /127.0.0.1:37632 dest: /127.0.0.1:50010
2015-12-15 01:27:26,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37632, dest: /127.0.0.1:50010, bytes: 8280, op: HDFS_WRITE, cliID: DFSClient_attempt_1450160190262_0006_r_000000_0_-350225901_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742678_1855, duration: 54573084
2015-12-15 01:27:26,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742678_1855, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:27:26,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37626, dest: /127.0.0.1:50010, bytes: 33348, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-425851021_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742677_1854, duration: 6244225389
2015-12-15 01:27:26,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742677_1854, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:27:26,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742679_1856 src: /127.0.0.1:37634 dest: /127.0.0.1:50010
2015-12-15 01:27:26,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37634, dest: /127.0.0.1:50010, bytes: 349, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-425851021_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742679_1856, duration: 1414610
2015-12-15 01:27:26,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742679_1856, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:27:26,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742680_1857 src: /127.0.0.1:37636 dest: /127.0.0.1:50010
2015-12-15 01:27:26,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37636, dest: /127.0.0.1:50010, bytes: 33348, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-425851021_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742680_1857, duration: 2393992
2015-12-15 01:27:26,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742680_1857, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:27:26,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1841603009-127.0.1.1-1449456040486:blk_1073742681_1858 src: /127.0.0.1:37637 dest: /127.0.0.1:50010
2015-12-15 01:27:26,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37637, dest: /127.0.0.1:50010, bytes: 103668, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-425851021_1, offset: 0, srvID: 80c0a626-2c99-47f4-b1c0-43fb6652c24b, blockid: BP-1841603009-127.0.1.1-1449456040486:blk_1073742681_1858, duration: 2828833
2015-12-15 01:27:26,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1841603009-127.0.1.1-1449456040486:blk_1073742681_1858, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-12-15 01:27:31,271 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742672_1849 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742672 for deletion
2015-12-15 01:27:31,272 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742673_1850 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742673 for deletion
2015-12-15 01:27:31,272 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742674_1851 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742674 for deletion
2015-12-15 01:27:31,272 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742672_1849 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742672
2015-12-15 01:27:31,272 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742675_1852 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742675 for deletion
2015-12-15 01:27:31,272 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742673_1850 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742673
2015-12-15 01:27:31,272 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742676_1853 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742676 for deletion
2015-12-15 01:27:31,272 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742674_1851 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742674
2015-12-15 01:27:31,272 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073742677_1854 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742677 for deletion
2015-12-15 01:27:31,272 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742675_1852 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742675
2015-12-15 01:27:31,272 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742676_1853 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742676
2015-12-15 01:27:31,272 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1841603009-127.0.1.1-1449456040486 blk_1073742677_1854 file /home/parallels/hadoop/tmp/dfs/data/current/BP-1841603009-127.0.1.1-1449456040486/current/finalized/subdir0/subdir3/blk_1073742677
2015-12-15 04:22:29,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from localhost/127.0.0.1:9000 with active state
2015-12-15 04:22:29,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-15 04:22:29,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-15 04:22:29,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 280 blocks total. Took 1 msec to generate and 13 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@544e9892
2015-12-15 04:22:29,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-15 06:24:23,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from localhost/127.0.0.1:9000 with active state
2015-12-15 06:24:23,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-15 06:24:23,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-15 06:24:23,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 280 blocks total. Took 1 msec to generate and 10 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@aacc7fa
2015-12-15 06:24:23,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-15 08:25:19,016 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from localhost/127.0.0.1:9000 with active state
2015-12-15 08:25:19,017 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-12-15 08:25:19,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1841603009-127.0.1.1-1449456040486 (Datanode Uuid 80c0a626-2c99-47f4-b1c0-43fb6652c24b) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-12-15 08:25:19,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 280 blocks total. Took 0 msec to generate and 11 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@21a1f581
2015-12-15 08:25:19,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1841603009-127.0.1.1-1449456040486
2015-12-15 09:39:34,007 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "ubuntu/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-12-15 09:39:37,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-12-15 09:39:38,033 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-12-15 09:39:38,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
